<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.151.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 8081. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/deep_neural_networks_and_julia/">Deep Neural Networks and Julia</a>
  </h1>
  <time datetime="2025-10-07T17:27:00Z" class="post-date">Tue, Oct 7, 2025</time>
  <p>Recently, I have spent some time on simple neural networks. The idea is to employ them as universal function approximators for some problems appearing in quantitative finance. There are some great papers on it such as the one from <a href="https://mathematicsinindustry.springeropen.com/articles/10.1186/s13362-019-0066-7">Liu et al. (2019)</a> or <a href="https://arxiv.org/abs/1901.09647">Horvath et al. (2019) Deep Learning Volatility</a> or <a href="https://arxiv.org/abs/2107.01611">Rosenbaum &amp; Zhang (2021)</a>.
Incidentally, I met Liu back when I was finishing my PhD in TU Delft around 2020.</p>
<p>I thought I would try out what Julia offers in terms of library for neural networks. Being a very trendy subject, and Julia a modern language for the scientific community, I had imagined the libraris to be of good quality (like the many I have been using in the past). Surprisingly, I was wrong.</p>
<p>First, I tried <a href="https://github.com/PumasAI/SimpleChains.jl">SimpleChain</a>, which just crashed (core dumped!) on a very simple example. I did not bother finding the root cause, I decided to look for another library. I then tried <a href="https://lux.csail.mit.edu/stable/">LUX</a>. The execution kept going forever without returning with the @compile keyword. I probably was not being lucky, even though my code was only 10s of lines long. So I decided to use <a href="https://fluxml.ai/">Flux</a>, which actually is as simple to use as <a href="https://pytorch.org/">Pytorch</a>, a well-known library in the Python world.</p>
<p>Things works with Flux, and I do manage to do many experiments. But the performance is not great. This was another surprise: Pytorch was actually faster for many tasks on the CPU (no GPU). For example, to train a multi-layer-perceptron with 4 hidden layers of 200 neurons, Julia was taking several hours, until I hit CTRL+C and launched the same training in Pytorch (which took 15 minutes). I think it may be something with AdamW optimizer and relatively wide (but not really compared to LMM stuff) networks.</p>
<p>Overall, the experience is pretty disappointing. Maybe it shows how much effort has been put in the Python ecosystem.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/stochastic_collocation_old_and_new/">Stochastic Collocation - Old And New</a>
  </h1>
  <time datetime="2025-08-05T17:27:00Z" class="post-date">Tue, Aug 5, 2025</time>
  <p>Thomas Roos recently put a preprint on SSRN called <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5215592">Simple, Flexible, Analytic, Arbitrage Free Volatility Interpolation</a>. Being interested in the subject, I had a detailed look at it. It turns out that Thomas stumbled upon <a href="https://www.mdpi.com/2227-9091/7/1/30">spline stochastic collocation</a> without realizing it.</p>
<p>There are a few differences in his approach:</p>
<ul>
<li>The optimization is on the x&rsquo;s instead of the y&rsquo;s, meaning the strike axis is fixed.</li>
<li>An original approach to avoid spurious modes, although I would have liked more details on it, with concrete examples of the penalty. Penalties are often challenging to get right.</li>
<li>A nice analysis of the asymptotic behavior in the wings, showing that an exponentially quadratic form for extrapolation corresponds to linear slopes in implied variance.</li>
</ul>
<p>I had  also explored a fixed strike axis initially (even back in 2014 - I uploaded <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5381580">some old notes on SSRN around this</a>). I found it to be somewhat unstable back then, but it may well be that I did not put enough efforts into it, especially since optimizing the y&rsquo;s instead of the x&rsquo;s allowed for a straightforward use of B-splines, which is very attractive. It is very direct to impose monotonicity with B-splines.</p>
<p>The main advantage of optimizing on the x&rsquo;s instead of the y&rsquo;s is that the knots are defined in the more usual strike space.</p>
<p>Now I find interesting that someone else stumbled upon essentially the same idea starting from another point of view. There may be more merits to this parameterization than I thought.</p>
<p>The challenges I found with stochastic collocations were:</p>
<ul>
<li>spurious spike when the collocation function derivative is close to zero which happens on occasion. This is not really like an extra mode - it is not smooth. Penalty or minimal slope are possible workarounds, but challenging to make robust.</li>
<li>dependency on the choice of knots, especially if nearly exact interpolation is required.</li>
<li>more minor, non-predictable runtime: the non-linear optimization is sometimes very fast, sometimes much slower. This is the case of most arbitrage-free interpolation techniques.</li>
</ul>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/nufft_and_cos/">NUFFT and COS</a>
  </h1>
  <time datetime="2025-07-14T17:27:00Z" class="post-date">Mon, Jul 14, 2025</time>
  <p><a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4335916">Leif Andersen and Mark Lake</a> recently proposed the use of Non-Uniform Fast Fourier Transform for option pricing via the characteristic function. Fourier techniques are most commonly used for pricing vanilla options under the Heston model, in order to calibrate the model. They can be applied to other models, typically with known characteristic function, but also with numerically solved characteristic function as in the rough Heston model, and to different kind of payoffs, for example variance/volatility swaps, options. The subject has been vastly explored already so what&rsquo;s new with this paper?</p>
<p>At first, it was not obvious to me. The paper presents 3 different approaches, two where the probability density and cumulative density is first computed at a (not so sparse) set of knots and the payoff is integrated over it. The remaining approach is the classic one presented in the <a href="https://www.researchgate.net/profile/Dilip-Madan-2/publication/2519144_Option_Valuation_Using_the_Fast_Fourier_Transform/links/55235a820cf2a2d9e146f0bf/Option-Valuation-Using-the-Fast-Fourier-Transform.pdf">Carr and Madan paper from 1999</a> as well as in most of the litterature on the subject. The only additional trick is really the use of NUFFT along with some clever adaptive quadrature to compute the integral.</p>
<p>I thought the main cost in the standard Fourier techniques was the evaluation of the characteristic function at many points, because the characteristic function is usually relatively complicated. And, in practice, it mainly is. So what does the new NUFFT algorithm bring? Surely the characteristic function must still be evaluated at the same points.</p>
<p>The NUFFT becomes interesting to compute the option price at many many strikes at the same time. The strikes do not need to be equidistant and can be (almost) arbitrarily chosen. In Andersen and Lake paper, even a very fast technique such as the COS method reaches a threshold of option per second throughput, as the number of strikes is increased, mainly because of the repeated evaluation of the sum over different strikes, so the cost becomes proportional to the number of strikes.</p>
<p>Evaluating at many many points becomes not much more expensive than evaluating at a few points only. This is what opens up the possibilities for the first 2 approaches based on the density.</p>
<p>It turns out that it is not very difficult to rewrite the COS method so that it can make use of the NUFFT as well. I explain how to do it <a href="https://arxiv.org/abs/2507.13186v1">here</a>. Below is the throughput using NFFT.jl Julia package:
<figure><img src="/post/heston_nufft.png"><figcaption>
      <h4>Throughput on the Heston model with M=256 points.</h4>
    </figcaption>
</figure>
</p>
<p>While a neat trick, it is however not 100% clear to me at this point where this is really useful: calibration typically involve a small number of strikes per maturity.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/forward-variance-models-and-calibration/">Forward Variance Models and Calibration</a>
  </h1>
  <time datetime="2025-06-21T17:27:00Z" class="post-date">Sat, Jun 21, 2025</time>
  <p>The modern rough volatility models adopt a forward variance curve terminology (see for example <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=3191578">this paper on a rational approximation for the rough Heston</a>, or <a href="https://mfe.baruch.cuny.edu/wp-content/uploads/2024/03/AFVPricingDevelopmentsQuantminds2022.pdf">this presentation on affine forward variance models</a> or <a href="https://arxiv.org/pdf/1801.06416">this paper on affine forward variance models</a>). In this form, the rough Heston model reads:</p>
<figure><img src="/post/rough_heston_eqn.png">
</figure>

<p>According to the litterature, the initial forward variance curve is typically built from the implied volatilities through the variance swap replication: for each maturity, the price of a newly issued variance swap is computed to this maturity, using the implied volatilities for this maturity. Then we may compute the forward variance by interpolating linearly the variance swap prices and differentiating. This leads to the plot below.</p>
<figure><img src="/post/hestonhagan_forwarvariance_oct2024.png"><figcaption>
      <h4>Forward variance curves for SPX500 as of October 2024.</h4>
    </figcaption>
</figure>

<p>One immediate question is how much does the truncation range play a role in this forward variance curve? Above, we plot the replications prices based on filtering the input data up to 10 Delta / 90 Delta for each maturity. Beyond this range, liquidity is usually much lower and the reliability of the input quotes may not be as good.  The short end, and not so short forward variance up to 4 years is quite different. The long end is more aligned with something that looks like a basis spread between the two curves.</p>
<p>We may also calibrate the forward variance curve model directly to the implied volatilities during the model calibration, at the same time as the other stochastic volatility parameters (using a time-dependent correlation and vol-of-vol). This time-dependent Heston parameterization has effectively 3 parameters per expiry. In this case, we may want to also filter the input data to focus on  the 10 Delta / 90 Delta range as we expect the stochastic volatility model to fit well where it matters the most - where the vanillas are liquid.</p>
<p>In the figure above, we also plot the forward variance curve implied by the model, through a calibration against vanilla options. We can see it is again quite different.
If we were to use the true replication based forward variance curve, we would effectively attempt to fit far away vols in the wings, which somehow seems wrong. Now if the model was able to fit everything well there would be no issue, but with 3 parameters per maturity, a choice has to be made. And the choice of the full replication does not look particularly great, as we fit exactly something that is not liquid.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/not-so-great-new-scheme-heston/">A Not So Great New Simulation Scheme for the Heston Model</a>
  </h1>
  <time datetime="2025-06-18T22:27:00Z" class="post-date">Wed, Jun 18, 2025</time>
  <p>I recently saw a news about a great <a href="https://arxiv.org/pdf/2412.11264">new simulation scheme for the Heston model by Abi Jaber</a>.
The paper suggests it is better than the popular alternatives such as the QE scheme of Leif Andersen. Reading it quickly, perhaps too quickly, I had the impression it would be more accurate especially when the number of time-steps is small.</p>
<p>The scheme is simple to implement so I decided to spend a few minutes to try it out. I had some test example for the <a href="/post/a-new-scheme-for-heston_part2/">DVSS2X scheme</a> pricing a vanilla at-the-money option with Heston parameters v0=0.04, kappa=0.5, theta=0.04, rho=-0.9, sigma=1.0, and a time to maturity of 10 years. I don&rsquo;t remember exactly where those parameters come from, possibly from Andersen paper. My example was using 8 time-steps per year, which is not that much. And here are the results with 1M paths (using scrambled Sobol):</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">N</th>
          <th style="text-align: left">Scheme</th>
          <th style="text-align: right">Price</th>
          <th style="text-align: right">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1M</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">13.0679</td>
          <td style="text-align: right">-0.0167</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">13.0302</td>
          <td style="text-align: right">-0.0545</td>
      </tr>
      <tr>
          <td style="text-align: left">4M</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">13.0645</td>
          <td style="text-align: right">-0.0202</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">13.0416</td>
          <td style="text-align: right">-0.0431</td>
      </tr>
  </tbody>
</table>
<p>With Sobol scrambling and 1 million paths, the standard error of the Monte-Carlo simulation is lower than 0.01 and the error with this new IVI scheme is (much) larger than 3 standard deviations, indicating that the dominating error in the simulation is due to the discretization.</p>
<p>It is not only less accurate, but also slower, because it requires 3 random numbers per time-step, compared to 2 random numbers for QE or DVSS2X. The paper is very well written, and this small example may not be representative but it does cast some doubts about how great is this new scheme in practice.</p>
<p>While writing this, I noticed that the paper actually uses this same example, it corresponds to their Case 3 and it is indeed not obvious from the plots in the paper that this new IVI scheme is significanly better. There is one case, deep in the money (strike=60%), and very few time-steps (2 per year for example):</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">N</th>
          <th style="text-align: left">Steps/Year</th>
          <th style="text-align: left">Scheme</th>
          <th style="text-align: right">Price</th>
          <th style="text-align: right">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">4M</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.1579</td>
          <td style="text-align: right">-0.1721</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.2852</td>
          <td style="text-align: right">-0.0449</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">4</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.2946</td>
          <td style="text-align: right">-0.0353</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.3113</td>
          <td style="text-align: right">-0.0187</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">8</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.3275</td>
          <td style="text-align: right">-0.0025</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.3239</td>
          <td style="text-align: right">-0.0061</td>
      </tr>
  </tbody>
</table>
<p>So the new scheme works reasonably well for (very) large time-steps, better than DVSS2 and likely better than QE (although, again, it is around 1.5x more costly). For smaller steps (but not that small), it may not be as accurate as QE and DVSS2. This is why QE was such a big deal at the time, it was significantly more accurate than a Euler discretization and allowed to use much less time-steps: from 100 or more to 10 (a factor larger than 10). IVI may be an improvement for very large step sizes, but it will matter much less for typical exotics pricing where observation dates are at worst yearly.</p>
<p><strong>Update June 19, 2025</strong>
Out of curiosity I wondered how it behaved on my <a href="/post/a-new-scheme-for-heston_part2/">forward start option test</a>. In the Table below I use 4M paths.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Scheme</th>
          <th style="text-align: left">Steps/Year</th>
          <th style="text-align: right">Price</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: left">4+1</td>
          <td style="text-align: right">0.0184</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">80</td>
          <td style="text-align: right">0.0196</td>
      </tr>
      <tr>
          <td style="text-align: left">QE</td>
          <td style="text-align: left">4+1</td>
          <td style="text-align: right">0.0190</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">160</td>
          <td style="text-align: right">0.0196</td>
      </tr>
      <tr>
          <td style="text-align: left">IVI</td>
          <td style="text-align: left">4+1</td>
          <td style="text-align: right">0.0116</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">80</td>
          <td style="text-align: right">0.0185</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">160</td>
          <td style="text-align: right">0.0191</td>
      </tr>
  </tbody>
</table>
<p>Clearly, the IVI scheme is not adequate here, it seems to converge very slowly. The price with 4+1 steps is very off, especially compared to the other schemes. The implementation is fairly straighforward, so the IVI scheme may well have a flaw.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/useless-expansions-heston/">Expansions for the time-dependent Heston model: Useless?</a>
  </h1>
  <time datetime="2025-06-01T17:27:00Z" class="post-date">Sun, Jun 1, 2025</time>
  <p>Fabrice Rouah wrote two books on the Heston model: one with C# and Matlab code, and one with VBA code. The two books are very similar. They are good in that they tackle most of the important points with the Heston model, from calibration to simulation. The calibration part (chapter 6) is a bit too short, it would have been great if it presented the actual difficulties with calibration in practice and went more in-depth with the techniques.</p>
<p>There is a full chapter on the time-dependent Heston model and it presents there the <a href="https://hal.science/hal-00370717/file/BenhamouGobetMiri_HestonModel.pdf">expansion of Benhamou, Gobet and Miri</a>. The code is relatively annoying to write, so it&rsquo;s great to have code available for it in the book. It is not so common for books to give source code with it, if you read the free access pages on <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118656471">Wiley&rsquo;s website</a>, you can download the source code.</p>
<p>Also the methodology used is the correct one to follow: first, reproduce the numbers of the original paper, second, use the approximation in a concrete calibration. There are however two major problems:</p>
<ul>
<li>The code has errors.</li>
<li>The expansion is not really good in practice.</li>
</ul>
<p>There are two errors in the code: one in the cross derivative of PHIgxy2 (first order on x and second order on y), and one in the second order cross derivative of the Black-Scholes price dPdx2dy2.</p>
<p><div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>PHIgxy2 =  (<span style="color:#40a070">1</span><span style="color:#666">/</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>y^(<span style="color:#666">-</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>(phigx<span style="color:#666">*</span>(f<span style="color:#666">+</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f^<span style="color:#40a070">2</span><span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span>) <span style="color:#666">+</span> phig<span style="color:#666">*</span>(fx<span style="color:#666">+</span>gx<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f<span style="color:#666">*</span>fx<span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f^<span style="color:#40a070">2</span><span style="color:#666">*</span>gx<span style="color:#666">/</span><span style="color:#40a070">2</span>));</span></span></code></pre></div>
should be
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>PHIgxy2 =  <span style="color:#666">-</span>(<span style="color:#40a070">3</span><span style="color:#666">/</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>y^(<span style="color:#666">-</span><span style="color:#40a070">2.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>(<span style="color:#40a070">1</span><span style="color:#666">-</span>f<span style="color:#666">*</span>g)<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">+</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phigy<span style="color:#666">*</span>(<span style="color:#40a070">1</span><span style="color:#666">-</span>f<span style="color:#666">*</span>g)<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">-</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>fy<span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">-</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>f<span style="color:#666">*</span>gy<span style="color:#666">/</span><span style="color:#40a070">2</span>;</span></span></code></pre></div></p>
<p>and</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>dPdx2dy2 = K<span style="color:#666">*</span><span style="color:#007020">exp</span>(<span style="color:#666">-</span>rf<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>PHIfx2y2 <span style="color:#666">-</span> <span style="color:#007020">exp</span>(<span style="color:#666">-</span>q<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>S<span style="color:#666">*</span>(PHIgy <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy <span style="color:#666">+</span> PHIgx2y <span style="color:#666">+</span> PHIgy2 <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy2 <span style="color:#666">+</span> PHIgx2y2);</span></span></code></pre></div>
<p>should be</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>dPdx2dy2 = K<span style="color:#666">*</span><span style="color:#007020">exp</span>(<span style="color:#666">-</span>rf<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>PHIfx2y2<span style="color:#666">*</span>PHIfx2y2 <span style="color:#666">-</span> <span style="color:#007020">exp</span>(<span style="color:#666">-</span>q<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>S<span style="color:#666">*</span>(PHIgy2 <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy2 <span style="color:#666">+</span> PHIgx2y2);</span></span></code></pre></div>
<p>The formula for the piecewise-constant coefficients also contains errors: for example, the total variance variable wT is wrong. It should be a double sum instead of the standard constant Heston like formula. Finally, the many sums are rederived in the book, differently from the paper and are not simplified (unlike in the paper where they are all single sums).</p>
<p>Indeed, with the original code from Rouah, the prices in the table of the paper from Benhamou Gobet and Miri are not reproduced to the last digit. With the above changes, they are.</p>
<p>On the second point, it is surprising that the book does not mention that the approximation is not great. In particular, it does not mention that the calibrated parameters between Table 9.6 and Table 9.3 are vastly different (except for v0). The calibrated smile is plotted, which is great, but it is only plotted with the approximation formula. Plotting the same using the nearly exact semi-analytical representation of vanlla option prices would have been enlightening. We do it below, as Fabrice Rouah gives all the inputs (a very good thing):</p>
<figure><img src="/post/hestonhagan_bgm_37d.png"><figcaption>
      <h4>DJIA 37 days maturity.</h4>
    </figcaption>
</figure>

<figure><img src="/post/hestonhagan_bgm_226d.png"><figcaption>
      <h4>DJIA 226 days maturity.</h4>
    </figcaption>
</figure>

<p>It looks like in the range the calibration stays in the range of applicability of the formula, which is good, but not necessarily always true. The main issue is however that the calibrated parameters with the approximation are not necessarily a good guess for the parameters of the true time-dependent Heston model, precisely because the actual optimal parameters are way outside the range where the approximation is accurate. This is clear in the 226D plot, the best fit from the approx (which is great) ends up a not so good fit for the real time-dependent Heston model. Somewhat interestingly, the approx is actually not so bad on the actual optimal parameters, gotten from a calibration of the model with the Cos method - it is however a less good fit with the approx than the optimal parameters gotten from a calibration of the model with the approximation.</p>
<p>More recently, Pat Hagan proposed a new approximation for the model, based on a SABR mapping. It seems a bit more precise than the approximation of Benhamou Gobet and Miri, but is not great either. And it is disappointing that the paper does not present any number, nor any plot to assess the quality of the approximation given. Van der Zwaard <a href="https://repository.tudelft.nl/file/File_bf06c85f-2c44-4fef-b94d-9d41e3e6704f">gives</a> some relatively realistic yet simple set of parameters for the Heston model (using Hagan&rsquo;s reparameterization with constant expected variance = 1) and on those, the approximated prices of <strong>at-the-money</strong> options are just not usable:</p>
<p>If we consider his Table 5.9 (maturity = 1.75 year), we have the following</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Method</th>
          <th style="text-align: center">Price</th>
          <th style="text-align: center">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">BGM</td>
          <td style="text-align: center">4.2206</td>
          <td style="text-align: center">0.21</td>
      </tr>
      <tr>
          <td style="text-align: center">Hagan</td>
          <td style="text-align: center">3.3441</td>
          <td style="text-align: center">0.66</td>
      </tr>
      <tr>
          <td style="text-align: center">Reference</td>
          <td style="text-align: center">4.0039</td>
          <td style="text-align: center">0</td>
      </tr>
  </tbody>
</table>
<p>With Table 5.10 (maturity = 1.3 year), it is even worse:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Method</th>
          <th style="text-align: center">Price</th>
          <th style="text-align: center">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">BGM</td>
          <td style="text-align: center">0.6659</td>
          <td style="text-align: center">1.17</td>
      </tr>
      <tr>
          <td style="text-align: center">Hagan</td>
          <td style="text-align: center">0.2263</td>
          <td style="text-align: center">1.61</td>
      </tr>
      <tr>
          <td style="text-align: center">Reference</td>
          <td style="text-align: center">1.8407</td>
          <td style="text-align: center">0</td>
      </tr>
  </tbody>
</table>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/new_spread_option_approximation/">A New Asian Basket Spread Option Approximation</a>
  </h1>
  <time datetime="2025-04-22T12:56:42&#43;0100" class="post-date">Tue, Apr 22, 2025</time>
  <p>Around 10 years ago, while reading the excellent paper of Etore and Gobet on <a href="https://hal.science/hal-00507787v1/document">stochastic Taylor expansions for the pricing of vanilla options with discrete (cash) dividends</a>, I had the idea of a small improvement, by using <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283">a more precise proxy</a> for the Taylor expansion.</p>
<p>More recently, I applied the idea to <a href="https://arxiv.org/abs/2402.17684v1">approximate arithmetic Asian options prices</a> by using the geometric Asian option price as a proxy (with some adjustments). This worked surprisingly well, and is competitive with the best implementations of Curran approach to Asian options pricing. I quickly noticed that I could apply the same idea to <a href="https://arxiv.org/abs/2402.17684">approximate Basket option prices</a>, and from it obtain another approximation for vanilla options with cash dividends through the <a href="https://arxiv.org/abs/2106.12971">mapping described by J. Healy</a>. Interestingly the resulting approximation is the most accurate amongst all other approximations for cash dividends.</p>
<p>During my Easter holidays, I took the time to further extend the idea <a href="https://arxiv.org/abs/2504.16011">to cover general spread options</a> such as Asian Basket spread options. I was slightly surprised at the resulting accuracy: the approximation is by far more accurate than any other published approximation on this problem.
Somewhat interestingly, I noticed that the first order expansion (which is not much more accurate than the proxy itself) seemed to correspond to a previously published approximation from <a href="https://www.scirp.org/journal/paperinformation?paperid=72439">Tommaso Pellegrino</a>, an extension of Bjerksund-Stensland approximation for spread options, although my derivation is very different and allows for higher-order formulae.</p>
<p>Below is an excerpt of some of the results</p>
<figure><img src="/post/deelstra_table_basket_spread.png"><figcaption>
      <h4>Original example from Deelstra et al. for an Basket spread option.</h4>
    </figcaption>
</figure>

<figure><img src="/post/deelstra_table_asian_basket.png"><figcaption>
      <h4>Original example from Deelstra et al. for an Asian Basket spread option. The Taylor approximation perform even better due to the Asianing.</h4>
    </figcaption>
</figure>

<figure><img src="/post/krekel_table_asian_spread.png"><figcaption>
      <h4>Original example from Martin Krekel for an Asian spread option. It was actually challenging to compute accurate Quasi Monte-Carlo prices.</h4>
    </figcaption>
</figure>

<p>The MM of Martin Krekel consists in a one-dimensional integration of two lognormal moment-mached distributions. It seems that the one-dimensional integration (which is exact for simple spread options) does not really improve the accuracy for the more general asian spread option case: the loss of accuracy due to moment matching dominates the error.</p>
<p>The preprint is <a href="https://arxiv.org/abs/2504.16011">available on Arxiv</a>.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/llm_not_good_at_homework/">LLMs Are Not Good For Basic Homework</a>
  </h1>
  <time datetime="2025-03-06T12:56:42&#43;0100" class="post-date">Thu, Mar 6, 2025</time>
  <p>My 12 years old child recently had a presentation to do with 3 other children for school, the subject was food and health.
It was particularly difficult to organize the children and one of the children came with a full presentation with text, generated by an LLM (possibly corrected by the parents).
At first, I thought this was just cheating and completly useless. Furthermore, he failed to read properly the details of the subject, and much of it wasn&rsquo;t relevant.</p>
<p>Then they did a second round together, at another parents home. And the outcome was even worse, in that it was even further away from the subject and had equally strong use of LLM.</p>
<p>I decided to try out some LLM queries on this subject (in French) after I installed a few local LLMs via <a href="https://ollama.com/">Ollama</a>. Ollama makes it really easy to run an LLM locally. The drawback is that
the models have not necessarily been trained very recently, for example Llama 3.2 was trained in 2021 according to itself. I tried Llama 3.2 and Mistral, with respectively 3B and 7B parameters. This is another
drawback, it is challenging to run much larger models as they would require too much RAM. Ollama github page mentions 32 GB to run the 33B models.</p>
<p>The answers we get are not bad, but they are not great either. For example to the question</p>
<p>En quoi les besoins alimentaires dépendent de l&rsquo;age, du sexe et de l&rsquo;activité physique de chaque individu?</p>
<blockquote>
<p>Les besoins alimentaires de chaque individu peuvent être influencés par différents facteurs tels que son âge, son genre, sa taille, son poids, son niveau
d&rsquo;activité physique et son état de santé.</p>
<p>Par exemple, les enfants ont besoin de plus de calories et de nutriments pour leur croissance rapide. De même, les femmes enceintes et allaitantes ont des
besoins alimentaires différents car elles devront fournir les éléments nécessaires à la formation du fœtus ou au lait maternel.</p>
<p>Les hommes et les femmes ont également des besoins alimentaires différents en raison de leurs besoins nutritionnels uniques, tels que la besoin plus élevé
de protéines chez les hommes en raison de leur plus grand poids moyen. En outre, certains nutriments sont <strong>spécifiquement</strong> importants pour des groupes
<strong>spécifiques</strong> d&rsquo;individus, par exemple, les femmes en âge de procréer ont besoin d&rsquo;un apport plus élevé en fer et folate pour s&rsquo;assurer que leur fœtus ait un
développement normal.</p>
<p>Enfin, les individus qui sont plus actifs physiquement ou ceux qui ont une activité physique professionnelle exigeante ont des besoins alimentaires
différents car ils nécessitent plus d&rsquo;énergie pour maintenir leur niveau d&rsquo;activité. Ils doivent consommer davantage de calories et de nutriments pour
répondre à leurs besoins énergétiques supérieurs, ce qui peut inclure des aliments riches en protéines, vitamines et minéraux.</p>
<p>En bref, les besoins alimentaires de chaque individu peuvent être influencés par différents facteurs tels que son âge, son genre, sa taille, son poids, son
niveau d&rsquo;activité physique et son état de santé. Il est donc important de comprendre ces facteurs pour assurer que nous consommons une alimentation adaptée à
nos besoins nutritifs uniques.</p></blockquote>
<p>The introduction and the conclusion are the same. Closely repeated words are another issue: spécifiquement. Furthermore, this word is likely a bad translation from English in the sentence.
&ldquo;spécifiquement importants&rdquo; does not sound quite correct in French. Finally, it&rsquo;s funny that they always produce the same structure of answer, small intro, a few points, and &ldquo;En bref&hellip;&rdquo;. At first it looks good, but very quickly, you
understand it as noise. In fact if we analyze the points given as answer by LLMs, we start to notice that they are not as distinct as their presentation and wording suggest. It is not uncommon to see very similar sentences in distincts enumerated points.</p>
<p>Google&rsquo;s internal LLM is not great either as it does not necessarily give the correct context for understanding.
<figure><img src="/post/google_bad_answer.png">
</figure>

It makes you believe that 1g of protein has 4 calories. But which <a href="https://en.wikipedia.org/wiki/Calorie">kind of calorie</a>? the large <strong>C</strong>alorie (kcal or C) or the small <strong>c</strong>alorie (c) ? The reality is 4000 c or 4 kcal. Btw, I did not know both were introduced by French scientists, and that the large calorie variant was from Marcelin Berthelot. This is a neat feature of websites or books compared to LLMs, you get more contextual information.</p>
<p>The main problem is that the LLMs tend to bring those lists on you, splitting into categories it decided itself, which are most of the time, not categories most human beings would choose.
Here it is to the point that children forget about the actual detailed guidelines given to them, because any of those LLM answers look so convincing. This leaves me with the impression that if they had had no LLM, no internet, no computer, they would have produced a better output.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston_variance_swap_calibration/">Calibrating Heston to Variance Swaps - a bad idea?</a>
  </h1>
  <time datetime="2025-02-11T12:56:42&#43;0100" class="post-date">Tue, Feb 11, 2025</time>
  <p>An interesting idea to calibrate the Heston model in a more stable manner and reduce the calibration time is to make use of variance swap prices. Indeed, there is a simple formula for the theoretical price of a variance swap in the Heston model.
<figure><img src="/post/heston_varswap_formula.png">
</figure>

It is not perfect since it approximates the variance swap price by the expectation of the integrated variance process over time. In particular it does not take into account eventual jumps (obviously), finiteness of replication, and discreteness of observations. But it may be good enough. Thanks to this formula, we can calibrate three parameters of the Heston model: the initial variance, the long-term mean variance, and the speed of mean reversion to the term-structure of variance swaps. We do not need market prices of variance swaps, we may simply use a replication based on market vanilla options prices, such as the model-free replication of Fukasawa.</p>
<p>The idea was studied in the paper <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=2255550">Heston model: the variance swap calibration</a> by F. Guillaume and W. Schoutens back in 2014. The authors however only used the variance swap calibration as an initial guess. How bad would it be to fix the parameters altogether, as in a variance curve approach?</p>
<p>It turns out it can be quite bad since the Heston model does not allow to represent many shapes of the variance swap term structure. Below is an example of term-structure calibration on SPX as of October 2024.</p>
<figure><img src="/post/heston_varswap_ts.png"><figcaption>
      <h4>Term-structure of variance swap prices.</h4>
    </figcaption>
</figure>

<p>The term-structure is V-shaped, and Heston can not fit well to this. The best fit leads to non-sensical parameters with a nearly zero kappa (mean reversion speed) and an exploding long-term mean theta. The next figure shows why it is non-sensical: because the kappa is very small, the variance will very often reach zero.</p>
<figure><img src="/post/heston_smallkappa_vol_path.png"><figcaption>
      <h4>Sample path of the variance process (in terms of vol) using the Euler scheme with full truncation.</h4>
    </figcaption>
</figure>

<p>The calibration using inverse vega weights on vanilla option prices leads a not much worse fit of the variance swap term structure, but exhibits a much to high, unrealistic vol-of-vol of 184%, while a calibration on equally weighted option prices does not fit the term structure well at all.</p>
<p>Somewhat interestingly, the Double-Heston model allows to fit the term-structure much better, but it is far from obvious that the resulting calibrated parameters are much more realistic as it typically leads to process with very small kappa or a process with very small theta (but as there are two processes, it may be more acceptable).</p>
<p>Previously, <a href="/post/unrealistic_variance_swaps_under_schobel_zhu/">I had explored</a> a similar subject for the Schobel-Zhu model. It turns out that Heston is not much more practical either.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/opensuse_tumbleweed/">OpenSuse Tumbleweed to the Rescue</a>
  </h1>
  <time datetime="2025-02-06T12:56:42&#43;0100" class="post-date">Thu, Feb 6, 2025</time>
  <p>I recently upgraded a desktop computer, and to my surprise, the new motherboard was not fully supported by most Linux distributions.
The main culprit was the network adapter, although the secure boot setup gave me lots of troubles as well. I had only a small usb key (2GB)
and most (all?) live distributions do not fit on 2GB anymore. With the exception of Ubuntu images, I did not manage to boot from USB hard drive where I dumped the live image ISO, due to Secure boot
related issues, even when disabling the feature or changing the settings in the BIOS. Using the small USB key worked with Secure boot enabled only.</p>
<p>So I had to find a distribution with a small size and with a new kernel which supported the network adapter (6.13+). I tried the rawhide Fedora, which has a small network install image, but that just did not work, because it&rsquo;s too alpha.</p>
<p>I thought most rolling distributions would fit the bill. It turns out they don&rsquo;t, EndeavourOS or Manjaro have relatively recent live install but (a) they are not small enough to fit on the USB key and (b) the images are a few months old where the kernel is too old.</p>
<p><a href="https://get.opensuse.org/tumbleweed/">OpenSuse Tumbleweed</a> provides a daily updated network installer iso (and a live CD), this is the only thing that worked, and it worked well.</p>
<p>Overall I was a bit surprised that it was so challenging to find an installable distribution with the latest kernel.</p>

  
</article>
</div>

<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/2/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
