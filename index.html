<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.147.8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2025. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/not-so-great-new-scheme-heston/">A Not So Great New Simulation Scheme for the Heston Model</a>
  </h1>
  <time datetime="2025-06-18T20:27:00Z" class="post-date">Wed, Jun 18, 2025</time>
  <p>I recently saw a news about a great <a href="https://arxiv.org/pdf/2412.11264">new simulation scheme for the Heston model by Abi Jaber</a>.
The paper suggests it is better than the popular alternatives such as the QE scheme of Leif Andersen. Reading it quickly, perhaps too quickly, I had the impression it would be more accurate especially when the number of time-steps is small.</p>
<p>The scheme is simple to implement so I decided to spend a few minutes to try it out. I had some test example for the <a href="/post/a-new-scheme-for-heston_part2/">DVSS2X scheme</a> pricing a vanilla at-the-money option with Heston parameters v0=0.04, kappa=0.5, theta=0.04, rho=-0.9, sigma=1.0, and a time to maturity of 10 years. I don&rsquo;t remember exactly where those parameters come from, possibly from Andersen paper. My example was using 8 time-steps per year, which is not that much. And here are the results with 1M paths (using scrambled Sobol):</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">N</th>
          <th style="text-align: left">Scheme</th>
          <th style="text-align: right">Price</th>
          <th style="text-align: right">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">1M</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">13.0679</td>
          <td style="text-align: right">-0.0167</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">13.0302</td>
          <td style="text-align: right">-0.0545</td>
      </tr>
      <tr>
          <td style="text-align: left">4M</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">13.0645</td>
          <td style="text-align: right">-0.0202</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">13.0416</td>
          <td style="text-align: right">-0.0431</td>
      </tr>
  </tbody>
</table>
<p>With Sobol scrambling and 1 million paths, the standard error of the Monte-Carlo simulation is lower than 0.01 and the error with this new IVI scheme is (much) larger than 3 standard deviations, indicating that the dominating error in the simulation is due to the discretization.</p>
<p>It is not only less accurate, but also slower, because it requires 3 random numbers per time-step, compared to 2 random numbers for QE or DVSS2X. The paper is very well written, and this small example may not be representative but it does cast some doubts about how great is this new scheme in practice.</p>
<p>While writing this, I noticed that the paper actually uses this same example, it corresponds to their Case 3 and it is indeed not obvious from the plots in the paper that this new IVI scheme is significanly better. There is one case, deep in the money (strike=60%), and very few time-steps (2 per year for example):</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">N</th>
          <th style="text-align: left">Steps/Year</th>
          <th style="text-align: left">Scheme</th>
          <th style="text-align: right">Price</th>
          <th style="text-align: right">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">4M</td>
          <td style="text-align: left">2</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.1579</td>
          <td style="text-align: right">-0.1721</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.2852</td>
          <td style="text-align: right">-0.0449</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">4</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.2946</td>
          <td style="text-align: right">-0.0353</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.3113</td>
          <td style="text-align: right">-0.0187</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left">8</td>
          <td style="text-align: left">DVSS2X</td>
          <td style="text-align: right">44.3275</td>
          <td style="text-align: right">-0.0025</td>
      </tr>
      <tr>
          <td style="text-align: left"></td>
          <td style="text-align: left"></td>
          <td style="text-align: left">IVI</td>
          <td style="text-align: right">44.3239</td>
          <td style="text-align: right">-0.0061</td>
      </tr>
  </tbody>
</table>
<p>So the new scheme works reasonably well for (very) large time-steps, better than DVSS2 and likely better than QE (although, again, it is around 1.5x more costly). For smaller steps (but not that small), it may not be as accurate as QE and DVSS2. This is why QE was such a big deal at the time, it was significantly more accurate than a Euler discretization and allowed to use much less time-steps: from 100 or more to 10 (a factor larger than 10). IVI may be an improvement for very large step sizes, but it will matter much less for typical exotics pricing where observation dates are at worst yearly.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/useless-expansions-heston/">Expansions for the time-dependent Heston model: Useless?</a>
  </h1>
  <time datetime="2025-06-01T17:27:00Z" class="post-date">Sun, Jun 1, 2025</time>
  <p>Fabrice Rouah wrote two books on the Heston model: one with C# and Matlab code, and one with VBA code. The two books are very similar. They are good in that they tackle most of the important points with the Heston model, from calibration to simulation. The calibration part (chapter 6) is a bit too short, it would have been great if it presented the actual difficulties with calibration in practice and went more in-depth with the techniques.</p>
<p>There is a full chapter on the time-dependent Heston model and it presents there the <a href="https://hal.science/hal-00370717/file/BenhamouGobetMiri_HestonModel.pdf">expansion of Benhamou, Gobet and Miri</a>. The code is relatively annoying to write, so it&rsquo;s great to have code available for it in the book. It is not so common for books to give source code with it, if you read the free access pages on <a href="https://onlinelibrary.wiley.com/doi/book/10.1002/9781118656471">Wiley&rsquo;s website</a>, you can download the source code.</p>
<p>Also the methodology used is the correct one to follow: first, reproduce the numbers of the original paper, second, use the approximation in a concrete calibration. There are however two major problems:</p>
<ul>
<li>The code has errors.</li>
<li>The expansion is not really good in practice.</li>
</ul>
<p>There are two errors in the code: one in the cross derivative of PHIgxy2 (first order on x and second order on y), and one in the second order cross derivative of the Black-Scholes price dPdx2dy2.</p>
<p><div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>PHIgxy2 =  (<span style="color:#40a070">1</span><span style="color:#666">/</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>y^(<span style="color:#666">-</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>(phigx<span style="color:#666">*</span>(f<span style="color:#666">+</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f^<span style="color:#40a070">2</span><span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span>) <span style="color:#666">+</span> phig<span style="color:#666">*</span>(fx<span style="color:#666">+</span>gx<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f<span style="color:#666">*</span>fx<span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span><span style="color:#666">-</span>f^<span style="color:#40a070">2</span><span style="color:#666">*</span>gx<span style="color:#666">/</span><span style="color:#40a070">2</span>));</span></span></code></pre></div>
should be
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>PHIgxy2 =  <span style="color:#666">-</span>(<span style="color:#40a070">3</span><span style="color:#666">/</span><span style="color:#40a070">2</span>)<span style="color:#666">*</span>y^(<span style="color:#666">-</span><span style="color:#40a070">2.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>(<span style="color:#40a070">1</span><span style="color:#666">-</span>f<span style="color:#666">*</span>g)<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">+</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phigy<span style="color:#666">*</span>(<span style="color:#40a070">1</span><span style="color:#666">-</span>f<span style="color:#666">*</span>g)<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">-</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>fy<span style="color:#666">*</span>g<span style="color:#666">/</span><span style="color:#40a070">2</span> <span style="color:#666">-</span> y^(<span style="color:#666">-</span><span style="color:#40a070">1.5</span>)<span style="color:#666">*</span>phig<span style="color:#666">*</span>f<span style="color:#666">*</span>gy<span style="color:#666">/</span><span style="color:#40a070">2</span>;</span></span></code></pre></div></p>
<p>and</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>dPdx2dy2 = K<span style="color:#666">*</span><span style="color:#007020">exp</span>(<span style="color:#666">-</span>rf<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>PHIfx2y2 <span style="color:#666">-</span> <span style="color:#007020">exp</span>(<span style="color:#666">-</span>q<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>S<span style="color:#666">*</span>(PHIgy <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy <span style="color:#666">+</span> PHIgx2y <span style="color:#666">+</span> PHIgy2 <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy2 <span style="color:#666">+</span> PHIgx2y2);</span></span></code></pre></div>
<p>should be</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-matlab" data-lang="matlab"><span style="display:flex;"><span>dPdx2dy2 = K<span style="color:#666">*</span><span style="color:#007020">exp</span>(<span style="color:#666">-</span>rf<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>PHIfx2y2<span style="color:#666">*</span>PHIfx2y2 <span style="color:#666">-</span> <span style="color:#007020">exp</span>(<span style="color:#666">-</span>q<span style="color:#666">*</span>T(NT))<span style="color:#666">*</span>S<span style="color:#666">*</span>(PHIgy2 <span style="color:#666">+</span> <span style="color:#40a070">2</span><span style="color:#666">*</span>PHIgxy2 <span style="color:#666">+</span> PHIgx2y2);</span></span></code></pre></div>
<p>The formula for the piecewise-constant coefficients also contains errors: for example, the total variance variable wT is wrong. It should be a double sum instead of the standard constant Heston like formula. Finally, the many sums are rederived in the book, differently from the paper and are not simplified (unlike in the paper where they are all single sums).</p>
<p>Indeed, with the original code from Rouah, the prices in the table of the paper from Benhamou Gobet and Miri are not reproduced to the last digit. With the above changes, they are.</p>
<p>On the second point, it is surprising that the book does not mention that the approximation is not great. In particular, it does not mention that the calibrated parameters between Table 9.6 and Table 9.3 are vastly different (except for v0). The calibrated smile is plotted, which is great, but it is only plotted with the approximation formula. Plotting the same using the nearly exact semi-analytical representation of vanlla option prices would have been enlightening. We do it below, as Fabrice Rouah gives all the inputs (a very good thing):</p>
<figure><img src="/post/hestonhagan_bgm_37d.png"><figcaption>
      <h4>DJIA 37 days maturity.</h4>
    </figcaption>
</figure>

<figure><img src="/post/hestonhagan_bgm_226d.png"><figcaption>
      <h4>DJIA 226 days maturity.</h4>
    </figcaption>
</figure>

<p>It looks like in the range the calibration stays in the range of applicability of the formula, which is good, but not necessarily always true. The main issue is however that the calibrated parameters with the approximation are not necessarily a good guess for the parameters of the true time-dependent Heston model, precisely because the actual optimal parameters are way outside the range where the approximation is accurate. This is clear in the 226D plot, the best fit from the approx (which is great) ends up a not so good fit for the real time-dependent Heston model. Somewhat interestingly, the approx is actually not so bad on the actual optimal parameters, gotten from a calibration of the model with the Cos method - it is however a less good fit with the approx than the optimal parameters gotten from a calibration of the model with the approximation.</p>
<p>More recently, Pat Hagan proposed a new approximation for the model, based on a SABR mapping. It seems a bit more precise than the approximation of Benhamou Gobet and Miri, but is not great either. And it is disappointing that the paper does not present any number, nor any plot to assess the quality of the approximation given. Van der Zwaard <a href="https://repository.tudelft.nl/file/File_bf06c85f-2c44-4fef-b94d-9d41e3e6704f">gives</a> some relatively realistic yet simple set of parameters for the Heston model (using Hagan&rsquo;s reparameterization with constant expected variance = 1) and on those, the approximated prices of <strong>at-the-money</strong> options are just not usable:</p>
<p>If we consider his Table 5.9 (maturity = 1.75 year), we have the following</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Method</th>
          <th style="text-align: center">Price</th>
          <th style="text-align: center">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">BGM</td>
          <td style="text-align: center">4.2206</td>
          <td style="text-align: center">0.21</td>
      </tr>
      <tr>
          <td style="text-align: center">Hagan</td>
          <td style="text-align: center">3.3441</td>
          <td style="text-align: center">0.66</td>
      </tr>
      <tr>
          <td style="text-align: center">Reference</td>
          <td style="text-align: center">4.0039</td>
          <td style="text-align: center">0</td>
      </tr>
  </tbody>
</table>
<p>With Table 5.10 (maturity = 1.3 year), it is even worse:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Method</th>
          <th style="text-align: center">Price</th>
          <th style="text-align: center">Error</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">BGM</td>
          <td style="text-align: center">0.6659</td>
          <td style="text-align: center">1.17</td>
      </tr>
      <tr>
          <td style="text-align: center">Hagan</td>
          <td style="text-align: center">0.2263</td>
          <td style="text-align: center">1.61</td>
      </tr>
      <tr>
          <td style="text-align: center">Reference</td>
          <td style="text-align: center">1.8407</td>
          <td style="text-align: center">0</td>
      </tr>
  </tbody>
</table>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/new_spread_option_approximation/">A New Asian Basket Spread Option Approximation</a>
  </h1>
  <time datetime="2025-04-22T12:56:42&#43;0100" class="post-date">Tue, Apr 22, 2025</time>
  <p>Around 10 years ago, while reading the excellent paper of Etore and Gobet on <a href="https://hal.science/hal-00507787v1/document">stochastic Taylor expansions for the pricing of vanilla options with discrete (cash) dividends</a>, I had the idea of a small improvement, by using <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283">a more precise proxy</a> for the Taylor expansion.</p>
<p>More recently, I applied the idea to <a href="https://arxiv.org/abs/2402.17684v1">approximate arithmetic Asian options prices</a> by using the geometric Asian option price as a proxy (with some adjustments). This worked surprisingly well, and is competitive with the best implementations of Curran approach to Asian options pricing. I quickly noticed that I could apply the same idea to <a href="https://arxiv.org/abs/2402.17684">approximate Basket option prices</a>, and from it obtain another approximation for vanilla options with cash dividends through the <a href="https://arxiv.org/abs/2106.12971">mapping described by J. Healy</a>. Interestingly the resulting approximation is the most accurate amongst all other approximations for cash dividends.</p>
<p>During my Easter holidays, I took the time to further extend the idea <a href="https://arxiv.org/abs/2504.16011">to cover general spread options</a> such as Asian Basket spread options. I was slightly surprised at the resulting accuracy: the approximation is by far more accurate than any other published approximation on this problem.
Somewhat interestingly, I noticed that the first order expansion (which is not much more accurate than the proxy itself) seemed to correspond to a previously published approximation from <a href="https://www.scirp.org/journal/paperinformation?paperid=72439">Tommaso Pellegrino</a>, an extension of Bjerksund-Stensland approximation for spread options, although my derivation is very different and allows for higher-order formulae.</p>
<p>Below is an excerpt of some of the results</p>
<figure><img src="/post/deelstra_table_basket_spread.png"><figcaption>
      <h4>Original example from Deelstra et al. for an Basket spread option.</h4>
    </figcaption>
</figure>

<figure><img src="/post/deelstra_table_asian_basket.png"><figcaption>
      <h4>Original example from Deelstra et al. for an Asian Basket spread option. The Taylor approximation perform even better due to the Asianing.</h4>
    </figcaption>
</figure>

<figure><img src="/post/krekel_table_asian_spread.png"><figcaption>
      <h4>Original example from Martin Krekel for an Asian spread option. It was actually challenging to compute accurate Quasi Monte-Carlo prices.</h4>
    </figcaption>
</figure>

<p>The MM of Martin Krekel consists in a one-dimensional integration of two lognormal moment-mached distributions. It seems that the one-dimensional integration (which is exact for simple spread options) does not really improve the accuracy for the more general asian spread option case: the loss of accuracy due to moment matching dominates the error.</p>
<p>The preprint is <a href="https://arxiv.org/abs/2504.16011">available on Arxiv</a>.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/llm_not_good_at_homework/">LLMs Are Not Good For Basic Homework</a>
  </h1>
  <time datetime="2025-03-06T12:56:42&#43;0100" class="post-date">Thu, Mar 6, 2025</time>
  <p>My 12 years old child recently had a presentation to do with 3 other children for school, the subject was food and health.
It was particularly difficult to organize the children and one of the children came with a full presentation with text, generated by an LLM (possibly corrected by the parents).
At first, I thought this was just cheating and completly useless. Furthermore, he failed to read properly the details of the subject, and much of it wasn&rsquo;t relevant.</p>
<p>Then they did a second round together, at another parents home. And the outcome was even worse, in that it was even further away from the subject and had equally strong use of LLM.</p>
<p>I decided to try out some LLM queries on this subject (in French) after I installed a few local LLMs via <a href="https://ollama.com/">Ollama</a>. Ollama makes it really easy to run an LLM locally. The drawback is that
the models have not necessarily been trained very recently, for example Llama 3.2 was trained in 2021 according to itself. I tried Llama 3.2 and Mistral, with respectively 3B and 7B parameters. This is another
drawback, it is challenging to run much larger models as they would require too much RAM. Ollama github page mentions 32 GB to run the 33B models.</p>
<p>The answers we get are not bad, but they are not great either. For example to the question</p>
<p>En quoi les besoins alimentaires dépendent de l&rsquo;age, du sexe et de l&rsquo;activité physique de chaque individu?</p>
<blockquote>
<p>Les besoins alimentaires de chaque individu peuvent être influencés par différents facteurs tels que son âge, son genre, sa taille, son poids, son niveau
d&rsquo;activité physique et son état de santé.</p>
<p>Par exemple, les enfants ont besoin de plus de calories et de nutriments pour leur croissance rapide. De même, les femmes enceintes et allaitantes ont des
besoins alimentaires différents car elles devront fournir les éléments nécessaires à la formation du fœtus ou au lait maternel.</p>
<p>Les hommes et les femmes ont également des besoins alimentaires différents en raison de leurs besoins nutritionnels uniques, tels que la besoin plus élevé
de protéines chez les hommes en raison de leur plus grand poids moyen. En outre, certains nutriments sont <strong>spécifiquement</strong> importants pour des groupes
<strong>spécifiques</strong> d&rsquo;individus, par exemple, les femmes en âge de procréer ont besoin d&rsquo;un apport plus élevé en fer et folate pour s&rsquo;assurer que leur fœtus ait un
développement normal.</p>
<p>Enfin, les individus qui sont plus actifs physiquement ou ceux qui ont une activité physique professionnelle exigeante ont des besoins alimentaires
différents car ils nécessitent plus d&rsquo;énergie pour maintenir leur niveau d&rsquo;activité. Ils doivent consommer davantage de calories et de nutriments pour
répondre à leurs besoins énergétiques supérieurs, ce qui peut inclure des aliments riches en protéines, vitamines et minéraux.</p>
<p>En bref, les besoins alimentaires de chaque individu peuvent être influencés par différents facteurs tels que son âge, son genre, sa taille, son poids, son
niveau d&rsquo;activité physique et son état de santé. Il est donc important de comprendre ces facteurs pour assurer que nous consommons une alimentation adaptée à
nos besoins nutritifs uniques.</p></blockquote>
<p>The introduction and the conclusion are the same. Closely repeated words are another issue: spécifiquement. Furthermore, this word is likely a bad translation from English in the sentence.
&ldquo;spécifiquement importants&rdquo; does not sound quite correct in French. Finally, it&rsquo;s funny that they always produce the same structure of answer, small intro, a few points, and &ldquo;En bref&hellip;&rdquo;. At first it looks good, but very quickly, you
understand it as noise. In fact if we analyze the points given as answer by LLMs, we start to notice that they are not as distinct as their presentation and wording suggest. It is not uncommon to see very similar sentences in distincts enumerated points.</p>
<p>Google&rsquo;s internal LLM is not great either as it does not necessarily give the correct context for understanding.
<figure><img src="/post/google_bad_answer.png">
</figure>

It makes you believe that 1g of protein has 4 calories. But which <a href="https://en.wikipedia.org/wiki/Calorie">kind of calorie</a>? the large <strong>C</strong>alorie (kcal or C) or the small <strong>c</strong>alorie (c) ? The reality is 4000 c or 4 kcal. Btw, I did not know both were introduced by French scientists, and that the large calorie variant was from Marcelin Berthelot. This is a neat feature of websites or books compared to LLMs, you get more contextual information.</p>
<p>The main problem is that the LLMs tend to bring those lists on you, splitting into categories it decided itself, which are most of the time, not categories most human beings would choose.
Here it is to the point that children forget about the actual detailed guidelines given to them, because any of those LLM answers look so convincing. This leaves me with the impression that if they had had no LLM, no internet, no computer, they would have produced a better output.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston_variance_swap_calibration/">Calibrating Heston to Variance Swaps - a bad idea?</a>
  </h1>
  <time datetime="2025-02-11T12:56:42&#43;0100" class="post-date">Tue, Feb 11, 2025</time>
  <p>An interesting idea to calibrate the Heston model in a more stable manner and reduce the calibration time is to make use of variance swap prices. Indeed, there is a simple formula for the theoretical price of a variance swap in the Heston model.
<figure><img src="/post/heston_varswap_formula.png">
</figure>

It is not perfect since it approximates the variance swap price by the expectation of the integrated variance process over time. In particular it does not take into account eventual jumps (obviously), finiteness of replication, and discreteness of observations. But it may be good enough. Thanks to this formula, we can calibrate three parameters of the Heston model: the initial variance, the long-term mean variance, and the speed of mean reversion to the term-structure of variance swaps. We do not need market prices of variance swaps, we may simply use a replication based on market vanilla options prices, such as the model-free replication of Fukasawa.</p>
<p>The idea was studied in the paper <a href="https://papers.ssrn.com/sol3/Delivery.cfm?abstractid=2255550">Heston model: the variance swap calibration</a> by F. Guillaume and W. Schoutens back in 2014. The authors however only used the variance swap calibration as an initial guess. How bad would it be to fix the parameters altogether, as in a variance curve approach?</p>
<p>It turns out it can be quite bad since the Heston model does not allow to represent many shapes of the variance swap term structure. Below is an example of term-structure calibration on SPX as of October 2024.</p>
<figure><img src="/post/heston_varswap_ts.png"><figcaption>
      <h4>Term-structure of variance swap prices.</h4>
    </figcaption>
</figure>

<p>The term-structure is V-shaped, and Heston can not fit well to this. The best fit leads to non-sensical parameters with a nearly zero kappa (mean reversion speed) and an exploding long-term mean theta. The next figure shows why it is non-sensical: because the kappa is very small, the variance will very often reach zero.</p>
<figure><img src="/post/heston_smallkappa_vol_path.png"><figcaption>
      <h4>Sample path of the variance process (in terms of vol) using the Euler scheme with full truncation.</h4>
    </figcaption>
</figure>

<p>The calibration using inverse vega weights on vanilla option prices leads a not much worse fit of the variance swap term structure, but exhibits a much to high, unrealistic vol-of-vol of 184%, while a calibration on equally weighted option prices does not fit the term structure well at all.</p>
<p>Somewhat interestingly, the Double-Heston model allows to fit the term-structure much better, but it is far from obvious that the resulting calibrated parameters are much more realistic as it typically leads to process with very small kappa or a process with very small theta (but as there are two processes, it may be more acceptable).</p>
<p>Previously, <a href="/post/unrealistic_variance_swaps_under_schobel_zhu/">I had explored</a> a similar subject for the Schobel-Zhu model. It turns out that Heston is not much more practical either.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/opensuse_tumbleweed/">OpenSuse Tumbleweed to the Rescue</a>
  </h1>
  <time datetime="2025-02-06T12:56:42&#43;0100" class="post-date">Thu, Feb 6, 2025</time>
  <p>I recently upgraded a desktop computer, and to my surprise, the new motherboard was not fully supported by most Linux distributions.
The main culprit was the network adapter, although the secure boot setup gave me lots of troubles as well. I had only a small usb key (2GB)
and most (all?) live distributions do not fit on 2GB anymore. With the exception of Ubuntu images, I did not manage to boot from USB hard drive where I dumped the live image ISO, due to Secure boot
related issues, even when disabling the feature or changing the settings in the BIOS. Using the small USB key worked with Secure boot enabled only.</p>
<p>So I had to find a distribution with a small size and with a new kernel which supported the network adapter (6.13+). I tried the rawhide Fedora, which has a small network install image, but that just did not work, because it&rsquo;s too alpha.</p>
<p>I thought most rolling distributions would fit the bill. It turns out they don&rsquo;t, EndeavourOS or Manjaro have relatively recent live install but (a) they are not small enough to fit on the USB key and (b) the images are a few months old where the kernel is too old.</p>
<p><a href="https://get.opensuse.org/tumbleweed/">OpenSuse Tumbleweed</a> provides a daily updated network installer iso (and a live CD), this is the only thing that worked, and it worked well.</p>
<p>Overall I was a bit surprised that it was so challenging to find an installable distribution with the latest kernel.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/saving_us_book_review/">Saving Us - Book Review</a>
  </h1>
  <time datetime="2024-10-13T12:56:42&#43;0100" class="post-date">Sun, Oct 13, 2024</time>
  <p>I had the opportunity to receive a free book on climate change, through the company I am working for. I had not heard of that book before, it called <em>Saving Us</em> and is written by an actual climate scientist (Katharine Hayhoe).
Unfortunately, written by does not mean that it is a scientific book, and it&rsquo;s not. The author does not spend much effort explaining the physics or the reports, but focuses on how to convince people this is an important problem to tackle. It is mildly interesting at first, as it presents the problem from a psychological angle. But it becomes quickly repetitive. The method is always the same, connect with what&rsquo;s important to an audience who initially rejects climate warming (mostly the phenomenon, sometimes proposals around it), and make them understand that climate warming plays a role in their life, in the very matters they care about. It is presented as a series of personal experiences of the author, a list of examples.</p>
<p>There is a small part on taxing carbon, it&rsquo;s only 3 pages, and quite optimistic. It consists in a socialist approach (which I am not necessarily opposed to). There is no mention of carbon trades, no mention of what can go wrong with the taxing or the trade of carbon.</p>
<p>On the global warming subject, <a href="/post/koonin-on-climate-book-review/">I much preferred</a> the more controversial book <em>Unsettled</em> of Steve Koonin, sometimes quoted by climate warming deniers (who may have not read it at all) and often <a href="https://www.scientificamerican.com/article/a-new-book-manages-to-get-climate-science-badly-wrong/">denounced</a> by climate warming supporters (who may have read it too closely). The arguments made were clearer, more scientific. Even if the book was not that great either, I found it helped discovering the subject from various angles.</p>
<p>Overall it&rsquo;s a bit strange that this kind of book is given away by companies. On one hand, I can see how it resonates with business values such as how to deal with antagonistic people, how to talk to people around you. On the other hand, it feels like a waste of time and money. There are much better books or courses that can teach you more in a shorter time. For example, I attended by accident two 4h courses around <a href="https://en.wikipedia.org/wiki/Myers%E2%80%93Briggs_Type_Indicator">Myers-Briggs</a>, and found it to make a much bigger impact.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/vol_monotonicity_in_practice/">Monotonicity of the Black-Scholes Option Prices in Practice</a>
  </h1>
  <time datetime="2024-09-29T12:56:42&#43;0100" class="post-date">Sun, Sep 29, 2024</time>
  <p>It is well known that vanilla option prices must increase when we increase the implied volatility. Recently, a post on the Wilmott forums wondered about the true accuracy of Peter Jaeckel implied volatility solver, whether it was truely IEEE 754 compliant. In fact, the author noticed some inaccuracy in the option price itself. Unfortunately I can not reply to the forum, its login process does not seem to be working anymore, and so I am left to blog about it.</p>
<p>It can be quite challenging to evaluate the accuracy. For example, one potential error that the author makes is to use numbers that are not exactly representable in IEEE 754 standard such as 4.45 or 0.04. In Julia, and I believe in Python mpmath as well, there is a difference between a BigFloat(4.45) and a BigFloat(&ldquo;4.45&rdquo;):</p>
<p><code>BigFloat(4.45)  = 4.45000000000000017763568394002504646778106689453125 BigFloat(&quot;4.45&quot;)= 4.449999999999999999999999999999999999999999999999999999999999999999999999999972</code></p>
<p>If we work only on usual 64-bit floats, we likely want to estimate the accuracy of an implementation using BigFloat(4.45) as input rather than BigFloat(&ldquo;4.45&rdquo;). To compute a vanilla option price, the naive way would be through the cumulative normal distribution function (the textbook way), which looks like</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">function</span> bs(strike, forward, voltte)
</span></span><span style="display:flex;"><span>    d1 <span style="color:#666">=</span> (log(forward<span style="color:#666">/</span>strike)<span style="color:#666">+</span>voltte<span style="color:#666">^</span><span style="color:#40a070">2</span> <span style="color:#666">/</span> <span style="color:#40a070">2</span>)<span style="color:#666">/</span>voltte
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">return</span> forward<span style="color:#666">*</span>cdf(Normal(),d1)<span style="color:#666">-</span>strike<span style="color:#666">*</span>cdf(Normal(),d1<span style="color:#666">-</span>voltte)
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">end</span></span></span></code></pre></div>
<p>It turns out that this produces relatively high error for out of the money prices. In fact, the prices are far from being monotonic if we increase the vol by 1 ulp a hundred times we obtain:
<figure><img src="/post/bs_price.png">
</figure>
</p>
<p>Peter Jaeckel proposes several improvements, the first simple one is to use the scaled complementary error function <code>erfcx</code> rather than the cumulative normal distribution function directly. A second improvement is to use Taylor expansions for small prices. It turns out that on the example with strike K=2.0, forward F=1.0, vol=0.125, the Taylor expansion is being used.</p>
<figure><img src="/post/bs_price_jaeckel.png">
</figure>

<p>The error is much lower, with Taylor being even a little bit better. But the price is still not monotonic.</p>
<p>How would the implied vol corresponding to those prices look like? If we apply Peter Jaeckel implied vol solver to the naive Black-Scholes formula, we end up with an accuracy limited by the naive formula.
<figure><img src="/post/bs_vol_naive.png">
</figure>
</p>
<p>If we apply it to the exact prices or to erfcx or to Taylor, we still end up with a non monotonic implied vol:</p>
<figure><img src="/post/bs_vol_jaeckel.png">
</figure>

<p>There is no visible difference in accuracy if we start from the erfcx prices and apply the SR Householder solver of <a href="https://github.com/jherekhealy/AQFED.jl/tree/master/src/black">Jherek Healy</a>:</p>
<figure><img src="/post/bs_vol_erfcx.png">
</figure>

<p>It should not be too surprising: if we compose the Black-Scholes formula with the associated solver from Peter Jaeckel, we do not obtain the identity function. Because the option prices are not monotonic with the vol, there are several vols which may be solution to the same price, and the solver is guaranteed to not solve exactly, which leads to the numerical noise we see on the plots. In contrast, it is remarkable that if we compose exp with log we have a nice monotonic shape:</p>
<figure><img src="/post/logexp.png">
</figure>

<p>The prerequisite for a more accurate solver would be a Black-Scholes function which is actually monotonic.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/">Copilot vs ChatGPT on the Optimal Finite Difference Step-Size</a>
  </h1>
  <time datetime="2024-07-25T12:56:42&#43;0100" class="post-date">Thu, Jul 25, 2024</time>
  <p>When computing the derivative of a function by finite difference, which step size is optimal? The answer depends on the kind of difference (forward, backward or central), and the degree of the derivative (first or second typically for finance).</p>
<p>For the first derivative, the result is very quick to find (it&rsquo;s on <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">wikipedia</a>). For the second derivative, it&rsquo;s more challenging. The <a href="https://paulklein.ca/newsite/teaching/Notes_NumericalDifferentiation.pdf">Lecture Notes</a> of Karen Kopecky provide an answer. I wonder if Copilot or ChatGPT would find a good solution to the question:</p>
<blockquote>
<p>&ldquo;What is the optimal step size to compute the second derivative of a function by centered numerical differentiation?&rdquo;</p></blockquote>
<p>Here is the answer of copilot:
<figure><img src="/post/step_size_copilot.png">
</figure>

and the one of chatGPT:
<figure><img src="/post/step_size_chatgpt1.png">
</figure>

and a second answer from chatGPT:
<figure><img src="/post/step_size_chatgpt2.png">
</figure>
</p>
<p>Copilot always fails to provide a valid answer. ChatGPT v4 proves to be quite impressive: it is able to reproduce some of the reasoning presented in the lecture notes.</p>
<p>Interestingly, with centered difference, for a given accuracy, the optimal step size is different for the first and for the second derivative. It may, at first, seem like a good idea to use a different size for each. In reality, when both the first and second derivative of the function are needed (for example the Delta and Gamma greeks of a financial product), it is rarely a good idea to use a different step size for each. Firstly, it will be slower since more the function will be evaluated at more points. Secondly, if there is a discontinuity in the function or in its derivatives, the first derivative may be estimated without going over the discontinuity while the second derivative may be estimated going over the discontinuity, leading to puzzling results.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/cos_method_truncation/">News on the COS Method Truncation</a>
  </h1>
  <time datetime="2024-05-13T20:56:42&#43;0100" class="post-date">Mon, May 13, 2024</time>
  <p>The COS method is a fast way to price vanilla European options under stochastic volatility models with a known characteristic function. There are alternatives, explored in <a href="/post/the-cos-method-for-heston/">previous</a> <a href="/post/attari-lord-kahl--cos-methods-comparison-on-heston/">blog</a> <a href="/post/making-classic-heston-integration-faster-than-the-cos-method/">posts</a>. A main advantage of the COS method is its simplicity. But this comes at the expense of finding the correct values for the truncation level and the (associated) number of terms.</p>
<p>A related issue of the COS method, or its more fancy wavelet cousin the SWIFT method, is to require a huge (&gt;65K) number of points to reach a reasonable accuracy for some somewhat extreme choices of Heston parameters. I provide an example in <a href="https://arxiv.org/abs/2401.01758">a recent paper</a> (see Section 5).</p>
<p>Gero Junike recently wrote <a href="https://arxiv.org/abs/2109.01030">several</a> <a href="https://arxiv.org/abs/2303.16012">papers</a> on how to find good estimates for those two parameters. Gero derives a slightly different formula for the put option, by centering the distribution on \( \mathbb{E}[\ln S] \). It is closer to my own <a href="https://arxiv.org/abs/2005.13248">improved COS formula</a>, where I center the integration on the forward. The estimate for the truncation is larger than the one we are used to (for example using the estimate based on 4 cumulants of Mike Staunton), and the number of points is very conservative.</p>
<p>The bigger issue with this new estimate, is that it relies on an integration of a function of the characteristic function, very much like the original problem we are trying to solve (the price of a vanilla option). This is in order to estimate the \( ||f^{(20)}||_{\infty} \). Interestingly, evaluating this integral is not always trivial, the <a href="https://github.com/machakann/DoubleExponentialFormulas.jl">double exponential quadrature in Julia</a> fails. I found that reusing the transform from \( (0,\infty) \) to (-1,1) of Andersen and Lake along with a Gauss-Legendre quadrature on 128 points seemed to be ok (at least for some values of the Heston parameters, it may be fragile, not sure).</p>
<p>While very conservative, it seems to produce the desired accuracy on the extreme example mentioned in the paper, it leads to N=756467 points and a upper truncation at b=402.6 for a relative tolerance of 1E-4. Of course, on such examples the COS method is not fast anymore. For comparison, the <a href="https://fbe.unimelb.edu.au/__data/assets/pdf_file/0008/2591783/223.pdf">Joshi-Yang technique</a> with 128 points produces the same accuracy in 235 μs as the COS method in 395 ms on this example, that is a factor of 1000 (on many other examples the COS method behaves significantly better of course).</p>
<p>Furthermore, as stated in Gero Junike&rsquo;s paper, the estimate fails for less smooth distributions such as the one of the Variance Gamma (VG) model.</p>

  
</article>
</div>

<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/2/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
