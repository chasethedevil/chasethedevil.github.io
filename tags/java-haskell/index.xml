<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java Haskell on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/java-haskell/</link>
    <description>Recent content in Java Haskell on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Mar 2013 12:20:00 +0000</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/tags/java-haskell/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>http://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>&lt;p&gt;&lt;br /&gt;In my &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/03/a-double-precision-puzzle-with-gaussian.html&#34;&gt;previous post&lt;/a&gt;, I stated that some library (SPECFUN by W.D. Cody) computes $$e^{-\frac{x^2}{2}}$$ the following way:&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;xsq = fint(x * 1.6) / 1.6;&lt;br /&gt;del = (x - xsq) * (x + xsq);&lt;br /&gt;result = exp(-xsq * xsq * 0.&lt;span style=&#34;font-size: x-small;&#34;&gt;5&lt;/span&gt;) * exp(-del &lt;em&gt;&amp;nbsp;&lt;span style=&#34;font-size: x-small;&#34;&gt;0.5&lt;/span&gt;);&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;where fint(z) computes the floor of z.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;1. Why 1.6?&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.6 is equivalent to multiplying by 10 and dividing by 16 which is an exact operation). It also allows to have something very close to a rounding function: x=2.6 will make xsq=2.5, x=2.4 will make xsq=1.875, x=2.5 will make xsq=2.5. The maximum difference between x and xsq will be 0.625.&lt;br /&gt;&lt;br /&gt;&lt;div&gt;&lt;b&gt;2. (a-b)&lt;/em&gt;(a+b) decomposition&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;del is of the order of 2&lt;em&gt;x&lt;/em&gt;(x-xsq). When (x-xsq) is very small, del will, most of the cases be small as well: when x is too high (beyond 39), the result will always be 0, because there is no small enough number to represent exp(-0.5*39*39) in double precision, while (x-xsq) can be as small as machine epsilon (around 2E-16). By splitting x*x into xsq*xsq and del, one allow exp to work on a more refined value of the remainder del, which in turn should lead to an increase of accuracy.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;b&gt;3. Real world effect&lt;/b&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Let&amp;rsquo;s make x move by machine epsilon and see how the result varies using the naive implementation exp(-0.5*x*x) and using the refined Cody way. We take x=20, and add machine epsilon a number of times (frac).&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-ViyG79syS2w/UUrmdbNcuQI/AAAAAAAAGSY/AIjw6PEgvMw/s1600/snapshot1.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://3.bp.blogspot.com/-ViyG79syS2w/UUrmdbNcuQI/AAAAAAAAGSY/AIjw6PEgvMw/s400/snapshot1.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;The staircase happens because if we add machine epsilon to 20, this results in the same 20, until we add it enough to describe the next number in double precision accuracy. But what&amp;rsquo;s interesting is that Cody staircase is regular, the stairs have similar height while the Naive implementation has stairs of uneven height.&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;This is the relative error between the Naive implementation and Cody. The difference is higher than one could expect: a factor of 20. But it has one big drawbacks: it requires 2 exponential evaluations, which are relatively costly.&amp;nbsp;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;b&gt;Update March 22, 2013&lt;/b&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;I looked for a higher precision exp implementation, that can go beyond double precision. I found an online calculator (not so great to do tests on), and after more search, I found one very simple way: mpmath python library.&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;I did some initial tests with the calculator and thought Cody was in reality not much better than the Naive implementation. The problem is that my tests were wrong, because the online calculator expects an input in terms of human digits, and I did not always use the correct amount of digits. For example a double of -37.7 is actually -37.7000000000000028421709430404007434844970703125.&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;Here is a plot of the relative error of our methods compared to the high accuracy python implementation, but using as input strict double numbers around x=20. The horizontal axis is x-20, the vertical is the relative error.&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-uZLpckLIIkM/UUxdN2B04dI/AAAAAAAAGT4/1preYocfLt0/s1600/snapshot5.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;291&#34; src=&#34;http://1.bp.blogspot.com/-uZLpckLIIkM/UUxdN2B04dI/AAAAAAAAGT4/1preYocfLt0/s400/snapshot5.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;We can see that Cody is really much more accurate (more than 20x). The difference will be lower when x is smaller, but there is still a factor 10 around x=-5.7&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&amp;nbsp;&lt;a href=&#34;http://4.bp.blogspot.com/-G1H1YTQCvtY/UUxjYNOP1eI/AAAAAAAAGUA/2YaXO1NscmU/s1600/snapshot7.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;265&#34; src=&#34;http://4.bp.blogspot.com/-G1H1YTQCvtY/UUxjYNOP1eI/AAAAAAAAGUA/2YaXO1NscmU/s400/snapshot7.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;Any calculation using a Cody like Gaussian density implementation, will likely not be as careful as this, so one can doubt of the usefulness in practice of such accuracy tricks.&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;The Cody implementation uses 2 exponentials, which can be costly to evaluate, however Gary commented out that we can cache the exp xsq because of fint and therefore have accuracy and speed.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Haskell Fibonacci Revisited</title>
      <link>http://chasethedevil.github.io/post/haskell-fibonacci-revisited/</link>
      <pubDate>Wed, 12 Dec 2007 17:08:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/haskell-fibonacci-revisited/</guid>
      <description>&lt;p&gt;Recently, there was an &lt;a href=&#34;http://cgi.cse.unsw.edu.au/~dons/blog/2007/11/29#smoking-4core&#34;&gt;interesting post&lt;/a&gt; about Haskell performance and Haskell parallelization showing Haskell could outperform C on a simple Fibonacci example.&lt;br /&gt;&lt;br /&gt;A friend of mine, Peter (that I seem to manage to constantly piss off) thought about it on another level, saying you could achieve a &lt;em&gt;MILLION&lt;/em&gt; times better using a direct formula in C or Java, &lt;a href=&#34;http://www.mcs.surrey.ac.uk/Personal/R.Knott/Fibonacci/fibFormula.html#formula&#34;&gt;the Binet formula&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;I decided to try as the improvement scale seemed a bit surprising. I first compared a Java recursive fibonacci with a Haskell one. Here are the results for Haskell GHC 6.6.1 vs Java 1.6.0 on Linux for fib(44):&lt;br /&gt;&lt;a onblur=&#34;try {parent.deselectBloggerImageGracefully();} catch(e) {}&#34; href=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AevOMNqtI/AAAAAAAABI4/Ju0-GjX6W14/s1600-h/simple_haskell.png&#34;&gt;&lt;img style=&#34;display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;&#34; src=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AevOMNqtI/AAAAAAAABI4/Ju0-GjX6W14/s320/simple_haskell.png&#34; border=&#34;0&#34; alt=&#34;&#34;id=&#34;BLOGGER_PHOTO_ID_5143144571069115090&#34; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;a onblur=&#34;try {parent.deselectBloggerImageGracefully();} catch(e) {}&#34; href=&#34;http://1.bp.blogspot.com/_9RyqGT46Fbk/R2AeveMNquI/AAAAAAAABJA/DJDx3Vw0LyU/s1600-h/simple_java.png&#34;&gt;&lt;img style=&#34;display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;&#34; src=&#34;http://1.bp.blogspot.com/_9RyqGT46Fbk/R2AeveMNquI/AAAAAAAABJA/DJDx3Vw0LyU/s320/simple_java.png&#34; border=&#34;0&#34; alt=&#34;&#34;id=&#34;BLOGGER_PHOTO_ID_5143144575364082402&#34; /&gt;&lt;/a&gt;&lt;br /&gt;Then I decided to check out the time for fib(44) or any fib at all, I was unable to measure precisely enough since it always came out as 0ms, in Haskell, or in Java. Looping out 10 million times, Java gave out 7.3s and Haskell something similar (but my method to loop 10 million times in Haskell is probably very bad).&lt;br /&gt;&lt;a onblur=&#34;try {parent.deselectBloggerImageGracefully();} catch(e) {}&#34; href=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AnZOMNqvI/AAAAAAAABJI/DHpUw2wMiTc/s1600-h/binet_haskell.png&#34;&gt;&lt;img style=&#34;display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;&#34; src=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AnZOMNqvI/AAAAAAAABJI/DHpUw2wMiTc/s320/binet_haskell.png&#34; border=&#34;0&#34; alt=&#34;&#34;id=&#34;BLOGGER_PHOTO_ID_5143154088716643058&#34; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;a onblur=&#34;try {parent.deselectBloggerImageGracefully();} catch(e) {}&#34; href=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AnZOMNqwI/AAAAAAAABJQ/dkgIlZuXSPE/s1600-h/binet_java.png&#34;&gt;&lt;img style=&#34;display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;&#34; src=&#34;http://4.bp.blogspot.com/_9RyqGT46Fbk/R2AnZOMNqwI/AAAAAAAABJQ/dkgIlZuXSPE/s320/binet_java.png&#34; border=&#34;0&#34; alt=&#34;&#34;id=&#34;BLOGGER_PHOTO_ID_5143154088716643074&#34; /&gt;&lt;/a&gt;&lt;br /&gt;The original post actually points to a link that describes various algorithms for Fibonacci. They basically say that for large n, the rounding is not precise enough, they also propose algorithms in log(n). I tried and was really impressed by the performance of those algorithms. Again I could not measure the difference for a single calculation between it and the binet formula as elapsed time is always 0. The binet formula becomes inexact already at n=71 in Java with doubles.&lt;br /&gt;&lt;a onblur=&#34;try {parent.deselectBloggerImageGracefully();} catch(e) {}&#34; href=&#34;http://2.bp.blogspot.com/_9RyqGT46Fbk/R2AoLuMNqxI/AAAAAAAABJY/o0Z4pI0ELPY/s1600-h/fancy_haskell.png&#34;&gt;&lt;img style=&#34;display:block; margin:0px auto 10px; text-align:center;cursor:pointer; cursor:hand;&#34; src=&#34;http://2.bp.blogspot.com/_9RyqGT46Fbk/R2AoLuMNqxI/AAAAAAAABJY/o0Z4pI0ELPY/s320/fancy_haskell.png&#34; border=&#34;0&#34; alt=&#34;&#34;id=&#34;BLOGGER_PHOTO_ID_5143154956300036882&#34; /&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;Of course the original post is still quite interesting, it shows how easy it can be to parallelize calculations in Haskell. But the example is silly as another algorithm can lead to 10 millions times the performance. Still Haskell performs well with the shit or good algorithm when compared to Java.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>