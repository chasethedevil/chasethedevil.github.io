<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>finance on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/finance/</link>
    <description>Recent content in finance on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Tue, 30 Apr 2013 17:05:00 +0000</lastBuildDate><atom:link href="https://chasethedevil.github.io/tags/finance/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Upper Bounds in American Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</link>
      <pubDate>Tue, 30 Apr 2013 17:05:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</guid>
      <description>Glasserman and Yu (GY) give a relatively simple algorithm to compute lower and upper bounds of a the price of a Bermudan Option through Monte-Carlo.
I always thought it was very computer intensive to produce an upper bound, and that the standard Longstaff Schwartz algorithm was quite precise already. GY algorithm is not much slower than the Longstaff-Schwartz algorithm, but what&#39;s a bit tricky is the choice of basis functions: they have to be Martingales.</description>
    </item>
    
    <item>
      <title>Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</link>
      <pubDate>Mon, 22 Apr 2013 18:00:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</guid>
      <description>In the book Monte Carlo Methods in Financial Engineering, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.
However Longstaff and Schwartz seem to just reuse the paths in their paper, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.</description>
    </item>
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.
Can we improve the speed of exp over the JDK one?</description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>Marsaglia in his paper on Normal Distribution made the same mistake I initially did while trying to verify the accuracy of the normal density.In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.</description>
    </item>
    
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>In my previous post, I stated that some library (SPECFUN by W.D. Cody) computes $$e^{-\frac{x^2}{2}}$$ the following way:xsq = fint(x * 1.6) / 1.6;del = (x - xsq) * (x + xsq);result = exp(-xsq * xsq * 0.5) * exp(-del 0.5);where fint(z) computes the floor of z.1. Why 1.6?An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.</description>
    </item>
    
    <item>
      <title>Local Volatility Delta &amp; Dynamic</title>
      <link>https://chasethedevil.github.io/post/local-volatility-delta--dynamic/</link>
      <pubDate>Thu, 29 Nov 2012 12:30:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/local-volatility-delta--dynamic/</guid>
      <description>This will be a very technical post, I am not sure that it will be very understandable by people not familiar with the implied volatility surface.
Something one notices when computing an option price under local volatility using a PDE solver, is how different is the Delta from the standard Black-Scholes Delta, even though the price will be very close for a Vanilla option. In deed, the Finite difference grid will have a different local volatility at each point and the Delta will take into account a change in local volatility as well.</description>
    </item>
    
    <item>
      <title>GPU computing in Finance</title>
      <link>https://chasethedevil.github.io/post/gpu-computing-in-finance/</link>
      <pubDate>Mon, 15 Oct 2012 16:14:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/gpu-computing-in-finance/</guid>
      <description>Very interesting presentation from Murex about their GPU computing. Some points were:
GPU demand for mostly exotics pricing and greeks Local vol main model for EQD exotics. Local vol calibrated via PDE approach. Markov functional model becoming main model for IRD. Use of local regression instead of Longstaff Schwartz (or worse CVA like sim of sim). philox RNG from DE Shaw. But the presenter does not seem to know RNGs very well (recommended Brownian Bridge for Mersenne Twister!</description>
    </item>
    
    <item>
      <title>Binary Voting</title>
      <link>https://chasethedevil.github.io/post/binary-voting/</link>
      <pubDate>Fri, 07 Sep 2012 17:21:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/binary-voting/</guid>
      <description>How many reports have you had to fill up with a number of stars to choose? How much useless time is spent on figuring the this number just because it is always very ambiguous?
Some blogger wrote an interesting entry on Why I Hate Five Stars Reviews. Basically he advocates binary voting instead via like/dislike. Maybe a ternary system via like/dislike/don&#39;t care would be ok too.
One coworker used to advocate the same for a similar reason: people reading those reports only pay attention to the extremes: the 5 stars or the 0 stars.</description>
    </item>
    
    <item>
      <title>Adaptive Quadrature for Pricing European Option with Heston</title>
      <link>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</link>
      <pubDate>Mon, 25 Jun 2012 12:50:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</guid>
      <description>The Quantlib code to evaluate the Heston integral for European options is quite nice. It proposes Kahl &amp;amp; Jaeckel method as well as Gatheral method for the complex logarithm. It also contains expansions where it matters so that the resulting code is very robust. One minor issue is that it does not integrate both parts at the same time, and also does not propose Attari method for the Heston integral that is supposed to be more stable.</description>
    </item>
    
    <item>
      <title>Why primitive arrays matter in Java</title>
      <link>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</link>
      <pubDate>Wed, 29 Feb 2012 10:01:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</guid>
      <description>In the past, I have seen that one could greatly improve performance of some Monte-Carlo simulation by using as much as possible double[][] instead of arrays of objects.
It was interesting to read this blog post explaining why that happens: it is all about memory access.</description>
    </item>
    
    <item>
      <title>Generating random numbers following a given discrete probability distribution</title>
      <link>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</link>
      <pubDate>Mon, 09 Jan 2012 00:14:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</guid>
      <description>I have never really thought very much about generating random numbers according to a precise discrete distribution, for example to simulate an unfair dice. In finance, we are generally interested in continuous distributions, where there is typically 2 ways: the inverse transform (usually computed in a numerical way), and the acceptance-rejection method (typically the ziggurat. The inverse transform is often preferred, because it&amp;rsquo;s usable method for Quasi Monte-Carlo simulations while the acceptance rejection is not.</description>
    </item>
    
    <item>
      <title>Quant Interview &amp; Education</title>
      <link>https://chasethedevil.github.io/post/quant-interview--education/</link>
      <pubDate>Wed, 21 Dec 2011 17:37:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quant-interview--education/</guid>
      <description>Recently, I interviewed someone for a quant position. I was very surprised to find out that someone who did one of the best master in probabilities and finance in France could not solve a very basic probability problem:
This is accessible to someone with very little knowledge of probabilities When I asked this problem around to co-workers (who have all at least a master in a scientific subject), very few could actually answer it properly.</description>
    </item>
    
    <item>
      <title>Good &amp; Popular Algorithms are Simple</title>
      <link>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</link>
      <pubDate>Thu, 17 Nov 2011 12:28:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</guid>
      <description>I recently tried to minimise a function according to some constraints. One popular method to minimise a function in several dimensions is Nelder-Mead Simplex. It is quite simple, so simple that I programmed it in Java in 1h30, including a small design and a test. It helped that the original paper from Nelder-Mead is very clear:
However the main issue is that it works only for unconstrained problems. Nelder and Mead suggested to add a penalty, but in practice this does not work so well.</description>
    </item>
    
    <item>
      <title>SIMD and Mersenne-Twister</title>
      <link>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</link>
      <pubDate>Sat, 05 Feb 2011 13:18:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</guid>
      <description>Since 2007, there is a new kind of Mersenne-Twister (MT) that exploits SIMD architecture, the SFMT. The Mersenne-Twister has set quite a standard in random number generation for Monte-Carlo simulations, even though it has flaws.
I was wondering if SFMT improved the performance over MT for a Java implementation. There is actually on the same page a decent Java port of the original algorithm. When I ran it, it ended up slower by more than 20% than the classical Mersenne-Twister (32-bit) on a 64-bit JDK 1.</description>
    </item>
    
    <item>
      <title>The CUDA Performance Myth</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth/</link>
      <pubDate>Mon, 03 Jan 2011 16:07:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth/</guid>
      <description>There is an interesting article on how to generate efficiently the inverse of the normal cumulative distribution on the GPU. This is useful for Monte-Carlo simulations based on normally distributed variables.
Another result of the paper is a method (breakless algorithm) to compute it apparently faster than the very good Wichura&amp;rsquo;s AS241 algorithm on the CPU as well keeping a similar precision. The key is to avoid branches (if-then) at the cost of not avoiding log() calls.</description>
    </item>
    
    <item>
      <title>Another Look at Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</link>
      <pubDate>Mon, 29 Nov 2010 12:45:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</guid>
      <description>A while ago, I was already looking for a good Java Matrix library, complaining that there does not seem any real good one where development is still active: the 2 best ones are in my opinion Jama and Colt.
Recently I tried to price options via RBF (radial basis functions) based on TR-BDF2 time stepping. This is a problem where one needs to do a few matrix multiplications and inverses (or better, LU solve) in a loop.</description>
    </item>
    
    <item>
      <title>Hull American Option Price Fallacies</title>
      <link>https://chasethedevil.github.io/post/hull-american-option-price-fallacies/</link>
      <pubDate>Fri, 15 May 2009 16:01:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/hull-american-option-price-fallacies/</guid>
      <description>Hull says American put is best exercised immediately and american call is optimal at expiry like a european. Is this really true?
At first it seems really clever and model show clearly this. But if we change the market assumptions only a tiny bit, everything falls down.
I could not detail everything in a blog post so I created a static web page about it. Everything was produced in Java using algorithm found in popular books and graphs through JFreeChart.</description>
    </item>
    
    <item>
      <title>Bachelier vs. Black</title>
      <link>https://chasethedevil.github.io/post/bachelier-vs.-black/</link>
      <pubDate>Mon, 23 Mar 2009 17:58:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/bachelier-vs.-black/</guid>
      <description>Black and Scholes gives a strange result for the price of a binary option under high volatility. You will learn here how to simulate a stock price evolution using Java, and how to show it using JFreeChart library. It starts with more complex concepts (don&#39;t be afraid) and goes done towards simpler things.
I could not write all that in a blog format, so I created a old HTML page about it here and a PDF version.</description>
    </item>
    
  </channel>
</rss>
