<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Programming Gpu on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/programming-gpu/</link>
    <description>Recent content in Programming Gpu on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Aug 2011 10:20:00 +0000</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/tags/programming-gpu/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Carmack &amp; GPGPU programming</title>
      <link>http://chasethedevil.github.io/post/carmack--gpgpu-programming/</link>
      <pubDate>Sun, 14 Aug 2011 10:20:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/carmack--gpgpu-programming/</guid>
      <description>&lt;p&gt;Finally someone who shares the same opinion on the current state of GPGPU programming. &lt;br /&gt;&lt;br /&gt;John Carmack: On the other hand, we have converted all of our offline processing stuff to ray tracing. For years, the back-end MegaTexture generation for Rage was done with&amp;hellip; we had a GPGPU cluster with NVIDIA cards and it was such a huge pain to keep. It was an amazing pain where one system would be having heat problems and would be behaving weird even though we thought they had identical drivers. Something would always be wrong with render farm 12, and whenever we wanted to put in new features it was like “Okay, writing new fragment programs to go into this.” Now, granted I did this just when CUDA was in its infancy. If I did re-implement it with OpenCL or CUDA we wouldn’t have some of these problems, but when I converted all these over to ray tracing there was a number of things that got a lot better. Things that we deal with, [for example] shadows and reflections that have to be approximated, and were so used to doing with rasterization&amp;hellip; we sometimes forget how big of hacks these are. To be able to say I really just want that ray, and tell me what it hit; not do a projection with feathering shadowed edges and whatever the heck else we’re doing there, so much of the code got so much easier. If it’s a choice of&amp;hellip; now that we have these awesome multi-core x86 CPUs where we can get 24 threads in commodity boxes&amp;hellip; it’s true that one GPU card can do more ray tracing than one 24 thread x86 box, but it’s not multiples more and if it’s just a matter of buying more $2000 boxes, it makes the development, maintenance, and upkeep much better. While everyone in high performance computing is all “rah-rah” GPUs right now, I’ve come full circle back around to saying the fact that we can get massive amounts of x86 cores and threads&amp;hellip; it wont win on FLOPS/watt or FLOPS/volume, but in terms of results per developer hour it is much, much better.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>