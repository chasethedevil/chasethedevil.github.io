<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Scala on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/scala/</link>
    <description>Recent content in Scala on Chase the Devil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Sat, 18 Apr 2015 22:58:00 +0000</lastBuildDate>
    <atom:link href="https://chasethedevil.github.io/tags/scala/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Modern Programming Language for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</link>
      <pubDate>Sat, 18 Apr 2015 22:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</guid>
      <description>&lt;p&gt;A few recent programming languages sparked my interest:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://julialang.org/&#34;&gt;Julia&lt;/a&gt; because of the wide coverage of mathematical functions, and great attention to quality of the implementations. It has also some interesting web interface.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.dartlang.org&#34;&gt;Dart&lt;/a&gt;: because it&amp;rsquo;s a language focused purely on building apps for the web, and has a supposedly good VM.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;: it&amp;rsquo;s the latest fad. It has interesting concepts around concurrency and a focus on being low level all the while being simpler than C.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I decided to see how well suited they would be on a simple Monte-Carlo simulation of a forward start option under the Black model. I am no expert at all in any of the languages, so this is a beginner&amp;rsquo;s test. I compared the runtime for executing 16K simulations times a multiplier.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flat Volatility Surfaces &amp; Discrete Dividends</title>
      <link>https://chasethedevil.github.io/post/flat-volatility-surfaces--discrete-dividends/</link>
      <pubDate>Tue, 25 Nov 2014 13:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/flat-volatility-surfaces--discrete-dividends/</guid>
      <description>In papers around volatility and cash (discrete) dividends, we often encounter the example of the flat volatility surface. For example, the &lt;a href=&#34;http://www.opengamma.com/sites/default/files/equity-variance-swaps-dividends-opengamma.pdf&#34;&gt;OpenGamma paper&lt;/a&gt; presents this graph:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-AuaTFyvjgVA/VHRxUid4HzI/AAAAAAAAHjs/T4PAQTnUBN8/s1600/Screenshot%2Bfrom%2B2014-11-25%2B12%3A59%3A09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-AuaTFyvjgVA/VHRxUid4HzI/AAAAAAAAHjs/T4PAQTnUBN8/s1600/Screenshot%2Bfrom%2B2014-11-25%2B12%3A59%3A09.png&#34; height=&#34;167&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It shows that if the Black volatility surface is fully flat, there are jumps in the pure volatility surface (corresponding to a process that includes discrete dividends in a consistent manner) at the dividend dates or equivalently if the pure volatility surface is flat, the Black volatility jumps.&lt;br /&gt;&lt;br /&gt;This can be traced to the fact that the Black formula does not respect C(S,K,Td-) = C(S,K-d,Td) as the forward drops from F(Td-) to F(Td-)-d where d is dividend amount at td, the dividend ex date.&lt;br /&gt;&lt;br /&gt;Unfortunately, those examples are not very helpful. In practice, the market observables are just Black volatility points, which can be interpolated to volatility slices for each expiry without regards to dividends, not a full volatility surface. Discrete dividends will mostly happen between two slices: the Black volatility jump will happen on some time-interpolated data.&lt;br /&gt;&lt;br /&gt;While the jump size is known (it must obey to the call price continuity), the question of how one should interpolate that data until the jump is far from trivial even using two flat Black volatility slices.&lt;br /&gt;&lt;br /&gt;The most logical is to consider a model that includes discrete dividends consistently. For example, one can fully lookup the Black volatility corresponding the price of an option assuming a piecewise lognormal process with jumps at the dividend dates. It can be priced by applying a finite difference method on the PDE. Alternatively, &lt;a href=&#34;http://www.risk.net/risk-magazine/technical-paper/1530307/finessing-fixed-dividends&#34;&gt;Bos &amp;amp; Vandermark&lt;/a&gt; propose a simple spot and strike adjusted Black formula that obey the continuity requirement (the Lehman model), which, in practice, stays quite close to the piecewise lognormal model price. Another possibility is to rely on a forward modelling of the dividends, as in &lt;a href=&#34;http://www.quantitative-research.de/dl/Dividends_And_Volatility.pdf&#34;&gt;Buehler&lt;/a&gt; (if one is comfortable with the idea that the option price will then depend ultimately on dividends past the option expiry).&lt;br /&gt;&lt;br /&gt;Recently, a &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/wilm.10112/abstract&#34;&gt;Wilmott article&lt;/a&gt; suggested to only rely on the jump adjustment, but did not really mention how to find the volatility just before or just after the dividend. Here is an illustration of how those assumptions can change the volatility in between slices using two dividends at T=0.9 and T=1.1.&lt;br /&gt;&lt;br /&gt;In the first graph, we just interpolate linearly in forward moneyness the pure vol from the Bos &amp;amp; Vandermark formula, as it should be continuous with the forward (the PDE would give nearly the same result) and compute the equivalent Black volatility (and thus the jump at the dividend dates).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-Pye5KeoR16M/VHR1WACQD3I/AAAAAAAAHj4/h65Vpj4mMjI/s1600/bos_2_div_flat.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-Pye5KeoR16M/VHR1WACQD3I/AAAAAAAAHj4/h65Vpj4mMjI/s1600/bos_2_div_flat.png&#34; height=&#34;300&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the second graph, we interpolate linearly the two Black slices, until we find a dividend, at which point we impose the jump condition and repeat the process until the next slice. We process forward (while the Wilmott article processes backward) as it seemed a bit more natural to make the interpolation not depend on future dividends. Processing backward would just make the last part flat and first part down-slopping. On this example backward would be closer to the Bos Black volatility, but when the dividends are near the first slice, the opposite becomes true.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-ScSlBHCBoWc/VHR1eOigrXI/AAAAAAAAHkA/3HJ9zRQvguA/s1600/blackjump_2_div_flat.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-ScSlBHCBoWc/VHR1eOigrXI/AAAAAAAAHkA/3HJ9zRQvguA/s1600/blackjump_2_div_flat.png&#34; height=&#34;300&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;While the scale of those changes is not that large on the example considered, the choice can make quite a difference in the price of structures that depend on the volatility in between slices. A recent example I encountered is the variance swap when one includes adjustment for discrete dividends (then the prices just after the dividend date are used).&lt;br /&gt;&lt;br /&gt;To conclude, if one wants to use the classic Black formula everywhere, the volatility must jump at the dividend dates. Interpolation in time is then not straightforward and one will need to rely on a consistent model to interpolate. It is not exactly clear then why would anyone stay with the Black formula except familiarity.</description>
    </item>
    <item>
      <title>Machine Learning &amp; Quantitative Finance</title>
      <link>https://chasethedevil.github.io/post/machine-learning--quantitative-finance/</link>
      <pubDate>Tue, 18 Nov 2014 12:34:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/machine-learning--quantitative-finance/</guid>
      <description>&lt;p&gt;There is an interesting course on &lt;a href=&#34;https://class.coursera.org/ml-007/lecture&#34;&gt;Machine Learning on Coursera&lt;/a&gt;, it does not require much knowledge and yet manages to teach quite a lot.&lt;/p&gt;&#xA;&lt;p&gt;I was struck by the fact that most techniques and ideas apply also to problems in quantitative finance.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Linear regression: used for example in the Longstaff-Schwartz approach to price Bermudan options with Monte-Carlo. Interestingly the teacher insists on feature normalization, something we can forget easily, especially with the polynomial features.&lt;/li&gt;&#xA;&lt;li&gt;Gradient descent: one of the most basic minimizer and we use minimizers all the time for model calibration.&lt;/li&gt;&#xA;&lt;li&gt;Regularization: in finance, this is sometimes used to smooth out the volatility surface, or can be useful to add stability in calibration. The lessons are very practical, they explain well how to find the right value of the regularization parameter.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;Neural networks&lt;/a&gt;: calibrating a model is very much like training a neural network. The &lt;a href=&#34;http://en.wikipedia.org/wiki/Backpropagation&#34;&gt;backpropagation&lt;/a&gt; is the same thing as the adjoint differentiation. It&amp;rsquo;s very interesting to see that it is a key feature for Neural networks, otherwise training would be much too slow and Neural networks would not be practical. Once the network is trained, it is evaluated relatively quickly forward. It&amp;rsquo;s basically the same thing as calibration and then pricing.&lt;/li&gt;&#xA;&lt;li&gt;Support vector machines: A gaussian kernel is often used to represent the frontier. We find the same idea in the particle Monte-Carlo method.&lt;/li&gt;&#xA;&lt;li&gt;Principal component analysis: can be applied to the covariance matrix square root in Monte-Carlo simulations, or to &amp;ldquo;compress&amp;rdquo; large baskets, as well as for portfolio risk.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;It&amp;rsquo;s also interesting to hear the teacher repeating that people should not try possible improvements at random (often because they have only one idea) but analyze before what makes the most sense. And that can imply digging in the details, looking at what&amp;rsquo;s going on 100 samples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pseudo-Random vs Quasi-Random Numbers</title>
      <link>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</link>
      <pubDate>Wed, 12 Nov 2014 17:05:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</guid>
      <description>Quasi-Random numbers (like &lt;a href=&#34;http://en.wikipedia.org/wiki/Sobol_sequence&#34;&gt;Sobol&lt;/a&gt;) are a relatively popular way in finance to improve the Monte-Carlo convergence compared to more classic Pseudo-Random numbers (like &lt;a href=&#34;http://en.wikipedia.org/wiki/Mersenne_twister&#34;&gt;Mersenne-Twister&lt;/a&gt;). Behind the scenes one has to be a bit more careful about the dimension of the problem as the Quasi-Random numbers depends on the dimension (defined by how many random variables are independent from each other).&lt;br /&gt;&lt;br /&gt;For a long time, Sobol was limited to 40 dimensions using the so called Bratley-Fox direction numbers (his paper actually gives the numbers for 50 dimensions). Later Lemieux gave direction numbers for up to 360 dimensions. Then, P. Jäckel proposed some extension with a random initialization of the direction vectors in his book from 2006. And finally Joe &amp;amp; Kuo published direction numbers for up to 21200 dimensions.&lt;br /&gt;&lt;br /&gt;But there are very few studies about how good are real world simulations with so many quasi-random dimensions. A recent paper &#34;&lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2210420&#34;&gt;Fast Ninomiya-Victoir Calibration of the Double-Mean-Reverting Model&lt;/a&gt;&#34; by Bayer, Gatheral &amp;amp; Karlsmark tests this for once, and the results are not so pretty:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;https://3.bp.blogspot.com/-aYBusg02Kr0/VGOAlrsHGjI/AAAAAAAAHis/o4zfFf8-5hA/s1600/Screenshot%2Bfrom%2B2014-11-12%2B16%3A15%3A17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;342&#34; src=&#34;https://3.bp.blogspot.com/-aYBusg02Kr0/VGOAlrsHGjI/AAAAAAAAHis/o4zfFf8-5hA/s640/Screenshot%2Bfrom%2B2014-11-12%2B16%3A15%3A17.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;With their model, the convergence with Sobol numbers becomes worse when the number of time-steps increases, that is when the number of dimension increases. There seems to be even a threshold around 100 time steps (=300 dimensions for Euler) beyond which a much higher number of paths (2^13) is necessary to restore a proper convergence. And they use the latest and greatest Joe-Kuo direction numbers.&lt;br /&gt;&lt;br /&gt;Still the total number of paths is not that high compared to what I am usually using (2^13 = 8192). It&#39;s an interesting aspect of their paper: the calibration with a low number of paths.</description>
    </item>
    <item>
      <title>Integrating an oscillatory function</title>
      <link>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</link>
      <pubDate>Wed, 05 Nov 2014 16:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</guid>
      <description>Recently, some instabilities were noticed in the Carr-Lee seasoned volatility swap price in some situations. &lt;br /&gt;&lt;br /&gt;The &lt;a href=&#34;https://math.nyu.edu/financial_mathematics/content/02_financial/2008-3.pdf&#34;&gt;Carr-Lee&lt;/a&gt; seasoned volatility swap price involve the computation of a double integral. The inner integral is really the problematic one as the integrand can be highly oscillating.&lt;br /&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-9Fh24CvDs_4/VFpCjz8_sMI/AAAAAAAAHig/Q0iTCTb3f9E/s1600/Screenshot%2Bfrom%2B2014-11-05%2B16%3A30%3A00.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-9Fh24CvDs_4/VFpCjz8_sMI/AAAAAAAAHig/Q0iTCTb3f9E/s1600/Screenshot%2Bfrom%2B2014-11-05%2B16%3A30%3A00.png&#34; height=&#34;151&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;br /&gt;I&amp;nbsp; first found a somewhat stable behavior using a specific adaptive Gauss-Lobatto implementation (&lt;a href=&#34;http://www.ii.uib.no/%7Eterje/Papers/bit2003.pdf&#34;&gt;the one from Espelid&lt;/a&gt;) and a change of variable. But it was not very satisfying to see that the outer integral was stable only with another specific adaptive Gauss-Lobatto (the one from Gander &amp;amp; Gauschi, present in Quantlib). I tried various choices of adaptive (coteda, modsim, adaptsim,...) or brute force trapezoidal integration, but either they were order of magnitudes slower or unstable in some cases. Just using the same Gauss-Lobatto implementation for both would fail...&lt;br /&gt;&lt;br /&gt;I then noticed you could write the integral as a Fourier transform as well, allowing the use of FFT. Unfortunately, while this worked, it turned out to require a very large number of points for a reasonable accuracy. This, plus the tricky part of defining the proper step size, makes the method not so practical.&lt;br /&gt;&lt;br /&gt;I had heard before of the &lt;a href=&#34;http://www.cs.berkeley.edu/~fateman/papers/oscillate.pdf&#34;&gt;Filon quadrature&lt;/a&gt;, which I thought was more of a curiosity. The main idea is to integrate exactly x^n * cos(k*x). One then relies on a piecewise parabolic approximation of the function f to integrate f(x) * cos(k*x). Interestingly, a very similar idea has been used in the &lt;a href=&#34;http://www.risk.net/risk-magazine/technical-paper/1500323/cutting-edges-domain-integration&#34;&gt;Sali quadrature method&lt;/a&gt; for option pricing, except one integrates exactly x^n * exp(-k*x^2).&lt;br /&gt;&lt;br /&gt;It turned out to be remarkable on that problem, combined with a &lt;a href=&#34;http://en.wikipedia.org/wiki/Adaptive_Simpson%27s_method&#34;&gt;simple adaptive Simpson&lt;/a&gt; like method to find the right discretization. Then as if by magic, any outer integration quadrature worked. &lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Barrier options under negative rates: complex numbers to the rescue</title>
      <link>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</link>
      <pubDate>Thu, 02 Oct 2014 11:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</guid>
      <description>I stumbled upon an unexpected problem: the &lt;a href=&#34;http://books.google.com/books?id=FU7gam7ZqVsC&amp;amp;q=haug+binary+barrier&amp;amp;dq=haug+binary+barrier&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=QyAtVITAGdjdatPxgMAO&amp;amp;ved=0CB0Q6AEwAA&#34;&gt;one touch barrier formula&lt;/a&gt; can break down under negative rates. While negative rates can sound fancy, they are actually quite real on some markets. Combined with relatively low volatilities, this makes the standard Black-Scholes one touch barrier formula blow up because somewhere the square root of a negative number is taken.&lt;br /&gt;&lt;br /&gt;At first, I had the idea to just floor the number to 0. But then I needed to see if this rough approximation would be acceptable or not. So I relied on a &lt;a href=&#34;http://www.risk.net/journal-of-computational-finance/technical-paper/2330321/tr-bdf2-for-fast-stable-american-option-pricing&#34;&gt;TR-BDF2&lt;/a&gt; discretization of the Black-Scholes PDE, where negative rates are not a problem.&lt;br /&gt;&lt;br /&gt;Later, I was convinced that we ought to be able to find a closed form formula for the case of negative rates. I went back to the derivation of the formula, &lt;a href=&#34;http://books.google.fr/books?id=2sGwSAfA8eAC&amp;amp;lpg=PA278&amp;amp;dq=kwok%20barrier&amp;amp;pg=PA193#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;the book from Kwok&lt;/a&gt; is quite good on that. The closed form formula just stems from being the solution of an integral of the first passage time density (which is a simpler way to compute the one touch price than the PDE approach). It turns out that, then, the closed form solution to this integral with negative rates is just the same formula with complex numbers (there are actually some simplifications then).&lt;br /&gt;&lt;br /&gt;It is a bit uncommon to use the cumulative normal distribution on complex numbers, but the error function on complex numbers is more popular: it&#39;s actually even on &lt;a href=&#34;http://en.wikipedia.org/wiki/Error_function&#34;&gt;the wikipedia page of the error function&lt;/a&gt;. And it can be computed very quickly with machine precision thanks to the &lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package&#34;&gt;Faddeeva library&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;With this simple closed form formula, there is no need anymore for an approximation. I wrote &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2501907&#34;&gt;a small paper around this here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Later a collegue made the remark that it could be interesting to have the bivariate complex normal distribution for the case of partial start one touch options or partial barrier option rebates (not sure if those are common). Unfortunately I could not find any code or paper for this. And after asking Prof. Genz (who found a very elegant and fast algorithm for the bivariate normal distribution), it looks like an open problem.</description>
    </item>
    <item>
      <title>Asymptotic Behavior of SVI vs SABR</title>
      <link>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</link>
      <pubDate>Tue, 23 Sep 2014 12:06:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</guid>
      <description>The variance under SVI becomes linear when the log-moneyness is very large in absolute terms. The lognormal SABR formula with beta=0 or beta=1 has a very different behavior. Of course, the theoretical SABR model has actually a different asymptotic behavior.&lt;br /&gt;&lt;br /&gt;As an illustration, we calibrate SABR (with two different values of beta) and SVI against the same implied volatility slice and look at the wings behavior. &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-1w0jjvR9-Mk/VCFFhSiOcdI/AAAAAAAAHg4/E3yP_m3vhKA/s1600/Screenshot%2Bfrom%2B2014-09-23%2B11%3A52%3A07.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-1w0jjvR9-Mk/VCFFhSiOcdI/AAAAAAAAHg4/E3yP_m3vhKA/s1600/Screenshot%2Bfrom%2B2014-09-23%2B11%3A52%3A07.png&#34; height=&#34;497&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;While the Lee moments formula implies that the variance should be at most linear, something that the SABR formula does not respect. It is in practice not the problem with SABR as the actual Lee boundary: V(x) &amp;lt; 2|x|/T (where V is the square of the implied volatility and x the log-moneyness) is attained for extremely low strikes only with SABR, except maybe for very long maturities.&lt;br /&gt;&lt;br /&gt;A related behavior is the fact that the lognormal SABR formula can actually match steeper curvatures at the money than SVI for given asymptotes.</description>
    </item>
    <item>
      <title>SVI and long maturities issues</title>
      <link>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</link>
      <pubDate>Fri, 01 Aug 2014 12:51:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</guid>
      <description>&lt;p&gt;On long maturities equity options, the smile is usually very much like a skew: very little curvature. This usually means that the SVI rho will be very close to -1, in a similar fashion as what can happen for the the correlation parameter of a real stochastic volatility model (Heston, SABR).&lt;/p&gt;&#xA;&lt;p&gt;In terms of initial guess, &lt;a href=&#34;https://chasethedevil.github.io/post/another-svi-initial-guess&#34;&gt;I looked&lt;/a&gt; at the more usual use cases and showed that matching a parabola at the minimum variance point often leads to a decent initial guess if one has an ok estimate of the wings. We will see here that we can do also something a bit better than just a flat slice at-the-money in the case where rho is close to -1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>More SVI Initial Guesses</title>
      <link>https://chasethedevil.github.io/post/more-svi-initial-guesses/</link>
      <pubDate>Thu, 31 Jul 2014 14:54:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/more-svi-initial-guesses/</guid>
      <description>In the previous post, I showed one could extract the SVI parameters from a best fit parabola at-the-money. It seemed to work reasonably well, but I found some real market data where it can be much less satisfying.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png&#34; height=&#34;588&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;Sometimes (actually not so rarely) the ATM slope and curvatures can&#39;t be matched given rho and b found through the asymptotes. As a result if I force to just match the curvature and set m=0 (when the slope can&#39;t be matched), the simple ATM parabolic guess looks shifted. It can be much worse than this specific example.&lt;br /&gt;&lt;br /&gt;It is then a bit clearer why Vogt looked to match the lowest variance instead of ATM. We can actually also fit a parabola at the lowest variance (MV suffix in the graph) instead of ATM. It seems to fit generally better.&lt;br /&gt;&lt;br /&gt;I also tried to estimate the asymptotic slopes more precisely (using the slope of the 5-points parabola at each end), but it seems to not always be an improvement.&lt;br /&gt;&lt;br /&gt;However this does not work when rho is close to -1 or 1 as there is then no minimum. Often, matching ATM also does not work when rho is -1 or 1. This specific case, but quite common as well for longer expiries in equities need more thoughts, usually a constant slice is ok, but this is clearly where Zeliade&#39;s quasi explicit method shines.&lt;br /&gt;&lt;br /&gt;So far it still all looks good, but then looking at medium maturities (1 year), sometimes all initial guesses don&#39;t look comforting (although Levenberg-Marquardt minimization still works on those - but one can easily imagine that it can break as well, for example by tweaking slightly the rho/b and look at what happens then).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png&#34; height=&#34;640&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;There is lots of data on this 1 year example. One can clearly see the problem when the slope can not be fitted ATM (SimpleParabolicATM-guess), and even if by chance when it can (TripleParabolicATM-guess), it&#39;s not so great.&lt;br /&gt;Similarly fitting the lowest variance leads only to a good fit of the right wing and a bad fit everywhere else.&lt;br /&gt;&lt;br /&gt;Still, as if by miracle, everything converges to the best fit on this example (again one can find cases where some guesses don&#39;t converge to the best fit). I have added some weights +-20% around the money, to ensure that we capture the ATM behavior accurately (otherwise the best fit is funny).&lt;br /&gt;&lt;br /&gt;It is interesting to see that if one minimizes the min square sum of variances (what I do in Vogt-LM, it&#39;s in theory slightly faster as there is no sqrt function cost) it results in an ugly looking steeper curvature, while if we just minimize the min square sum of volatilities (what I do in SimpleParabolicMV_LM), it looks better.</description>
    </item>
    <item>
      <title>Another SVI Initial Guess</title>
      <link>https://chasethedevil.github.io/post/another-svi-initial-guess/</link>
      <pubDate>Tue, 29 Jul 2014 14:39:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/another-svi-initial-guess/</guid>
      <description>The SVI formula is:&lt;br /&gt;$$w(k) = a + b ( \rho (k-m) + \sqrt{(k-m)^2+ \sigma^2}$$&lt;br /&gt;where k is the log-moneyness, w(k) the implied variance at a given moneyness and a,b,rho,m,sigma the 5 SVI parameters.&lt;br /&gt;&lt;br /&gt;A. Vogt described a particularly simple way to find an initial guess to fit SVI to an implied volatility slice &lt;a href=&#34;http://www.nuclearphynance.com/User%20Files/66/GatheralSmile_estimation_for_one_expiry_NP.pdf&#34; target=&#34;_blank&#34;&gt;a while ago&lt;/a&gt;. The idea to compute rho and sigma from the left and right asymptotic slopes. a,m are recovered from the crossing point of the asymptotes and sigma using the minimum variance.&lt;br /&gt;&lt;br /&gt;Later, &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB4QFjAA&amp;amp;url=http%3A%2F%2Fwww.zeliade.com%2Fwhitepapers%2Fzwp-0005.pdf&amp;amp;ei=xI3XU9T3OYme7AbzwoHQDw&amp;amp;usg=AFQjCNGsvbseObGCDAZ1QbYtvOL9J2-aRw&amp;amp;sig2=rz4SilY2q1RSuD9XgWKFig&amp;amp;bvm=bv.71778758,d.ZGU&#34; target=&#34;_blank&#34;&gt;Zeliade has shown&lt;/a&gt; a very nice reduction of the problem to 2 variables, while the remaining 3 can be deduced explicitly. The practical side is that constraints are automatically included, the less practical side is the choice of minimizer for the two variables (Nelder-Mead) and of initial guess (a few random points).&lt;br /&gt;&lt;br /&gt;Instead, a simple alternative is the following: given b and rho from the asymptotic slopes, one could also just fit a parabola at-the-money, in a similar spirit as the &lt;a href=&#34;http://papers.ssrn.com/abstract=2467231&#34; target=&#34;_blank&#34;&gt;explicit SABR calibration&lt;/a&gt;, and recover explicitly a, m and sigma.&lt;br /&gt;&lt;br /&gt;To illustrate I take the data from Zeliade, where the input is already some SVI fit to market data.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png&#34; height=&#34;385&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;3M expiry - Zeliade data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png&#34; height=&#34;391&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;4Y expiry, Zeliade data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;We clearly see that ATM the fit is better for the parabolic initial guess than for Vogt, but as one goes further away from ATM, Vogt guess seems better.&lt;br /&gt;&lt;br /&gt;Compared to SABR, the parabola itself fits decently only very close to ATM. If one computes the higher order Taylor expansion of SVI around k=0, powers of (k/sigma) appear, while sigma is often relatively small especially for short expiries: the fourth derivative will quickly make a difference.&lt;br /&gt;&lt;br /&gt;On implied volatilities stemming from a SABR fit of the SP500, here is how the various methods behave:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png&#34; height=&#34;383&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;1M expiry on SABR data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png&#34; height=&#34;383&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;4Y expiry on SABR data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;As expected, because SABR (and thus the input implied vol) is much closer to a parabola, the parabolic initial guess is much better than Vogt. The initial guess of Vogt is particularly bad on long expiries, although it will still converge quite quickly to the true minimum with Levenberg-Marquardt.&lt;br /&gt;&lt;br /&gt;In practice, I have found the method of Zeliade to be very robust, even if a bit slower than Vogt, while Vogt can sometimes (rarely) be too sensitive to the estimate of the asymptotes.&lt;br /&gt;&lt;br /&gt;The parabolic guess method could also be applied to always fit exactly ATM vol, slope and curvature, and calibrate rho, b to gives the best overall fit. It might be an idea for the next blog post.</description>
    </item>
    <item>
      <title>Martin Odersky teaches Scala to the Masses</title>
      <link>https://chasethedevil.github.io/post/martin-odersky-teaches-scala-to-the-masses/</link>
      <pubDate>Tue, 17 Sep 2013 20:11:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/martin-odersky-teaches-scala-to-the-masses/</guid>
      <description>&lt;p&gt;I tried today the &lt;a href=&#34;https://www.coursera.org/course/progfun&#34;&gt;Scala courses on coursera&lt;/a&gt; by the Scala creator, Martin Odersky. I was quite impressed by the quality: I somehow believed Scala to be only a better Java, now I think otherwise. Throughout the course, even though it all sounds very basic, you understand the key concepts of Scala and why functional programming + OO concepts are a natural idea. What&amp;rsquo;s nice about Scala is that it avoids the functional vs OO or even the functional vs procedural debate by allowing both, because both can be important, at different scales. Small details can be (and probably should be) procedural for efficiency, because a processor is a processor, but higher level should probably be more functional (immutable) to be clearer, easier to evolve and more easily parallelized.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Setting Values in Java Enum - A Bad Idea</title>
      <link>https://chasethedevil.github.io/post/setting-values-in-java-enum---a-bad-idea/</link>
      <pubDate>Thu, 12 Sep 2013 10:06:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/setting-values-in-java-enum---a-bad-idea/</guid>
      <description>My Scala habits have made me create a stupid bug related to Java enums. In Scala, the concept of &lt;a href=&#34;http://www.scala-lang.org/old/node/107&#34;&gt;case classes&lt;/a&gt; is very neat and recently, I just confused enum in Java with what I sometimes do in Scala case classes.&lt;br /&gt;&lt;br /&gt;I wrote an enum with a setter like:&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;&amp;nbsp; public static enum BlackVariateType {&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; V0,&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ZERO_DERIVATIVE;&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; private double volSquare;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; public double getBlackVolatilitySquare() {&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; return volSquare;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; public void setBlackVolatilitySquare(double volSquare) {&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; this.volSquare = volSquare;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; }&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;&amp;nbsp; }&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Here, calling setBlackVolatilitySquare will override any previous value, and thus, if several parts are calling it with different values, it will be a mess as there is only a single instance.&lt;br /&gt;&lt;br /&gt;I am not sure if there is actually one good use case to have a setter on an enum. This sounds like a very dangerous practice in general. Member variables allowed should be only final. &lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;I just stumbled upon &lt;!-- raw HTML omitted --&gt;Julia&lt;!-- raw HTML omitted --&gt;, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library. I had not heard of this algorithm or this library before, but the author, &lt;!-- raw HTML omitted --&gt;Steven G. Johnson&lt;!-- raw HTML omitted --&gt;, claims it is faster and as precise as Cody &amp;amp; SLATEC implementations. As &lt;!-- raw HTML omitted --&gt;I previously had a look at those algorithms&lt;!-- raw HTML omitted --&gt; and was quite impressed by Cody&amp;rsquo;s implementation.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The &lt;!-- raw HTML omitted --&gt;source of Faddeeva&lt;!-- raw HTML omitted --&gt; shows a big list (100) of Chebychev expansions for various ranges of a normalized error function. I slightly modified the Faddeva code to compute directly the cumulative normal distribution, avoiding some exp(-x&lt;em&gt;x)&lt;em&gt;exp(x&lt;/em&gt;x) calls on the way.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Is it as accurate? I compared against a high precision implementation as in my previous test of cumulative normal distribution algorithms. And after replacing the exp(-x&lt;/em&gt;x) with &lt;!-- raw HTML omitted --&gt;Cody&amp;rsquo;s trick&lt;!-- raw HTML omitted --&gt; to compute it with higher accuracy, here is how it looks (referenced as &amp;ldquo;Johnson&amp;rdquo;).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I also measured performance on various ranges, and found out that this Johnson algorithm is around 2x faster than Cody (in Scala) and 30% faster than my optimization of Cody (using a table of exponentials for Cody&amp;rsquo;s trick).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>&lt;p&gt;I was used to &lt;a href=&#34;https://www.scilab.org&#34;&gt;Scilab&lt;/a&gt; for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.&lt;/p&gt;&#xA;&lt;p&gt;Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the &lt;a href=&#34;https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/&#34;&gt;Arbitrage Free SABR problem&lt;/a&gt;. It worked nicely and is a bit nicer to read than my equivalent Scala program. But I was a bit surprised by the low performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scala Build Tool : SBT</title>
      <link>https://chasethedevil.github.io/post/scala-build-tool--sbt/</link>
      <pubDate>Wed, 19 Jun 2013 18:01:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/scala-build-tool--sbt/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s been a while since I do a pet project in Scala, and today, after many trials before, I decided to give another go at Jetbrain Idea for Scala development, as Eclipse with the Scala plugin tended to crash a little bit too often for my taste (resulting sometimes in loss of a few lines of code). I could have just probably updated eclipse and the scala plugin, mine were not very old, but not the latest.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Simple &#39;Can Scala Do This?&#39; Questions</title>
      <link>https://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</link>
      <pubDate>Tue, 11 Jun 2013 00:28:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</guid>
      <description>&lt;p&gt;Today, a friend asked me if Scala could pass primitives (such as Double) by reference. It can be useful sometimes instead of creating a full blown object. In Java there is commons lang MutableDouble. It could be interesting if there was some optimized way to do that.&lt;/p&gt;&#xA;&lt;p&gt;One answer could be: it&amp;rsquo;s not functional programming oriented and therefore not too surprising this is not encouraged in Scala.&lt;/p&gt;&#xA;&lt;p&gt;Then he wondered if we could use it for C#.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;Marsaglia in &lt;!-- raw HTML omitted --&gt;his paper on Normal Distribution&lt;!-- raw HTML omitted --&gt; made the same mistake I initially did while trying to verify &lt;!-- raw HTML omitted --&gt;the accuracy of the normal density&lt;!-- raw HTML omitted --&gt;.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.6 because of the truncation at machine epsilon precision. Using Python mpmath, one can see that:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; mpf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;rsquo;-16.6000000000000014210854715202004&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;This is the more accurate representation if one goes beyond double precision (here 30 digits). And the value of the cumulative normal distribution is:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.4845465199503256054808152068743e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It is different from:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(mpf(&amp;quot;-16.6&amp;quot;))&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.48454651995040810217553910503186e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;where in this case it is really evaluated around -16.6 (up to 30 digits precision). Marsaglia gives this second number as reference. But all the other algorithms will actually take as input the first input. It is more meaningful to compare results using the exact same input. Using human readable but computer truncated numbers is not the best.  The cumulative normal distribution will often be computed using some output of some calculation where one does not have an exact human readable input.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The standard code for Ooura and Schonfelder (as well as Marsaglia) algorithms for the cumulative normal distribution don&amp;rsquo;t use Cody&amp;rsquo;s trick to evaluate the exp(-x&lt;em&gt;x). This function appears in all those implementations because it is part of the dominant term in the usual expansions. Out of curiosity, I replaced this part with Cody trick. For Ooura I also made minor changes to make it work directly on the CND instead of going through the error function erfc indirection. Here are the results without the Cody trick (except for Cody):&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;and with it:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;All 3 algorithms are now of similiar accuracy (note the difference of scale compared to the previous graph), with Schonfelder being a bit worse, especially for x &amp;gt;= -20. If one uses only easily representable numbers (for example -37, -36,75, -36,5, &amp;hellip;) in double precision then, of course, Cody trick importance won&amp;rsquo;t be visible and here is how the 3 algorithms would fare with or without Cody trick:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Schonfelder looks now worse than it actually is compared to Cody and Ooura.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;To conclude, if someone claims that a cumulative normal distribution is up to double precision accuracy and it does not use any tricks to compute exp(-x&lt;/em&gt;x), then beware, it probably is quite a bit less than double precision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/&#34;&gt;previous post&lt;/a&gt;, I stated that some library (SPECFUN by W.D. Cody) computes \(e^{-\frac{x^2}{2}}\) the following way:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;xsq &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fint&lt;/span&gt;(x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.6&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.6&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;del &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; xsq) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; xsq);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;xsq &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; xsq &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;del &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;);&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;p&gt;where &lt;code class=&#34;code-inline language-C&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;fint&lt;/span&gt;(z)&lt;/code&gt; computes the floor of z.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Why 1.6?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.6 is equivalent to multiplying by 10 and dividing by 16 which is an exact operation). It also allows to have something very close to a rounding function: x=2.6 will make xsq=2.5, x=2.4 will make xsq=1.875, x=2.5 will make xsq=2.5. The maximum difference between x and xsq will be 0.625.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scala is Mad (part 2)</title>
      <link>https://chasethedevil.github.io/post/scala-is-mad-part-2/</link>
      <pubDate>Wed, 13 Feb 2013 16:20:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/scala-is-mad-part-2/</guid>
      <description>&lt;p&gt;I still did not abandon Scala despite my &lt;!-- raw HTML omitted --&gt;previous post&lt;!-- raw HTML omitted --&gt;, mainly because I have already quite a bit of code, and am too lazy to port it. Furthermore the issues I detailed were not serious enough to motivate a switch. But these days I am more and more fed up with Scala, especially because of the Eclipse plugin. I tried the newer, the beta, and the older, the stable, the conclusion is the same. It&amp;rsquo;s welcome but:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;code completion is not great compared to Java. For example one does not seem to be able to see the constructor parameters, or the method parameters can not be automatically populated.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;the plugin makes Eclipse &lt;em&gt;very&lt;/em&gt; slow. Everything seems at least 3-5x slower. On the fly compilation is also much much slower than Java&amp;rsquo;s.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It&amp;rsquo;s nice to type less, but if overall writing is slower because of the above issues, it does not help. Beside curiosity of a new language features, I don&amp;rsquo;t see any point in Scala today, even if some of the ideas are interesting. I am sure it will be forgotten/abandoned in a couple of years. Today, if I would try a new language, I would give Google Go a try: I don&amp;rsquo;t think another big language can make it/be useful on the JVM (beside a scripting kind of language, like JavaScript or Jython).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Google Go focuses on the right problem: concurrency. It also is not constrained to JVM limitation (on the other side one can not use a Java library - but open source stuff is usually not too difficult to port from one language to another). It has one of the fastest compilers. It makes interesting practical choices: no inheritance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scala is Mad</title>
      <link>https://chasethedevil.github.io/post/scala-is-mad/</link>
      <pubDate>Wed, 12 Dec 2012 16:07:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/scala-is-mad/</guid>
      <description>I spent quick a bit of time to figure out why something that is usually simple to do in Java did not work in Scala: Arrays and ArrayLists with generics.&lt;br /&gt;&lt;br /&gt;For some technical reason (type erasure at the JVM level), Array sometimes need a parameter with a ClassManifest !?! a generic type like [T :&amp;lt; Point : ClassManifest] need to be declared instead of simply [T :&amp;lt; Point].&lt;br /&gt;&lt;br /&gt;And then the quickSort method somehow does not work if invoked on a generic... like quickSort(points) where points: Array[T]. I could not figure out yet how to do this one, I just casted to points.asInstanceOf[Array[Point]], quite ugly.&lt;br /&gt;&lt;br /&gt;In contrast I did not even have to think much to write the Java equivalent. Generics in Scala, while having a nice syntax, are just crazy. This is something that goes beyond generics. Some of the Scala library and syntax is nice, but overall, the IDE integration is still very buggy, and productivity is not higher.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update Dec 12 2012&lt;/b&gt;: here is the actual code (this is kept close to the Java equivalent on purpose):&lt;br /&gt;&lt;pre&gt;object Point {&lt;br /&gt;  def sortAndRemoveIdenticalPoints[T &lt;: Point : ClassManifest](points : Array[T]) : Array[T] = {&lt;br /&gt;      Sorting.quickSort(points.asInstanceOf[Array[Point]])&lt;br /&gt;      val l = new ArrayBuffer[T](points.length)&lt;br /&gt;      var previous = points(0)&lt;br /&gt;      l += points(0)&lt;br /&gt;      for (i &lt;- 1 until points.length) {&lt;br /&gt;        if(math.abs(points(i).value - previous.value)&lt; Epsilon.MACHINE_EPSILON_SQRT) {&lt;br /&gt;          l += points(i)&lt;br /&gt;        }&lt;br /&gt;      }&lt;br /&gt;      return l.toArray&lt;br /&gt;    }&lt;br /&gt;    return points&lt;br /&gt;  }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;class Point(val value: Double, val isMiddle: Boolean) extends Ordered[Point] {&lt;br /&gt;  def compare(that: Point): Int = {&lt;br /&gt;    return math.signum(this.value - that.value).toInt&lt;br /&gt;  }&lt;br /&gt;}&lt;br /&gt;&lt;!-----&gt;&lt;!--:--&gt;&lt;/-&gt;&lt;/:&gt;&lt;/pre&gt;In Java one can just use Arrays.sort(points) if points is a T[]. And the method can work with a subclass of Point.</description>
    </item>
    <item>
      <title>Scala Again</title>
      <link>https://chasethedevil.github.io/post/scala-again/</link>
      <pubDate>Mon, 06 Feb 2012 17:52:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/scala-again/</guid>
      <description>I am trying &lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;Scala&lt;/a&gt; again. Last time, several years ago, I played around with it as a web tool, combining it with a Servlet Runner like Tomcat. This time, I play around with it for some quantitative finance experiments.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Why Scala?&lt;/b&gt; It still seem the most advanced alternative to Java on the JVM, and the mix of functional programming and OO programming is interesting. Furthermore it goes quite far as it ships with its own library. I was curious to see if I could express some things better with Scala.&lt;br /&gt;&lt;br /&gt;Here are my first impressions after a week:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I like the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt; keyword. It avoids the messy singleton pattern, or the classes with many static methods. I think it makes things much cleaner to not use static at all but distinguish between &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt; &amp;amp; &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;class&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;I like the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Array[Double]&lt;/span&gt;, and especially &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ArrayBuffer[Double]&lt;/span&gt;. Finally we don&#39;t have to worry between the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Double&lt;/span&gt; and &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;double&lt;/span&gt; performance issues.&lt;/li&gt;&lt;li&gt;I was a bit annoyed by &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;a(i)&lt;/span&gt; instead of &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;a[i]&lt;/span&gt; but it makes sense. I wonder if there is a performance implication for arrays, hopefully not.&lt;/li&gt;&lt;li&gt;I like the real properties, automatic getter/setter: less boilerplate code, less getThis(), setThat(toto).&lt;/li&gt;&lt;li&gt;Very natural interaction with Java libraries. &lt;/li&gt;&lt;li&gt;I found a good use of &lt;b&gt;case classes&lt;/b&gt; (to my surprise): typically an enum that can have some well defined parameters, and that you don&#39;t want to make a class (because it&#39;s not). My use case was to define boundaries of a spline.&lt;/li&gt;&lt;li&gt;I love the formatter in the scala (eclipse) IDE. Finally a formatter in eclipse that does not produce crap.&lt;/li&gt;&lt;/ul&gt;Now things I still need time to get used to:&lt;br /&gt;&lt;ul&gt;&lt;li&gt; member variable declared implicitly in the constructor. I first made the mistake (still?) to declare some variables twice.&lt;/li&gt;&lt;li&gt;I got hit by starting a line with a &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;+&lt;/span&gt; instead of ending with a &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;+&lt;/span&gt;. It is dangerous, but it certainly makes the code more consistent.&lt;/li&gt;&lt;li&gt;Performance impacts: I will need to take a look at the bytecode for some scala constructs to really understand the performance impact of some uses. For example I tend to use while loops instead of for comprehension after some scary post of the Twitter guys about for comprehension. But at first, it looks as fast as Java.&lt;/li&gt;&lt;li&gt;I wrote my code a bit fast. I am sure I could make use of more Scala features.&lt;/li&gt;&lt;li&gt;The scala IDE in eclipse 3.7.1 has known issues. I wish it was a bit more functional, but it&#39;s quite ok (search for references works, renaming works to some extent).&lt;/li&gt;&lt;li&gt;Scala unit tests: I used scala tests, but it seems a bit funny at first. Also I am not convinced by the syntax that avoid method names and prefer test(&#34;test name&#34;). It makes it more difficult to browse the source code.&lt;/li&gt;&lt;/ul&gt;Some things they should consider:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Integrate directly a Log API. I just use SLF4J without any scala wrapper, but it feels like it should be part of the standard API (even if that did not work out so well for Sun).&lt;/li&gt;&lt;li&gt;Double.Epsilon is not the machine epsilon: very strange. I found out somewhere else there was the machine epsilon, don&#39;t remember where because I ended up just making a small &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Unit tests should be part of the standard API.&lt;/li&gt;&lt;/ul&gt;Overall I found it quite exciting as there are definitely new ways to solve problems. It was a while since I had been excited with actual coding.</description>
    </item>
    <item>
      <title>A Very Interesting Feature of Scala</title>
      <link>https://chasethedevil.github.io/post/a-very-interesting-feature-of-scala/</link>
      <pubDate>Sat, 07 Aug 2010 12:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-very-interesting-feature-of-scala/</guid>
      <description>I tried Scala &lt;a href=&#34;http://chasethedevil.blogspot.com/2007/09/fast-web-development-with-scala.html&#34;&gt;a few years ago&lt;/a&gt;. There are several good ideas in it, but I found the language to be a bit too complicated to master. But I recently stubbled upon &lt;a href=&#34;http://lamp.epfl.ch/~dragos/files/scala-spec.pdf&#34;&gt;a paper on Scala generics&lt;/a&gt;&amp;nbsp;that might change my mind about using Scala.&lt;br /&gt;&lt;br /&gt;Scala Generics used to work in a similar way as Java Generics: via type erasure. One main reason is compatibility with Java, another is that C++ like templates make the code base blow up. Scala Generics offered some additional behavior (the variance/covariance notion).&amp;nbsp;C++ templates, however, have some very interesting aspects: one is that everything is done at compile time, the other is &amp;nbsp;performance. If the generics are involved in any kind of computation intensive task, all the Java type conversion will create a significant overhead.&lt;br /&gt;&lt;br /&gt;Now Scala has &lt;b&gt;&lt;a href=&#34;http://www.scala-lang.org/api/current/scala/specialized.html&#34;&gt;@specialized&lt;/a&gt;&lt;/b&gt;&amp;nbsp;(since Scala 2.8). Annotating a generic type with @specialized will generate code. One has the choice to accept the performance penalty or to get all the performance but accept the code blow up. I think this is very useful.&lt;br /&gt;&lt;br /&gt;If you read the paper you will see that the performance implications of this are not always small.&lt;br /&gt;&lt;br /&gt;UPDATE: I thank the readers for pointing that this work only with primitive types to avoid autoboxing. It is still valuable but less than I first thought.</description>
    </item>
  </channel>
</rss>
