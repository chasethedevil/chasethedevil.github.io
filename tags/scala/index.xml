<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>scala on Chase the Devil</title>
    <link>https:///chasethedevil.github.io/tags/scala/</link>
    <description>Recent content in scala on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Tue, 25 Nov 2014 13:58:00 +0000</lastBuildDate>
    
	<atom:link href="https:///chasethedevil.github.io/tags/scala/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Flat Volatility Surfaces &amp; Discrete Dividends</title>
      <link>https:///chasethedevil.github.io/post/flat-volatility-surfaces-discrete-dividends/</link>
      <pubDate>Tue, 25 Nov 2014 13:58:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/flat-volatility-surfaces-discrete-dividends/</guid>
      <description>In papers around volatility and cash (discrete) dividends, we often encounter the example of the flat volatility surface. For example, the OpenGamma paperpresents this graph:It shows that if the Black volatility surface is fully flat, there are jumps in the pure volatility surface (corresponding to a process that includes discrete dividends in a consistent manner) at the dividend dates or equivalently if the pure volatility surface is flat, the Black volatility jumps.</description>
    </item>
    
    <item>
      <title>Machine Learning &amp; Quantitative Finance</title>
      <link>https:///chasethedevil.github.io/post/machine-learning-quantitative-finance/</link>
      <pubDate>Tue, 18 Nov 2014 12:34:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/machine-learning-quantitative-finance/</guid>
      <description>There is an interesting course on Machine Learning on Coursera, it does not require much knowledge and yet manages to teach quite a lot.I was struck by the fact that most techniques and ideas apply also to problems in quantitative finance.Linear regression: used for example in the Longstaff-Schwartz approach to price Bermudan options with Monte-Carlo. Interestingly the teacher insists on feature normalization, something we can forget easily, especially with the polynomial features.</description>
    </item>
    
    <item>
      <title>Pseudo-Random vs Quasi-Random Numbers</title>
      <link>https:///chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</link>
      <pubDate>Wed, 12 Nov 2014 17:05:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</guid>
      <description>Quasi-Random numbers (like Sobol) are a relatively popular way in finance to improve the Monte-Carlo convergence compared to more classic Pseudo-Random numbers (like Mersenne-Twister). Behind the scenes one has to be a bit more careful about the dimension of the problem as the Quasi-Random numbers depends on the dimension (defined by how many random variables are independent from each other).For a long time, Sobol was limited to 40 dimensions using the so called Bratley-Fox direction numbers (his paper actually gives the numbers for 50 dimensions).</description>
    </item>
    
    <item>
      <title>Integrating an oscillatory function</title>
      <link>https:///chasethedevil.github.io/post/integrating-an-oscillatory-function/</link>
      <pubDate>Wed, 05 Nov 2014 16:48:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/integrating-an-oscillatory-function/</guid>
      <description>Recently, some instabilities were noticed in the Carr-Lee seasoned volatility swap price in some situations. The Carr-Leeseasoned volatility swap price involve the computation of a double integral. The inner integral is really the problematic one as the integrand can be highly oscillating.I first found a somewhat stable behavior using a specific adaptive Gauss-Lobatto implementation (the one from Espelid) and a change of variable. But it was not very satisfying to see that the outer integral was stable only with another specific adaptive Gauss-Lobatto (the one from Gander &amp;amp; Gauschi, present in Quantlib).</description>
    </item>
    
    <item>
      <title>The elusive reference: the Lamperti transform</title>
      <link>https:///chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</link>
      <pubDate>Mon, 03 Nov 2014 11:23:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</guid>
      <description>Without knowing that it was a well known general concept, I first noticed the use of the Lamperti transform in the Andersen-Piterbarg &amp;ldquo;Interest rate modeling&amp;rdquo; book p292 &amp;ldquo;finite difference solutions for general phi&amp;rdquo;. Pat Hagan used that transformation for a better discretization of the arbitrage free SABR PDE model.I then started to notice the use of this transformation in many more papers. The first one I saw naming it &amp;ldquo;Lamperti transform&amp;rdquo; was the paper from Ait-Sahalia &amp;ldquo;Maximum likelyhood estimation of discretely sampled diffusions: a closed-form approximation approach&amp;rdquo;.</description>
    </item>
    
    <item>
      <title>Barrier options under negative rates: complex numbers to the rescue</title>
      <link>https:///chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</link>
      <pubDate>Thu, 02 Oct 2014 11:58:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</guid>
      <description>I stumbled upon an unexpected problem: the one touch barrier formulacan break down under negative rates. While negative rates can sound fancy, they are actually quite real on some markets. Combined with relatively low volatilities, this makes the standard Black-Scholes one touch barrier formula blow up because somewhere the square root of a negative number is taken.At first, I had the idea to just floor the number to 0. But then I needed to see if this rough approximation would be acceptable or not.</description>
    </item>
    
    <item>
      <title>Initial Guesses for SVI - A Summary</title>
      <link>https:///chasethedevil.github.io/post/initial-guesses-for-svi-a-summary/</link>
      <pubDate>Fri, 26 Sep 2014 10:46:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/initial-guesses-for-svi-a-summary/</guid>
      <description>I have been looking at various ways of finding initial guesses for SVI calibration (Another SVI Initial Guess, More SVI Initial Guesses, SVI and long maturities issues). I decided to write a papersummarizing this. I find that the process of writing a paper makes me think more carefully about a problem.In this case, it turns out that the Vogt initial guess method (guess via asymptotes and minimum variance) is actually very good as long as one has a good way to lookup the asymptotes (the data is not always convex, while SVI is) and as long as rho is not close to -1, that is for long maturity affine like smiles, where SVI is actually more difficult to calibrate properly due to the over-parameterisation in those cases.</description>
    </item>
    
    <item>
      <title>Asymptotic Behavior of SVI vs SABR</title>
      <link>https:///chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</link>
      <pubDate>Tue, 23 Sep 2014 12:06:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</guid>
      <description>The variance under SVI becomes linear when the log-moneyness is very large in absolute terms. The lognormal SABR formula with beta=0 or beta=1 has a very different behavior. Of course, the theoretical SABR model has actually a different asymptotic behavior.As an illustration, we calibrate SABR (with two different values of beta) and SVI against the same implied volatility slice and look at the wings behavior. While the Lee moments formula implies that the variance should be at most linear, something that the SABR formula does not respect.</description>
    </item>
    
    <item>
      <title>SVI and long maturities issues</title>
      <link>https:///chasethedevil.github.io/post/svi-and-long-maturities-issues/</link>
      <pubDate>Fri, 01 Aug 2014 12:51:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/svi-and-long-maturities-issues/</guid>
      <description>On long maturities equity options, the smile is usually very much like a skew: very little curvature. This usually means that the SVI rho will be very close to -1, in a similar fashion as what can happen for the the correlation parameter of a real stochastic volatility model (Heston, SABR).In terms of initial guess, we lookedat the more usual use cases and showed that matching a parabola at the minimum variance point often leads to a decent initial guess if one has an ok estimate of the wings.</description>
    </item>
    
    <item>
      <title>More SVI Initial Guesses</title>
      <link>https:///chasethedevil.github.io/post/more-svi-initial-guesses/</link>
      <pubDate>Thu, 31 Jul 2014 14:54:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/more-svi-initial-guesses/</guid>
      <description>In the previous post, I showed one could extract the SVI parameters from a best fit parabola at-the-money. It seemed to work reasonably well, but I found some real market data where it can be much less satisfying.Sometimes (actually not so rarely) the ATM slope and curvatures can&amp;rsquo;t be matched given rho and b found through the asymptotes. As a result if I force to just match the curvature and set m=0 (when the slope can&amp;rsquo;t be matched), the simple ATM parabolic guess looks shifted.</description>
    </item>
    
    <item>
      <title>Another SVI Initial Guess</title>
      <link>https:///chasethedevil.github.io/post/another-svi-initial-guess/</link>
      <pubDate>Tue, 29 Jul 2014 14:39:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/another-svi-initial-guess/</guid>
      <description>The SVI formula is:$$w(k) = a + b ( \rho (k-m) + \sqrt{(k-m)^2+ \sigma^2}$$where k is the log-moneyness, w(k) the implied variance at a given moneyness and a,b,rho,m,sigma the 5 SVI parameters.A. Vogt described a particularly simple way to find an initial guess to fit SVI to an implied volatility slice a while ago. The idea to compute rho and sigma from the left and right asymptotic slopes. a,m are recovered from the crossing point of the asymptotes and sigma using the minimum variance.</description>
    </item>
    
    <item>
      <title>Martin Odersky teaches Scala to the Masses</title>
      <link>https:///chasethedevil.github.io/post/martin-odersky-teaches-scala-to-the-masses/</link>
      <pubDate>Tue, 17 Sep 2013 20:11:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/martin-odersky-teaches-scala-to-the-masses/</guid>
      <description>I tried today the Scala courses on courseraby the Scala creator, Martin Odersky. I was quite impressed by the quality: I somehow believed Scala to be only a better Java, now I think otherwise. Throughout the course, even though it all sounds very basic, you understand the key concepts of Scala and why functional programming + OO concepts are a natural idea. What&amp;rsquo;s nice about Scala is that it avoids the functional vs OO or even the functional vs procedural debate by allowing both, because both can be important, at different scales.</description>
    </item>
    
    <item>
      <title>Setting Values in Java Enum - A Bad Idea</title>
      <link>https:///chasethedevil.github.io/post/setting-values-in-java-enum-a-bad-idea/</link>
      <pubDate>Thu, 12 Sep 2013 10:06:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/setting-values-in-java-enum-a-bad-idea/</guid>
      <description>My Scala habits have made me create a stupid bug related to Java enums. In Scala, the concept of case classesis very neat and recently, I just confused enum in Java with what I sometimes do in Scala case classes.I wrote an enum with a setter like:public static enum BlackVariateType {V0,ZERO_DERIVATIVE;private double volSquare;public double getBlackVolatilitySquare() {return volSquare;}public void setBlackVolatilitySquare(double volSquare) {this.volSquare = volSquare;}}Here, calling setBlackVolatilitySquare will override any previous value, and thus, if several parts are calling it with different values, it will be a mess as there is only a single instance.</description>
    </item>
    
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>https:///chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>I just stumbled upon Julia, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library.</description>
    </item>
    
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>https:///chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>I was used to Scilabfor small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the Arbitrage Free SABR problem. It worked nicely and is a bit nicer to read than my equivalent Scala program.</description>
    </item>
    
    <item>
      <title>Scala Build Tool : SBT</title>
      <link>https:///chasethedevil.github.io/post/scala-build-tool-sbt/</link>
      <pubDate>Wed, 19 Jun 2013 18:01:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/scala-build-tool-sbt/</guid>
      <description>It&amp;rsquo;s been a while since I do a pet project in Scala, and today, after many trials before, I decided to give another go at Jetbrain Idea for Scala development, as Eclipse with the Scala plugin tended to crash a little bit too often for my taste (resulting sometimes in loss of a few lines of code). I could have just probably updated eclipse and the scala plugin, mine were not very old, but not the latest.</description>
    </item>
    
    <item>
      <title>Simple &#39;Can Scala Do This?&#39; Questions</title>
      <link>https:///chasethedevil.github.io/post/simple-can-scala-do-this-questions/</link>
      <pubDate>Tue, 11 Jun 2013 00:28:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/simple-can-scala-do-this-questions/</guid>
      <description>Today, a friend asked me if Scala could pass primitives (such as Double) by reference. It can be useful sometimes instead of creating a full blown object. In Java there is commons lang MutableDouble. It could be interesting if there was some optimized way to do that.
One answer could be: it&amp;rsquo;s not functional programming oriented and therefore not too surprising this is not encouraged in Scala.
Then he wondered if we could use it for C#.</description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https:///chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>Marsaglia in his paper on Normal Distributionmade the same mistake I initially did while trying to verify the accuracy of the normal density.In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.</description>
    </item>
    
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https:///chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>In my previous post, I stated that some library (SPECFUN by W.D. Cody) computes $$e^{-\frac{x^2}{2}}$$ the following way:xsq = fint(x * 1.6) / 1.6;del = (x - xsq) * (x + xsq);result = exp(-xsq * xsq * 0.5) * exp(-del  0.5);where fint(z) computes the floor of z.1. Why 1.6?An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.</description>
    </item>
    
    <item>
      <title>Scala is Mad (part 2)</title>
      <link>https:///chasethedevil.github.io/post/scala-is-mad-part-2/</link>
      <pubDate>Wed, 13 Feb 2013 16:20:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/scala-is-mad-part-2/</guid>
      <description>I still did not abandon Scala despite my previous post, mainly because I have already quite a bit of code, and am too lazy to port it. Furthermore the issues I detailed were not serious enough to motivate a switch. But these days I am more and more fed up with Scala, especially because of the Eclipse plugin. I tried the newer, the beta, and the older, the stable, the conclusion is the same.</description>
    </item>
    
    <item>
      <title>Scala is Mad</title>
      <link>https:///chasethedevil.github.io/post/scala-is-mad/</link>
      <pubDate>Wed, 12 Dec 2012 16:07:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/scala-is-mad/</guid>
      <description>I spent quick a bit of time to figure out why something that is usually simple to do in Java did not work in Scala: Arrays and ArrayLists with generics.For some technical reason (type erasure at the JVM level), Array sometimes need a parameter with a ClassManifest !?! a generic type like [T :&amp;lt; Point : ClassManifest] need to be declared instead of simply [T :&amp;lt; Point].And then the quickSort method somehow does not work if invoked on a generic&amp;hellip; like quickSort(points) where points: Array[T].</description>
    </item>
    
    <item>
      <title>Scala Again</title>
      <link>https:///chasethedevil.github.io/post/scala-again/</link>
      <pubDate>Mon, 06 Feb 2012 17:52:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/scala-again/</guid>
      <description>I am trying Scalaagain. Last time, several years ago, I played around with it as a web tool, combining it with a Servlet Runner like Tomcat. This time, I play around with it for some quantitative finance experiments.Why Scala?It still seem the most advanced alternative to Java on the JVM, and the mix of functional programming and OO programming is interesting. Furthermore it goes quite far as it ships with its own library.</description>
    </item>
    
    <item>
      <title>A Very Interesting Feature of Scala</title>
      <link>https:///chasethedevil.github.io/post/a-very-interesting-feature-of-scala/</link>
      <pubDate>Sat, 07 Aug 2010 12:35:00 +0000</pubDate>
      
      <guid>https:///chasethedevil.github.io/post/a-very-interesting-feature-of-scala/</guid>
      <description>I tried Scala a few years ago. There are several good ideas in it, but I found the language to be a bit too complicated to master. But I recently stubbled upon a paper on Scala genericsthat might change my mind about using Scala.Scala Generics used to work in a similar way as Java Generics: via type erasure. One main reason is compatibility with Java, another is that C++ like templates make the code base blow up.</description>
    </item>
    
  </channel>
</rss>