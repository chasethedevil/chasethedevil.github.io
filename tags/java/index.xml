<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/java/</link>
    <description>Recent content in Java on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Sun, 29 Jun 2014 10:40:00 +0000</lastBuildDate>
    <atom:link href="/tags/java/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>On the Role of Static Types and Generic Types on Productivity</title>
      <link>http://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/</link>
      <pubDate>Sun, 29 Jun 2014 10:40:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/</guid>
      <description>Most developers have strong opinions on dynamic types programming languages vs static types programming languages. The former is often assumed to be good for small projects/prototyping while the later better for bigger projects. But there is a surprisingly small number of studies to back those claims.
One such study is &amp;ldquo;An experiment about static and dynamic type systems: doubts about the positive impact of static type systems on development time&amp;rdquo; and came to the conclusion that on a small project, static typing did not decrease programming time, and actually increased debugging time. <a href="/post/on-the-role-of-static-types-and-generic-types-on-productivity/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>throws Exception</title>
      <link>http://chasethedevil.github.io/post/throws-exception/</link>
      <pubDate>Tue, 27 May 2014 10:49:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/throws-exception/</guid>
      <description>There was a big debate at work around Exception declaration in a Java API. I was quite surprised that such an apparently simple subject could end up being so controversial. The controversy was around the choice of declaring in the interfaces:
void myMethod() throws Exception
instead of
void myMethod() throws MyAPIException
void myMethod() throws MyAPIRuntimeException
void myMethod() 
where MyAPI represents either a generic API related exception or a specific exception related to the method in question. <a href="/post/throws-exception/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>5 Minutes of Xtend</title>
      <link>http://chasethedevil.github.io/post/5-minutes-of-xtend/</link>
      <pubDate>Tue, 08 Apr 2014 17:37:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/5-minutes-of-xtend/</guid>
      <description>There is a relatively new JVM based language, Xtend. Their homepage says &amp;ldquo;JAVA 10, TODAY!&amp;rdquo;, so I thought I would give it a try, I was especially interested in operator overloading support, and the fact that it compiles to Java code, not Java byte code.
Unfortunately, after 5 minutes with it, and pasting some non Java code in an xtend file, Eclipse hangs forever, even on restart. After creating another workspace, just to trash the new workspace a similar way. <a href="/post/5-minutes-of-xtend/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>http://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>I just stumbled upon Julia, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.
Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library. <a href="/post/julia-and-the-cumulative-normal-distribution/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>http://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>I was used to Scilab for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.
Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the Arbitrage Free SABR problem. <a href="/post/octave-vs-scilab-for-pdes-in-finance/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>The CUDA Performance Myth II</title>
      <link>http://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</link>
      <pubDate>Fri, 12 Jul 2013 15:23:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</guid>
      <description>This is a kind of following to the CUDA performance myth. There is a recent news on the java concurrent mailing list about SplittableRandom class proposed for JDK8. It is a new parallel random number generator a priori usable for Monte-Carlo simulations.
It seems to rely on some very recent algorithm. There are some a bit older ones: the ancestor, L&amp;rsquo;Ecuyer MRG32k3a that can be parallelized through relatively costless skipTo methods, a Mersenne Twister variant MTGP, and even the less rigourous XorWow popularized by NVidia CUDA. <a href="/post/the-cuda-performance-myth-ii/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Simple &#39;Can Scala Do This?&#39; Questions</title>
      <link>http://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</link>
      <pubDate>Tue, 11 Jun 2013 00:28:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</guid>
      <description>Today, a friend asked me if Scala could pass primitives (such as Double) by reference. It can be useful sometimes instead of creating a full blown object. In Java there is commons lang MutableDouble. It could be interesting if there was some optimized way to do that.
One answer could be: it&amp;rsquo;s not functional programming oriented and therefore not too surprising this is not encouraged in Scala.
Then he wondered if we could use it for C#. <a href="/post/simple-can-scala-do-this-questions/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>SABR with the new Hagan PDE Approach</title>
      <link>http://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</link>
      <pubDate>Tue, 28 May 2013 15:56:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</guid>
      <description>At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it&amp;rsquo;s just like a regular PDE with proper boundary conditions.
I was wondering how it compared to Andreasen Huge results.

My first implementation was quite slow. <a href="/post/sabr-with-the-new-hagan-pde-approach/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.
Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an intrinsic function call and that should be difficult to beat. <a href="/post/a-fast-exponential-function-in-java/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>In my previous post, I explored the Lord-Kahl method to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise. <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
  </channel>
</rss>
