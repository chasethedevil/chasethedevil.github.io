<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Java on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/java/</link>
    <description>Recent content in Java on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 29 Jun 2014 10:40:00 +0000</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/tags/java/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>On the Role of Static Types and Generic Types on Productivity</title>
      <link>http://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/</link>
      <pubDate>Sun, 29 Jun 2014 10:40:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/</guid>
      <description>&lt;p&gt;Most developers have strong opinions on dynamic types programming languages vs static types programming languages. The former is often assumed to be good for small projects/prototyping while the later better for bigger projects. But there is a surprisingly small number of studies to back those claims.&lt;br /&gt;&lt;br /&gt;One such study is &amp;ldquo;&lt;a href=&#34;http://diyhpl.us/~bryan/papers2/paperbot/7a01e5a892a6d7a9f408df01905f9359.pdf&#34; target=&#34;_blank&#34;&gt;An experiment about static and dynamic type systems: doubts about the positive impact of static type systems on development time&lt;/a&gt;&amp;rdquo; and came to the conclusion that on a small project, static typing did not decrease programming time, and actually increased debugging time. However 4 years later, &amp;ldquo;&lt;a href=&#34;http://users.dcc.uchile.cl/~rrobbes/p/ICPC2014-idetypes.pdf&#34; target=&#34;_blank&#34;&gt;An empirical comparison of static and dynamic type systems on API usage in the presence of an IDE: Java vs. groovy with eclipse&lt;/a&gt;&amp;rdquo; shows that a developer is 2x more productive with Java than with Groovy using an unknown API. This contrasts a bit (but does not contradict) with their previous study &amp;ldquo;&lt;a href=&#34;http://swp.dcc.uchile.cl/TR/2012/TR_DCC-20120418-005.pdf&#34; target=&#34;_blank&#34;&gt;Static Type Systems (Sometimes) have a Positive Impact on the Usability of Undocumented Software: An Empirical Evaluation&lt;/a&gt;&amp;rdquo; that showed Groovy to be more productive on small projects. One problem is that all these studies stem from the same person.&lt;br /&gt;&lt;br /&gt;It&amp;rsquo;s more interesting to look at generic types vs raw types use, where even less studies have been done. &amp;ldquo;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=2509528&#34; target=&#34;_blank&#34;&gt;Do developers benefit from generic types?: an empirical comparison of generic and raw types in java&lt;/a&gt;&amp;rdquo; concludes that generic types do not provide any advantages to fix typing errors, hardly surprising in my opinion. Generic types (especially with type erasure as in Java) is the typical idea that sounds good but that in practice does not really help: it makes the code actually more awkward to read and tend to make developers too lazy to create new classes that would often be more appropriate than a generic type (think Map&amp;lt;String,List&amp;lt;Map&amp;lt;String, Date&amp;gt;&amp;gt;&amp;gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>throws Exception</title>
      <link>http://chasethedevil.github.io/post/throws-exception/</link>
      <pubDate>Tue, 27 May 2014 10:49:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/throws-exception/</guid>
      <description>&lt;p&gt;There was a big debate at work around Exception declaration in a Java API. I was quite surprised that such an apparently simple subject could end up being so controversial. The controversy was around the choice of declaring in the interfaces:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;void myMethod() throws Exception&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;instead of&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;void myMethod() throws MyAPIException&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;void myMethod() throws MyAPIRuntimeException&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;void myMethod() &lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;where MyAPI represents either a generic API related exception or a specific exception related to the method in question.&lt;br /&gt;&lt;br /&gt;The choice of &amp;ldquo;throws Exception&amp;rdquo; did not even occur to me as a possibility, but after some digging, I found that some relatively famous libraries actually followed that principle at one point, for example Apache Struts 1.x or Spring MVC. &lt;br /&gt;&lt;br /&gt;More modern libraries, like Google Guava, commons-math 3.x, Struts 2.x generally favor MyAPIRuntimeException where MyAPI is actually context-specific. Some old popular libraries declare a checked Exception, for example the HibernateException in Hibernate.&lt;br /&gt;&lt;br /&gt;This seems to be a recurring subject on Stackoverflow:&lt;br /&gt;&lt;a href=&#34;http://stackoverflow.com/questions/20530221/java-interface-throws-exception-best-practice&#34; target=&#34;_blank&#34;&gt;Stackoverflow - Java interface throws Exception best practice&lt;/a&gt;&lt;br /&gt;&lt;a href=&#34;http://stackoverflow.com/questions/4283634/what-to-put-in-the-throws-clause-of-an-interface-method&#34; target=&#34;_blank&#34;&gt;Stackoverflow - What to put in the throws clause of an interface method&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;But those are quite poor in terms of explanations. The best comments on this subjects are from:&lt;br /&gt;&lt;a href=&#34;http://www.artima.com/intv/handcuffs.html&#34;&gt;&lt;span class=&#34;ts&#34;&gt;Anders         Hejlsberg (C#, Delphi, Turbo Pascal creator) - The Trouble with         Checked Exceptions&lt;/span&gt;&lt;/a&gt;&lt;br /&gt;    &lt;a href=&#34;http://www.artima.com/intv/solid2.html&#34;&gt;&lt;span class=&#34;ts&#34;&gt;James         Gosling (Java creator) - Failure and Exceptions&lt;/span&gt;&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;    &lt;br /&gt;This comment from Anders is particularly acute:&lt;br /&gt;    &amp;ldquo;&lt;b&gt;To work around this requirement, people do ridiculous things.       For example, they decorate every method with, &amp;ldquo;&lt;/b&gt;&lt;b&gt;&lt;code&gt;throws         Exception&lt;/code&gt;&lt;/b&gt;&lt;b&gt;.&amp;rdquo; That just completely defeats the       feature, and you just made the programmer write more gobbledy       gunk. That doesn&amp;rsquo;t help anybody.&lt;/b&gt;    &amp;ldquo;&lt;br /&gt;    &lt;br /&gt;&lt;br /&gt;         &lt;br /&gt;Today I believe the API in question declares &amp;ldquo;throws Exception&amp;rdquo;&amp;hellip; &lt;br /&gt;         &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>5 Minutes of Xtend</title>
      <link>http://chasethedevil.github.io/post/5-minutes-of-xtend/</link>
      <pubDate>Tue, 08 Apr 2014 17:37:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/5-minutes-of-xtend/</guid>
      <description>&lt;p&gt;There is a relatively new JVM based language, &lt;a href=&#34;https://www.eclipse.org/xtend&#34; target=&#34;_blank&#34;&gt;Xtend&lt;/a&gt;. Their homepage says &amp;ldquo;&lt;b&gt;JAVA 10, TODAY!&lt;/b&gt;&amp;rdquo;, so I thought I would give it a try, I was especially interested in operator overloading support, and the fact that it compiles to Java code, not Java byte code.&lt;br /&gt;&lt;br /&gt;Unfortunately, after 5 minutes with it, and pasting some non Java code in an xtend file, Eclipse hangs forever, even on restart. After creating another workspace, just to trash the new workspace a similar way. This is quite incredible for a nearly 2 years old project, on eclipse.org.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>http://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;I just stumbled upon &lt;a href=&#34;http://julialang.org/&#34;&gt;Julia&lt;/a&gt;, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.&lt;br /&gt;&lt;br /&gt;Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library. I had not heard of this algorithm or this library before, but the author, &lt;a href=&#34;http://math.mit.edu/~stevenj&#34;&gt;Steven G. Johnson&lt;/a&gt;, claims it is faster and as precise as Cody &amp;amp; SLATEC implementations. As &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/04/from-double-precision-normal-density-to.html&#34;&gt;I previously had a look at those algorithms&lt;/a&gt; and was quite impressed by Cody&amp;rsquo;s implementation.&lt;br /&gt;&lt;br /&gt;The &lt;a href=&#34;http://ab-initio.mit.edu/Faddeeva.cc&#34;&gt;source of Faddeeva&lt;/a&gt; shows a big list (100) of Chebychev expansions for various ranges of a normalized error function. I slightly modified the Faddeva code to compute directly the cumulative normal distribution, avoiding some exp(-x*x)*exp(x*x) calls on the way.&lt;br /&gt;&lt;br /&gt;Is it as accurate? I compared against a high precision implementation as in my previous test of cumulative normal distribution algorithms. And after replacing the exp(-x*x) with &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/03/cracking-double-precision-gaussian.html&#34;&gt;Cody&amp;rsquo;s trick&lt;/a&gt; to compute it with higher accuracy, here is how it looks (referenced as &amp;ldquo;Johnson&amp;rdquo;).&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-v3ZZs43eSpI/Ugo3ILYFIYI/AAAAAAAAGoE/zAIS-WFsrnk/s1600/snapshot37.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;241&#34; src=&#34;http://1.bp.blogspot.com/-v3ZZs43eSpI/Ugo3ILYFIYI/AAAAAAAAGoE/zAIS-WFsrnk/s640/snapshot37.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;I also measured performance on various ranges, and found out that this Johnson algorithm is around 2x faster than Cody (in Scala) and 30% faster than my optimization of Cody (using a table of exponentials for Cody&amp;rsquo;s trick).&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>http://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>&lt;p&gt;I was used to &lt;a href=&#34;http://www.scilab.org/&#34;&gt;Scilab&lt;/a&gt; for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.&lt;br /&gt;&lt;br /&gt;Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/05/sabr-with-new-hagan-pde-approach.html&#34;&gt;Arbitrage Free SABR problem&lt;/a&gt;. It worked nicely and is a bit nicer to read than my equivalent Scala program. But I was a bit surprised by the low performance.&lt;br /&gt;&lt;br /&gt;Then I heard about &lt;a href=&#34;http://www.gnu.org/software/octave/&#34;&gt;Octave&lt;/a&gt;, which is even closer to Matlab syntax than Scilab and started wondering if it was better or faster. Here are my results for 1000 points and 10 time-steps: &lt;br /&gt;&lt;br /&gt;Scilab 4.3s&lt;br /&gt;Octave 4.1s&lt;br /&gt;&lt;br /&gt;I then added the keyword sparse when I build the tridiagonal matrix and end up with:&lt;br /&gt;&lt;br /&gt;Scilab 0.04s&lt;br /&gt;Octave 0.02s&lt;br /&gt;Scala 0.034s (first run)&lt;br /&gt;Scala 0.004s (once Hotpot has kicked in)&lt;br /&gt;&lt;br /&gt;So Octave looks a bit better than Scilab in terms of performance. However I could not figure out from the documentation what algorithm was used for the cumulative normal distribution and if there was a monotonic spline interpolation in Octave.&lt;br /&gt;&lt;br /&gt;In general I find it impressive that Octave is faster than the first run of Scala or Java, and impressive as well that the Hotspot makes gain of x10.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The CUDA Performance Myth II</title>
      <link>http://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</link>
      <pubDate>Fri, 12 Jul 2013 15:23:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</guid>
      <description>&lt;p&gt;This is a kind of following to the &lt;a href=&#34;http://chasethedevil.blogspot.fr/2011/01/cuda-performance-myth.html&#34;&gt;CUDA performance myth&lt;/a&gt;. There is a recent news on the java concurrent mailing list about &lt;a href=&#34;http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/SplittableRandom.java?revision=1.7&amp;amp;view=markup&#34;&gt;SplittableRandom class&lt;/a&gt; proposed for JDK8. It is a new parallel random number generator a priori usable for Monte-Carlo simulations.&lt;br /&gt;&lt;br /&gt;It seems to rely on some very recent algorithm. There are some a bit older ones: the ancestor, L&amp;rsquo;Ecuyer &lt;a href=&#34;http://www.iro.umontreal.ca/~simardr/rng/MRG32k3a.c&#34;&gt;MRG32k3a&lt;/a&gt; that can be parallelized through relatively costless skipTo methods, a Mersenne Twister variant &lt;a href=&#34;http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MTGP/&#34;&gt;MTGP&lt;/a&gt;, and even the less rigourous XorWow popularized by NVidia CUDA.&lt;br /&gt;&lt;br /&gt;The book &lt;a href=&#34;http://my.safaribooksonline.com/book/-/9780123849885/chapter-16dot-parallelization-techniques-for-random-number-generators/232#X2ludGVybmFsX0J2ZGVwRmxhc2hSZWFkZXI/eG1saWQ9OTc4MDEyMzg0OTg4NS8yNDQ=&#34;&gt;GPU Computing Gems&lt;/a&gt; provides some interesting stats as to GPU vs CPU performance for various generators (L&amp;rsquo;Ecuyer, Sobol, and Mersenne Twister) &lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-lETLXaEkXn8/UeABqgvJndI/AAAAAAAAGiU/MzLslJR0R_c/s1600/snapshot32.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;640&#34; src=&#34;http://4.bp.blogspot.com/-lETLXaEkXn8/UeABqgvJndI/AAAAAAAAGiU/MzLslJR0R_c/s640/snapshot32.png&#34; width=&#34;572&#34; /&gt;&lt;/a&gt;&lt;/div&gt;A Quad core Xeon is only 4 times slower to generate normally distributed random numbers with Sobol. Fermi cards are faster now, but probably so are newer Xeons. I would have expected this kind of task to be the typical not too complex parallelizable task doable by a GPU, and yet the improvements are not very good (except if you look at raw random numbers, which is almost useless in applications). It confirms the idea that many real world algorithms are not so much faster with GPUs than with CPUs. I suppose what&amp;rsquo;s interesting is that the GPU abstractions forces you to be relatively efficient, while the CPU flexibility might make you lazy.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple &#39;Can Scala Do This?&#39; Questions</title>
      <link>http://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</link>
      <pubDate>Tue, 11 Jun 2013 00:28:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/simple-can-scala-do-this-questions/</guid>
      <description>&lt;p&gt;Today, a friend asked me if Scala could pass primitives (such as Double) by reference. It can be useful sometimes instead of creating a full blown object. In Java there is commons lang MutableDouble. It could be interesting if there was some optimized way to do that.&lt;/p&gt;

&lt;p&gt;One answer could be: it&amp;rsquo;s not functional programming oriented and therefore not too surprising this is not encouraged in Scala.&lt;/p&gt;

&lt;p&gt;Then he wondered if we could use it for C#.&lt;/p&gt;

&lt;p&gt;I know this used to be possible in Scala 1.0, I believe it&amp;rsquo;s not anymore since 2.x. This was a cool feature, especially if they had managed to develop strong libraries around it. I think it was abandoned to focus on other things, because of lack of resources, but it&amp;rsquo;s sad.&lt;/p&gt;

&lt;p&gt;Later today, I tried to use the nice syntax to return multiple values from a method:
var (a,b) = mymethod(1)&lt;/p&gt;

&lt;p&gt;I noticed you then could not do:
(a,b) = mymethod(2)&lt;/p&gt;

&lt;p&gt;So declaring a var seems pointless in this case.&lt;/p&gt;

&lt;p&gt;One way to achieve this is to:&lt;/p&gt;

&lt;p&gt;var tuple = mymethod(1)
var a = tuple._1
var b = tuple._2&lt;/p&gt;

&lt;p&gt;This does not look so nice.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SABR with the new Hagan PDE Approach</title>
      <link>http://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</link>
      <pubDate>Tue, 28 May 2013 15:56:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</guid>
      <description>&lt;p&gt;At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it&amp;rsquo;s just like a regular PDE with proper boundary conditions.&lt;br /&gt;&lt;br /&gt;I was wondering how it compared to Andreasen Huge results.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s1600/snapshot14.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s640/snapshot14.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;My first implementation was quite slow. I postulated it was likely the Math.pow function calls. It turns out they could be reduced a great deal. As a result, it&amp;rsquo;s now quite fast. But it would still be much slower than Andreasen Huge. Typically, one might use 40 time steps, while Andreasen Huge is 1, so it could be around a 40 to 1 ratio. In practice it&amp;rsquo;s likely to be less than 10x slower, but still.&lt;br /&gt;&lt;br /&gt;While looking at the implied volatilities I found something intriguing with Andreasen Huge: the implied volatilities from the refined solution using the corrected forward volatility look further away from the Hagan implied volatilitilies than without adjustment, and it&amp;rsquo;s quite pronounced at the money.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s1600/snapshot15.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s640/snapshot15.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Interestingly, the authors don&amp;rsquo;t plot that graph in their paper. They  plot a similar graph of their own closed form analytic formula, that is  in reality used to compute the forward volatility. I suppose that  because they calibrate and price through their method, they don&amp;rsquo;t really  care so much that the ATM prices don&amp;rsquo;t match Hagan original formula.&lt;br /&gt;&lt;br /&gt;We can see something else on that graph: Hagan PDE boundary is not as nice as Andreasen Huge boundary for high strikes (they use a Hagan like approx at the boundaries, this is why it crosses the Hagan implied volatilities there). &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s1600/snapshot16.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s640/snapshot16.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;If we use a simple option gamma = 0 boundary in Andreasen Huge, this results in a very similar shape as the Hagan PDE. This is because the option price is effectively 0 at the boundary.&lt;br /&gt;Hagan chose a specifically taylored Crank-Nicolson scheme. I was wondering how it fared when I reduced the number of time-steps. &lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s1600/snapshot17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s400/snapshot17.png&#34; height=&#34;190&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The answer is: not good. This is the typical Crank-Nicolson issue. It could be interesting to adapt the method to use Lawson-Morris-Goubet or TR-BDF2, or a simple Euler Richardson extrapolation. This would allow to use less time steps, as in practice, the accuracy is not so bad with 10 time steps only.&lt;br /&gt;&lt;br /&gt;What I like about the Hagan PDE approach is that the implied vols and the probability density converge well to the standard Hagan formula, when there is no negative density problem, for example for shorter maturities. This is better than Andreasen Huge, where there seems to be always 1 vol point difference. However their method is quite slow compared to the original simple analytic formula.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update March 2014&lt;/b&gt; - I have now a paper around this &amp;ldquo;&lt;a href=&#34;http://ssrn.com/abstract=2402001&#34;&gt;Finite Difference Techniques for Arbitrage Free SABR&lt;/a&gt;&amp;ldquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>&lt;p&gt;In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.&lt;br /&gt;&lt;br /&gt;Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an &lt;a href=&#34;http://bad-concurrency.blogspot.co.uk/2012/08/arithmetic-overflow-and-intrinsics.html&#34;&gt;intrinsic function call&lt;/a&gt; and that should be difficult to beat. However what if one is ok for a bit lower accuracy? Could a simple &lt;a href=&#34;http://www.siam.org/books/ot99/OT99SampleChapter.pdf&#34;&gt;Chebyshev polynomial expansion&lt;/a&gt; be faster?&lt;br /&gt;&lt;br /&gt;Out of curiosity, I tried a Chebyshev polynomial expansion with 10 coefficients stored in a final double array. I computed the coefficient using a precise quadrature (Newton-Cotes) and end up with 1E-9, 1E-10 absolute and relative accuracy on [-1,1].&lt;br /&gt;&lt;br /&gt;Here are the results of a simple sum of 10M random numbers:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.75s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.48s for ChebyshevExp sum=1.718281669341388E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;So while this simple implementation is actually faster than Math.exp (but only works within [-1,1]), FastMath from Apache commons maths, that relies on a table lookup algorithm is just faster (in addition to being more precise and not limited to [-1,1]).&lt;br /&gt;&lt;br /&gt;Of course if I use only 5 coefficients, the speed is better, but the relative error becomes around 1e-4 which is unlikely to be satisfying for a finance application.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.78s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.27s for ChebyshevExp sum=1.718193001875838E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/04/root-finding-in-lord-kahl-method-to.html&#34;&gt;previous post&lt;/a&gt;, I explored the &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord-Kahl method&lt;/a&gt; to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise.&lt;br /&gt;&lt;br /&gt;However the big trick in it is to find the optimal alpha used in the integration. A suboptimal alpha will often lead to high inaccuracy, because of some strong oscillations that will appear in the integration. So the method is robust only if the root finding (for the optimal alpha) is robust.&lt;br /&gt;&lt;br /&gt;The original paper looks the Ricatti equation for B where B is the following term in the characteristic function:&lt;br /&gt;$$\phi(u) = e^{iuf+A(u,t)+B(u,t)\sigma&lt;em&gt;0}$$&lt;br /&gt;The solution defines the (\alpha&lt;/em&gt;{max}) where the characteristic function explodes. While the Ricatti equation is complex but not complicated:&lt;br /&gt;$$ dB/dt = \hat{\alpha}(u)-\beta(u) B+\gamma B^2 $$&lt;br /&gt;I initially did not understand its role (to compute (\alpha_{max})), so that, later, one can compute alpha&lt;em&gt;optimal with a good bracketing. The bracketing is particularly important to use a decent solver, like the Brent solver. Otherwise, one is left with, mostly, Newton&amp;rsquo;s method. It turns out that I explored a reduced function, which is quite simpler than the Ricatti and seems to work in all the cases I have found/tried: solve $$1/B = 0$$&lt;br /&gt;&amp;nbsp;If B explodes, (\phi) will explode. The trick, like when solving the Ricatti equation, is to have either a good starting point (for Newton) or, better, a bracketing. It turns out that Lord and Kahl give a bracketing for (1/B), even if they don&amp;rsquo;t present it like this: their (\tau&lt;/em&gt;{D+}) on page 10 for the lower bracket, and (\tau&lt;em&gt;+) for the upper bracket. (\tau&lt;/em&gt;+) will make (1/B) explode, exactly. One could also find the next periods by adding (4\pi/t) instead of (2\pi/t) like they do to move from (\tau&lt;em&gt;{D+}) to (\tau&lt;/em&gt;+). But this does not have much interest as we don&amp;rsquo;t want to go past the first explosion.&lt;br /&gt;&lt;br /&gt;It&amp;rsquo;s quite interesting to see that my simple approach is actually closely related to the more involved Ricatti approach. The starting point could be the same. Although it is much more robust to just use Brent solver on the bracketed max. I actually believe that the Ricatti equation explodes at the same points, except, maybe for some rare combination of Heston parameters.&lt;br /&gt;&lt;br /&gt;From a coding perspective, I found that &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDUQFjAA&amp;amp;url=http%3A%2F%2Fcommons.apache.org%2Fmath&amp;amp;ei=kctmUajvIoWs0QXC-4HoDQ&amp;amp;usg=AFQjCNFaOPmpFKpVp5Ba9fVtRNSgefKwhA&amp;amp;sig2=vQZ7geKUB1iGDu5cDOjO0g&amp;amp;bvm=bv.45107431,d.d2k&#34;&gt;Apache commons maths&lt;/a&gt; was a decent library to do complex calculus or solve/minimize functions. The complex part was better than some in-house implementation: for example the square root was more precise in commons maths, and the solvers are robust. It even made me think that it is often a mistake to reinvent to wheel. It&amp;rsquo;s good to choose the best implementations/algorithms as possible. But reinventing a Brent solver??? a linear interpolator??? Also the commons maths library imposes a good structure. In house stuff tends to be messy (not real interfaces, or many different ones). I believe the right approach is to use and embrace/extends Apache commons maths. If some algorithms are badly coded/not performing well, then write your own using the same kind of interfaces as commons maths (or some other good maths library).&lt;br /&gt;&lt;br /&gt;The next part of this series on Lord-Kahl method is &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/04/root-finding-in-lord-kahl-method-to_12.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price</title>
      <link>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</link>
      <pubDate>Tue, 09 Apr 2013 19:49:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</guid>
      <description>&lt;p&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/--7JNbvzFOTA/UWRTIdNqqvI/AAAAAAAAGUw/Aa7HmEB0LlU/s1600/Screenshot+from+2013-04-09+19:42:09.png&#34; imageanchor=&#34;1&#34; style=&#34;clear: left; float: left; margin-bottom: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/--7JNbvzFOTA/UWRTIdNqqvI/AAAAAAAAGUw/Aa7HmEB0LlU/s320/Screenshot+from+2013-04-09+19:42:09.png&#34; width=&#34;297&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;I just tried to implement &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord Kahl algorithm to compute the Heston call price&lt;/a&gt;. The big difficulty of their method is to find the optimal alpha.  That&amp;rsquo;s what make it work or break. The tricky part is that the function  of alpha we want to minimize has multiple discontinuities (it&amp;rsquo;s  periodic in some ways). This is why the authors rely on the computation  of an alpha_max: bracketing is very important, otherwise your optimizer  will jump the discontinuity without even noticing it, while you really  want to stay in the region before the first discontinuity.&lt;br /&gt;&lt;br /&gt;To find alpha_max, they solve a non linear differential equation,  for which I would need a few more readings to really understand it.  Given that the problem looked simple: if you graph the function to  minimize, it seems so simple to find the first discontinuity. So I just  tried to do it directly. Numerically, I was surprised it was not so  simple. I did find a solution that, amazingly seems to work in all the  examples of the paper, but it&amp;rsquo;s luck. I use Newton-Raphson to find the  discontinuity, on a reduced function where the discontinuity really  lies. I solve the inverse of the discontinuity so that I can just solve  for 0. Earlier on I reduced the function too much and it did not work,  this is why I believe it is not very robust. Newton-Raphson is quite  simple, but also simple to understand why it breaks if it breaks, and  does not need a bracketing (what I am looking for in the first place).  Once I find the discontinuity, I can just use Brent on the right  interval and it works well.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-4u9Lo1gSCTc/UWRTIaG6yQI/AAAAAAAAGU8/WDwK_Hu-n_0/s1600/Screenshot+from+2013-04-09+19%253A42%253A30.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://2.bp.blogspot.com/-4u9Lo1gSCTc/UWRTIaG6yQI/AAAAAAAAGU8/WDwK_Hu-n_0/s320/Screenshot+from+2013-04-09+19%253A42%253A30.png&#34; width=&#34;299&#34; /&gt;&lt;/a&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-veJTNC7C4Rk/UWRT_SjU-tI/AAAAAAAAGVE/DXacavkVhhQ/s1600/Screenshot+from+2013-04-09+19%253A45%253A39.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/-veJTNC7C4Rk/UWRT_SjU-tI/AAAAAAAAGVE/DXacavkVhhQ/s320/Screenshot+from+2013-04-09+19%253A45%253A39.png&#34; width=&#34;299&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the end, it&amp;rsquo;s neat to be able to compute option prices under  machine epsilon. But in practice, it&amp;rsquo;s probably not that useful. For  calibration, those options should have a very small (insignificant)  weight. The only use case I found is really for graphing so that you  don&amp;rsquo;t have some flat extrapolation too quickly, especially for short  maturities. I was curious as well about the accuracy of some  approximations of the implied volatility in the wings, to see if I could  use them instead of all this machinery.&lt;br /&gt;&lt;br /&gt;In any case I did not think that such a simple problem was so challenging numerically.&lt;br /&gt;&lt;br /&gt;There is a &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/04/root-finding-in-lord-kahl-method-to_11.html&#34;&gt;part II&lt;/a&gt; to this article.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>http://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;Marsaglia in &lt;a href=&#34;http://www.jstatsoft.org/v11/a05/paper&#34;&gt;his paper on Normal Distribution&lt;/a&gt; made the same mistake I initially did while trying to verify &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/03/cracking-double-precision-gaussian.html&#34;&gt;the accuracy of the normal density&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.6 because of the truncation at machine epsilon precision. Using Python mpmath, one can see that:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;gt;&amp;gt;&amp;gt; mpf(-16.6)&lt;br /&gt;mpf(&amp;lsquo;-16.6000000000000014210854715202004&amp;rsquo;)&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;This is the more accurate representation if one goes beyond double precision (here 30 digits). And the value of the cumulative normal distribution is:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;br /&gt;&lt;/span&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(-16.6)&lt;br /&gt;mpf(&amp;lsquo;3.4845465199503256054808152068743e-62&amp;rsquo;)&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;It is different from:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(mpf(&amp;ldquo;-16.6&amp;rdquo;))&lt;br /&gt;mpf(&amp;lsquo;3.48454651995040810217553910503186e-62&amp;rsquo;)&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;where in this case it is really evaluated around -16.6 (up to 30 digits precision). Marsaglia gives this second number as reference. But all the other algorithms will actually take as input the first input. It is more meaningful to compare results using the exact same input. Using human readable but computer truncated numbers is not the best.  The cumulative normal distribution will often be computed using some output of some calculation where one does not have an exact human readable input.&lt;br /&gt;&lt;br /&gt;The standard code for Ooura and Schonfelder (as well as Marsaglia) algorithms for the cumulative normal distribution don&amp;rsquo;t use Cody&amp;rsquo;s trick to evaluate the exp(-x*x). This function appears in all those implementations because it is part of the dominant term in the usual expansions. Out of curiosity, I replaced this part with Cody trick. For Ooura I also made minor changes to make it work directly on the CND instead of going through the error function erfc indirection. Here are the results without the Cody trick (except for Cody):&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-mNamXn3QlGQ/UVrMEkkqrWI/AAAAAAAAGUU/1hihN6pqiC4/s1600/Screenshot+from+2013-04-02+14:15:51.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;307&#34; src=&#34;http://4.bp.blogspot.com/-mNamXn3QlGQ/UVrMEkkqrWI/AAAAAAAAGUU/1hihN6pqiC4/s640/Screenshot+from+2013-04-02+14:15:51.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;and with it:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-bFq5HVfCCOs/UVrMEjJbuJI/AAAAAAAAGUQ/uyHKMbEsmco/s1600/Screenshot+from+2013-04-02+14:14:02.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;307&#34; src=&#34;http://4.bp.blogspot.com/-bFq5HVfCCOs/UVrMEjJbuJI/AAAAAAAAGUQ/uyHKMbEsmco/s640/Screenshot+from+2013-04-02+14:14:02.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;All 3 algorithms are now of similiar accuracy (note the difference of scale compared to the previous graph), with Schonfelder being a bit worse, especially for x &amp;gt;= -20. If one uses only easily representable numbers (for example -37, -36,75, -36,5, &amp;hellip;) in double precision then, of course, Cody trick importance won&amp;rsquo;t be visible and here is how the 3 algorithms would fare with or without Cody trick:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-zOHn3Lp96zY/UVrM8iOtgaI/AAAAAAAAGUg/7GdvJ534F60/s1600/Screenshot+from+2013-04-02+11:24:08.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;301&#34; src=&#34;http://1.bp.blogspot.com/-zOHn3Lp96zY/UVrM8iOtgaI/AAAAAAAAGUg/7GdvJ534F60/s400/Screenshot+from+2013-04-02+11:24:08.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Schonfelder looks now worse than it actually is compared to Cody and Ooura.&lt;br /&gt;&lt;br /&gt;To conclude, if someone claims that a cumulative normal distribution is up to double precision accuracy and it does not use any tricks to compute exp(-x*x), then beware, it probably is quite a bit less than double precision.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scala is Mad (part 2)</title>
      <link>http://chasethedevil.github.io/post/scala-is-mad-part-2/</link>
      <pubDate>Wed, 13 Feb 2013 16:20:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/scala-is-mad-part-2/</guid>
      <description>&lt;p&gt;I still did not abandon Scala despite my &lt;a href=&#34;http://chasethedevil.blogspot.fr/2012/12/scala-is-mad.html&#34;&gt;previous post&lt;/a&gt;, mainly because I have already quite a bit of code, and am too lazy to port it. Furthermore the issues I detailed were not serious enough to motivate a switch. But these days I am more and more fed up with Scala, especially because of the Eclipse plugin. I tried the newer, the beta, and the older, the stable, the conclusion is the same. It&amp;rsquo;s welcome but:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;code completion is not great compared to Java. For example one does not seem to be able to see the constructor parameters, or the method parameters can not be automatically populated.&lt;/li&gt;&lt;li&gt;the plugin makes Eclipse &lt;em&gt;very&lt;/em&gt; slow. Everything seems at least 3-5x slower. On the fly compilation is also much much slower than Java&amp;rsquo;s.&lt;/li&gt;&lt;/ul&gt;&lt;br /&gt;It&amp;rsquo;s nice to type less, but if overall writing is slower because of the above issues, it does not help. Beside curiosity of a new language features, I don&amp;rsquo;t see any point in Scala today, even if some of the ideas are interesting. I am sure it will be forgotten/abandoned in a couple of years. Today, if I would try a new language, I would give Google Go a try: I don&amp;rsquo;t think another big language can make it/be useful on the JVM (beside a scripting kind of language, like JavaScript or Jython).&lt;br /&gt;&lt;br /&gt;Google Go focuses on the right problem: concurrency. It also is not constrained to JVM limitation (on the other side one can not use a Java library - but open source stuff is usually not too difficult to port from one language to another). It has one of the fastest compilers. It makes interesting practical choices: no inheritance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scala is Mad</title>
      <link>http://chasethedevil.github.io/post/scala-is-mad/</link>
      <pubDate>Wed, 12 Dec 2012 16:07:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/scala-is-mad/</guid>
      <description>&lt;p&gt;I spent quick a bit of time to figure out why something that is usually simple to do in Java did not work in Scala: Arrays and ArrayLists with generics.&lt;br /&gt;&lt;br /&gt;For some technical reason (type erasure at the JVM level), Array sometimes need a parameter with a ClassManifest !?! a generic type like [T :&amp;lt; Point : ClassManifest] need to be declared instead of simply [T :&amp;lt; Point].&lt;br /&gt;&lt;br /&gt;And then the quickSort method somehow does not work if invoked on a generic&amp;hellip; like quickSort(points) where points: Array[T]. I could not figure out yet how to do this one, I just casted to points.asInstanceOf[Array[Point]], quite ugly.&lt;br /&gt;&lt;br /&gt;In contrast I did not even have to think much to write the Java equivalent. Generics in Scala, while having a nice syntax, are just crazy. This is something that goes beyond generics. Some of the Scala library and syntax is nice, but overall, the IDE integration is still very buggy, and productivity is not higher.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update Dec 12 2012&lt;/b&gt;: here is the actual code (this is kept close to the Java equivalent on purpose):&lt;br /&gt;&lt;pre&gt;object Point {&lt;br /&gt;  def sortAndRemoveIdenticalPoints&lt;a href=&#34;points : Array[T]&#34;&gt;T &amp;lt;: Point : ClassManifest&lt;/a&gt; : Array[T] = {&lt;br /&gt;      Sorting.quickSort(points.asInstanceOf[Array[Point]])&lt;br /&gt;      val l = new ArrayBuffer&lt;a href=&#34;points.length&#34;&gt;T&lt;/a&gt;&lt;br /&gt;      var previous = points(0)&lt;br /&gt;      l += points(0)&lt;br /&gt;      for (i &amp;lt;- 1 until points.length) {&lt;br /&gt;        if(math.abs(points(i).value - previous.value)&amp;lt; Epsilon.MACHINE_EPSILON_SQRT) {&lt;br /&gt;          l += points(i)&lt;br /&gt;        }&lt;br /&gt;      }&lt;br /&gt;      return l.toArray&lt;br /&gt;    }&lt;br /&gt;    return points&lt;br /&gt;  }&lt;br /&gt;}&lt;br /&gt;&lt;br /&gt;class Point(val value: Double, val isMiddle: Boolean) extends Ordered[Point] {&lt;br /&gt;  def compare(that: Point): Int = {&lt;br /&gt;    return math.signum(this.value - that.value).toInt&lt;br /&gt;  }&lt;br /&gt;}&lt;br /&gt;&lt;!-----&gt;&lt;!--:--&gt;&amp;lt;/-&amp;gt;&amp;lt;/:&amp;gt;&lt;/pre&gt;In Java one can just use Arrays.sort(points) if points is a T[]. And the method can work with a subclass of Point.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scala Again</title>
      <link>http://chasethedevil.github.io/post/scala-again/</link>
      <pubDate>Mon, 06 Feb 2012 17:52:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/scala-again/</guid>
      <description>&lt;p&gt;I am trying &lt;a href=&#34;http://www.scala-lang.org/&#34;&gt;Scala&lt;/a&gt; again. Last time, several years ago, I played around with it as a web tool, combining it with a Servlet Runner like Tomcat. This time, I play around with it for some quantitative finance experiments.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Why Scala?&lt;/b&gt; It still seem the most advanced alternative to Java on the JVM, and the mix of functional programming and OO programming is interesting. Furthermore it goes quite far as it ships with its own library. I was curious to see if I could express some things better with Scala.&lt;br /&gt;&lt;br /&gt;Here are my first impressions after a week:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;I like the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt; keyword. It avoids the messy singleton pattern, or the classes with many static methods. I think it makes things much cleaner to not use static at all but distinguish between &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt; &amp;amp; &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;class&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;I like the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Array[Double]&lt;/span&gt;, and especially &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ArrayBuffer[Double]&lt;/span&gt;. Finally we don&amp;rsquo;t have to worry between the &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Double&lt;/span&gt; and &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;double&lt;/span&gt; performance issues.&lt;/li&gt;&lt;li&gt;I was a bit annoyed by &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;a(i)&lt;/span&gt; instead of &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;a[i]&lt;/span&gt; but it makes sense. I wonder if there is a performance implication for arrays, hopefully not.&lt;/li&gt;&lt;li&gt;I like the real properties, automatic getter/setter: less boilerplate code, less getThis(), setThat(toto).&lt;/li&gt;&lt;li&gt;Very natural interaction with Java libraries. &lt;/li&gt;&lt;li&gt;I found a good use of &lt;b&gt;case classes&lt;/b&gt; (to my surprise): typically an enum that can have some well defined parameters, and that you don&amp;rsquo;t want to make a class (because it&amp;rsquo;s not). My use case was to define boundaries of a spline.&lt;/li&gt;&lt;li&gt;I love the formatter in the scala (eclipse) IDE. Finally a formatter in eclipse that does not produce crap.&lt;/li&gt;&lt;/ul&gt;Now things I still need time to get used to:&lt;br /&gt;&lt;ul&gt;&lt;li&gt; member variable declared implicitly in the constructor. I first made the mistake (still?) to declare some variables twice.&lt;/li&gt;&lt;li&gt;I got hit by starting a line with a &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;+&lt;/span&gt; instead of ending with a &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;+&lt;/span&gt;. It is dangerous, but it certainly makes the code more consistent.&lt;/li&gt;&lt;li&gt;Performance impacts: I will need to take a look at the bytecode for some scala constructs to really understand the performance impact of some uses. For example I tend to use while loops instead of for comprehension after some scary post of the Twitter guys about for comprehension. But at first, it looks as fast as Java.&lt;/li&gt;&lt;li&gt;I wrote my code a bit fast. I am sure I could make use of more Scala features.&lt;/li&gt;&lt;li&gt;The scala IDE in eclipse 3.7.1 has known issues. I wish it was a bit more functional, but it&amp;rsquo;s quite ok (search for references works, renaming works to some extent).&lt;/li&gt;&lt;li&gt;Scala unit tests: I used scala tests, but it seems a bit funny at first. Also I am not convinced by the syntax that avoid method names and prefer test(&amp;ldquo;test name&amp;rdquo;). It makes it more difficult to browse the source code.&lt;/li&gt;&lt;/ul&gt;Some things they should consider:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Integrate directly a Log API. I just use SLF4J without any scala wrapper, but it feels like it should be part of the standard API (even if that did not work out so well for Sun).&lt;/li&gt;&lt;li&gt;Double.Epsilon is not the machine epsilon: very strange. I found out somewhere else there was the machine epsilon, don&amp;rsquo;t remember where because I ended up just making a small &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;object&lt;/span&gt;.&lt;/li&gt;&lt;li&gt;Unit tests should be part of the standard API.&lt;/li&gt;&lt;/ul&gt;Overall I found it quite exciting as there are definitely new ways to solve problems. It was a while since I had been excited with actual coding.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>