<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quant on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/quant/</link>
    <description>Recent content in Quant on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 22 Aug 2015 16:13:00 +0000</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/tags/quant/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Go for Monte-Carlo</title>
      <link>http://chasethedevil.github.io/post/go-for-monte-carlo/</link>
      <pubDate>Sat, 22 Aug 2015 16:13:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/go-for-monte-carlo/</guid>
      <description>&lt;p&gt;I have &lt;a href=&#34;http://chasethedevil.blogspot.fr/2015/04/modern-programming-language-for-monte.html&#34;&gt;looked&lt;/a&gt; a few months ago already at Julia, Dart, Rust and Scala programming languages to see how practical they could be for a simple Monte-Carlo option pricing.&lt;br /&gt;&lt;br /&gt;I forgot &lt;a href=&#34;https://golang.org/&#34;&gt;the Go language&lt;/a&gt;. I had tried it 1 or 2 years ago, and at that time, did not enjoy it too much. Looking at Go 1.5 benchmarks on the&lt;a href=&#34;http://benchmarksgame.alioth.debian.org/&#34;&gt; computer language shootout&lt;/a&gt;, I was surprised that it seemed so close to Java performance now, while having a GC that guarantees pauses of less 10ms and consuming much less memory.&lt;br /&gt;&lt;br /&gt;I am in general a bit skeptical about those benchmarks, some can be rigged. A few years ago, I &lt;a href=&#34;http://chasethedevil.blogspot.fr/2009/01/end-of-rings-around-plain-java-better.html&#34;&gt;tried my hand at the thread ring&lt;/a&gt; test, and found that it actually performed fastest on a single thread while it is supposed to measure the language threading performance. I looked yesterday at one Go source code (I think it was for pidigits) and saw that it just called a C library (gmp) to compute with big integers. It&amp;rsquo;s no surprise then that Go would be faster than Java on this test.&lt;br /&gt;&lt;br /&gt;So what about my small Monte-Carlo test?&lt;br /&gt;Well it turns out that Go is quite fast on it:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Multipl. Rust&amp;nbsp;&amp;nbsp;&amp;nbsp; Go&lt;br /&gt;1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.005&amp;nbsp; 0.007&lt;br /&gt;10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.03&amp;nbsp;&amp;nbsp; 0.03&lt;br /&gt;100&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.21&amp;nbsp;&amp;nbsp; 0.29&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;1000&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.01&amp;nbsp;&amp;nbsp; 2.88&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;It is faster than Java/Scala and not too far off Rust, except if one uses FastMath in Scala, then the longest test is slighly faster with Java (not the other ones).&lt;br /&gt;&lt;br /&gt;There are some issues with the Go language: there is no operator overloading, which can make matrix/vector algebra more tedious and there is no generic/template. The later is somewhat mitigated by the automatic interface implementation. And fortunately for the former, complex numbers are a standard type. Still, automatic differentiation would be painful.&lt;br /&gt;&lt;br /&gt;Still it was extremely quick to grasp and write code, because it&amp;rsquo;s so simple, especially when compared to Rust. But then, contrary to Rust, there is not as much safety provided by the language. Rust is quite impressive on this side (but unfortunately that implies less readable code). I&amp;rsquo;d say that Go could become a serious alternative to Java.&lt;br /&gt;&lt;br /&gt;I also found an interesting minor performance issue with the default Go &lt;a href=&#34;https://golang.org/src/math/rand/rand.go&#34;&gt;Rand.Float64&lt;/a&gt;, the library convert an Int63 to a double precision number this way:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;func (r *Rand) Float64() float64 {&lt;/pre&gt;&lt;pre&gt;  f := float64(r.Int63()) / (1 &amp;lt;&amp;lt; 63)&lt;br /&gt;  if f == 1 {&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L128&#34;&gt;    &lt;/span&gt;f = 0&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L129&#34;&gt;  }&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L130&#34;&gt;  &lt;/span&gt;return f&lt;br /&gt; }&lt;/pre&gt;&lt;br /&gt;I was interested in having a number in (0,1) and not [0,1), so I just used the conversion pattern from MersenneTwister 64 code:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;f := (float64(r.Int63() &amp;gt;&amp;gt; 11) + 0.5) * (1.0/4503599627370496.0)&lt;/pre&gt;&lt;pre&gt;&amp;nbsp;&lt;/pre&gt;The reasoning behind this later code is that the mantissa is 52 bits, and this is the most accuracy we can have between 0 and 1. There is no need to go further, this also avoids the issue around 1. It&amp;rsquo;s also straightforward that is will preserve the uniform property, while it&amp;rsquo;s not so clear to me that r.Int63()/2^63 is going to preserve uniformity as double accuracy is higher around 0 (as the exponent part can be used there) and lesser around 1: there is going to be much more multiple identical results near 1 than near 0.&lt;br /&gt;&lt;br /&gt;It turns out that the if check adds 5% performance penalty on this test, likely because of processor caching issues. I was surprised by that since there are many other ifs afterwards in the code, for the inverse cumulative function, and for the payoff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bumping Correlations</title>
      <link>http://chasethedevil.github.io/post/bumping-correlations/</link>
      <pubDate>Sat, 25 Jul 2015 18:36:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/bumping-correlations/</guid>
      <description>&lt;p&gt;In his book &amp;ldquo;&lt;i&gt;Monte Carlo Methods in Finance&lt;/i&gt;&amp;rdquo;, P. Jäckel explains a simple way to clean up a correlation matrix. When a given correlation matrix is not positive semi-definite, the idea is to do a &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular value decomposition&lt;/a&gt; (SVD), replace the negative eigenvalues by 0, and renormalize the corresponding eigenvector accordingly.&lt;br /&gt;&lt;br /&gt;One of the cited applications is &amp;ldquo;&lt;i&gt;stress testing and scenario analysis for market risk&lt;/i&gt;&amp;rdquo; or &amp;ldquo;&lt;i&gt;comparative pricing in order to ascertain the extent of correlation exposure for multi-asset derivatives&lt;/i&gt;&amp;rdquo;, saying that &amp;ldquo;&lt;i&gt;In many of these cases we end up with a matrix that is no longer positive semi-definite&lt;/i&gt;&amp;rdquo;.&lt;br /&gt;&lt;br /&gt;It turns out that if one bumps an invalid correlation matrix (the input), that is then cleaned up automatically, the effect can be a very different bump. Depending on how familiar you are with SVD, this could be more or less obvious from the procedure,&lt;br /&gt;&lt;br /&gt;As a simple illustration I take the matrix representing 3 assets A, B, C with rho_ab = -0.6, rho_ac = rho_bc = -0.5.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.60000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.60000&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.50000&amp;nbsp; -0.50000&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;For those rho_ac and rho_bc, the correlation matrix is not positive definite unless rho_ab in in the range (-0.5, 1). One way to verify this is to use the fact that positive definiteness is equivalent to a positive determinant. The determinant will be 1 - 2*0.25 - rho_ab^2 + 2*0.25*rho_ab.&lt;br /&gt;&lt;br /&gt;After using P. Jaeckel procedure, we end up with: &lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56299&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.56299&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.46745&amp;nbsp; -0.46745&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;If we bump now rho_bc by 1% (absolute), we end up after cleanup with:&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56637&amp;nbsp; -0.47045&lt;br /&gt;&amp;nbsp; -0.56637&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46081&lt;br /&gt;&amp;nbsp; -0.47045&amp;nbsp; -0.46081 &amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;It turns out that rho_bc has changed by only 0.66% and rho_ac by -0.30%, rho_ab by -0.34%. So our initial bump (0,0,1) has been translated to a bump (-0.34, -0.30, 0.66). In other words, it does not work to compute sensitivities.&lt;br /&gt;&lt;br /&gt;One can optimize to obtain the nearest correlation matrix in some norm. Jaeckel proposes a hypersphere decomposition based optimization, using as initial guess the SVD solution. &lt;a href=&#34;https://nickhigham.wordpress.com/2013/02/13/the-nearest-correlation-matrix/&#34;&gt;Higham proposed a specific algorithm&lt;/a&gt; just for that purpose. It turns out that on this example, they will converge to the same solution (if we use the same norm). I tried out of curiosity to see if that would lead to some improvement. The first matrix becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56435&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.56435&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.46672&amp;nbsp; -0.46672&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;And the bumped one becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56766&amp;nbsp; -0.46984&lt;br /&gt;&amp;nbsp; -0.56766&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46002&lt;br /&gt;&amp;nbsp; -0.46984&amp;nbsp; -0.46002&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;We find back the same issue, rho_bc has changed by only 0.67%, rho_ac by -0.31% and rho_ab by -0.33%. We also see that the SVD correlation or the real near correlation matrix are quite close, as noticed by P. Jaeckel.&lt;br /&gt;&lt;br /&gt;Of course, one should apply the bump directly to the cleaned up matrix, in which case it will actually work as expected, unless our bump produces another non positive definite matrix, and then we would have correlation leaking a bit everywhere. It&amp;rsquo;s not entirely clear what kind of meaning the risk figures would have then.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Andreasen Huge extrapolation</title>
      <link>http://chasethedevil.github.io/post/andreasen-huge-extrapolation/</link>
      <pubDate>Mon, 13 Jul 2015 17:35:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/andreasen-huge-extrapolation/</guid>
      <description>&lt;p&gt;There are not many arbitrage free extrapolation schemes. Benaim et al. extrapolation is one of the few that claims it. However, despite the paper&amp;rsquo;s title, it is not truely arbitrage free. The density might be positive, but the forward is not preserved by the implied density. It can also lead to wings that don&amp;rsquo;t obey Lee&amp;rsquo;s moments condition.&lt;br /&gt;&lt;br /&gt;On a Wilmott forum, &lt;a href=&#34;http://www.wilmott.com/messageview.cfm?catid=4&amp;amp;threadid=95309&#34;&gt;P. Caspers proposed&lt;/a&gt; the following counter-example based on extrapolating SABR: ( \alpha=15\%, \beta=80\%, \nu=50\%, \rho=-48\%, f=3\%, T=20.0 ). He cut this smile at 2.5% and 6% and used the BDK extrapolation scheme with mu=nu=1.&lt;br /&gt;&lt;br /&gt;A truly arbitrage free extrapolation can be obtained through &lt;a href=&#34;http://ssrn.com/abstract=1694972&#34;&gt;Andreasen Huge volatility interpolation&lt;/a&gt;, making sure the grid is wide enough to allow extrapolation. Their method is basically a one step finite difference implicit Euler scheme applied to a local volatility parameterization that has as many parameters than prices. The method is presented with piecewise constant local volatility, but actually used with piecewise linear local volatility in their example.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-Cp1keeZvJjY/VaPaq-_ttVI/AAAAAAAAIFU/3M-0n5N4eqw/s1600/Screenshot-Untitled%2BWindow-5.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://3.bp.blogspot.com/-Cp1keeZvJjY/VaPaq-_ttVI/AAAAAAAAIFU/3M-0n5N4eqw/s400/Screenshot-Untitled%2BWindow-5.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Smile&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-mG5xhhikk5Y/VaPWmDv875I/AAAAAAAAIFA/X8SU8NUmEAE/s1600/Screenshot-Untitled%2BWindow-4.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://2.bp.blogspot.com/-mG5xhhikk5Y/VaPWmDv875I/AAAAAAAAIFA/X8SU8NUmEAE/s400/Screenshot-Untitled%2BWindow-4.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Density with piecewise linear local volatility&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;There is still a tiny oscillation that makes the density negative, but one understands why typical extrapolations fail on the example: the change in density must be very steep.&lt;br /&gt;Note that moving the left extrapolation point even closer to the forward might fix BDK negative density, but we  are already very close, and we can really wonder if going closer is  really a good idea since we would effectively use a somewhat arbitrary  extrapolation in most of the interpolation zone.&lt;br /&gt;&lt;br /&gt;It turns out that we can also use a cubic spline local volatility with linear extrapolation, and the density would look then: &lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-UytrsK6ficA/VaPWr-BEHmI/AAAAAAAAIFI/BtzCxV0MCSI/s1600/Screenshot-Untitled%2BWindow-3.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://2.bp.blogspot.com/-UytrsK6ficA/VaPWr-BEHmI/AAAAAAAAIFI/BtzCxV0MCSI/s400/Screenshot-Untitled%2BWindow-3.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Density with cubic spline local volatility&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Interestingly, the right side of the density is much better captured.&lt;br /&gt;The wiggle persists, although it is smaller. This is likely due to the fact that I am using a cubic spline on top of the finite difference prices (in order to have a C2 density). Using a better C2 convexity preserving interpolation would likely remove this artefact.&lt;br /&gt;&lt;br /&gt;Those figures also show why relying just on extrapolation to fix SABR is not necessarily a good idea: even a real arbitrage free extrapolation will make a not so meaningful density. The proper solution is to really use &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/12/arbitrage-free-sabr-another-view-on.html&#34;&gt;Hagan&amp;rsquo;s arbitrage free SABR PDE&lt;/a&gt;, which would be as nearly fast in this case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unintuitive behavior of the Black-Scholes formula - negative volatilities in displaced diffusion extrapolation</title>
      <link>http://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</link>
      <pubDate>Tue, 07 Jul 2015 16:43:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</guid>
      <description>&lt;p&gt;I am looking at various extrapolation schemes of the implied volatilities. An interesting one I stumbled upon is due to Kahale. Even if &lt;a href=&#34;http://nkahale.free.fr/papers/Interpolation.pdf&#34;&gt;his paper&lt;/a&gt; is on interpolation, there is actually a small paragraph on using the same kind of function for extrapolation. His idea is to simply lookup the standard deviation \( \Sigma \) and the forward \(f\) corresponding to a given market volatility and slope:
$$ c_{f,\Sigma} = f N(d_1) - k N(d_2)$$
with
$$ d_1 = \frac{\log(f/k)+\Sigma^&lt;sup&gt;2&lt;/sup&gt;&amp;frasl;&lt;sub&gt;2&lt;/sub&gt;}{\Sigma} $$&lt;/p&gt;

&lt;p&gt;We have simply:
$$ c&amp;rsquo;(k) = - N(d_2)$$&lt;/p&gt;

&lt;p&gt;He also proves that we can always find those two parameters for any \( k_0 &amp;gt; c_0 &amp;gt; 0,  -1 &amp;lt; c_0&amp;rsquo; &amp;lt; 0 \)&lt;/p&gt;

&lt;p&gt;Then I had the silly idea of trying to match with a put&amp;nbsp; instead of a call for the left wing (as those are out-of-the-money, and therefore easier to invert numerically). It turns out that it works in most cases in practice and produces relatively nice looking extrapolations, but it does not always work. This is because contrary to the call, the put value is bounded with \(f\).
$$ p_{f,\Sigma} = k N(-d_2) - f N(-d_1)$$&lt;/p&gt;

&lt;p&gt;Inverting \(p_0&amp;rsquo;\) is going to lead to a specific \(d&lt;em&gt;2\), and you are not guaranteed that you can push \(f\) high and have \(p&lt;/em&gt;{f,\Sigma}\) large enough to match \(p_0\). As example we can just take \(p_0 \geq k N(-d_2)\) which will only be matched if \(f \leq 0\).&lt;/p&gt;

&lt;p&gt;This is slightly unintuitive as put-call parity would suggest some kind of equivalence. The problem here is that we would need to consider the function of (k) instead of \(f\) for it to work, so we can&amp;rsquo;t really work with a put directly.&lt;/p&gt;

&lt;p&gt;Here are the two different extrapolations on Kahale own example:
&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-UcomxGsx_r0/VZvSdzoBBVI/AAAAAAAAIEU/_V542xidbgc/s1600/Screenshot-Untitled%2BWindow.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-UcomxGsx_r0/VZvSdzoBBVI/AAAAAAAAIEU/_V542xidbgc/s400/Screenshot-Untitled%2BWindow.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing with calls (blue doted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-sDl37fFAImE/VZvSd5eWCgI/AAAAAAAAIEQ/tOUG7nHrg_Q/s1600/Screenshot-Untitled%2BWindow-1.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-sDl37fFAImE/VZvSd5eWCgI/AAAAAAAAIEQ/tOUG7nHrg_Q/s400/Screenshot-Untitled%2BWindow-1.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing with puts (blue doted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
Displaced diffusion extrapolation is sometimes advocated. It is not the same as Kahale extrapolation: In Kahale, only the forward variable is varying in the Black-Scholes formula, and there is no real underlying stochastic process. In a displaced diffusion setting, we would adjust both strike and forward, keeping put-call parity at the formula level. But unfortunately, it suffers from the same kind of problem: it can not always be solved for slope and price. When it can however, it will give a more consistent extrapolation.
I find it interesting that some smiles can not be extrapolated by displaced diffusion in a C1 manner except if one allows negative volatilities in the formula (in which case we are not anymore in a pure displaced diffusion setting).
&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-VSy7uTzu56U/VZwJUDT5iJI/AAAAAAAAIEo/lsi8EakZ-kA/s1600/Screenshot-Untitled%2BWindow-2.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-VSy7uTzu56U/VZwJUDT5iJI/AAAAAAAAIEo/lsi8EakZ-kA/s400/Screenshot-Untitled%2BWindow-2.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing using negative displaced diffusion volatilities (blue dotted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Square Root Crank-Nicolson</title>
      <link>http://chasethedevil.github.io/post/square-root-crank-nicolson/</link>
      <pubDate>Fri, 19 Jun 2015 16:41:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/square-root-crank-nicolson/</guid>
      <description>&lt;p&gt;C. Reisinger kindly pointed out to me &lt;a href=&#34;http://arxiv.org/abs/1210.5487&#34;&gt;this paper around square root Crank-Nicolson&lt;/a&gt;. The idea is to apply a square root of time transformation to the PDE, and discretize the resulting PDE with Crank-Nicolson. Two reasons come to mind to try this: &lt;br /&gt;&lt;ul&gt;&lt;li&gt;the square root transform will result in small steps initially, where the solution is potentially not so smooth, making Crank-Nicolson behave better.&lt;/li&gt;&lt;li&gt;&amp;nbsp;it is the natural time of the Brownian motion.&lt;/li&gt;&lt;/ul&gt;Interestingly, it has nicer properties than what those reasons may suggest. On the Fokker-Planck density PDE, it does not oscillate under some very mild conditions and &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2605160&#34;&gt;preserves density positivity at the peak&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Out of curiosity I tried it to price a one touch barrier option. Of course there is an analytical solution in my test case (Black-Scholes assumptions), but as soon as rates are assumed not constant or local volatility is used, there is no other solution than a numerical method. In the later case, finite difference methods are quite good in terms of performance vs accuracy.&lt;br /&gt;&lt;br /&gt;The classic Crank-Nicolson gives a reasonable price, but the strong oscillations near the barrier, at every time step are not very comforting.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-XNr7vE77Dfo/VYQoBELx2KI/AAAAAAAAICw/FVVYrehW39Y/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A24%253A59.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;337&#34; src=&#34;http://3.bp.blogspot.com/-XNr7vE77Dfo/VYQoBELx2KI/AAAAAAAAICw/FVVYrehW39Y/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A24%253A59.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Crank-Nicolson Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Moving to square root of time removes nearly all oscillations on this problem, even with a relatively low number of time steps compared to the number of space steps.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-JXVRuhLrMOQ/VYQodTFBDGI/AAAAAAAAIC4/hrsSdQbA5Wo/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A14.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://1.bp.blogspot.com/-JXVRuhLrMOQ/VYQodTFBDGI/AAAAAAAAIC4/hrsSdQbA5Wo/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A14.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Square Root Crank-Nicolson Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;We can see that the second step prices are a bit higher than the third step (the lines cross), which looks like a small numerical oscillation in time, even if there is no oscillation is space.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-d-xXHOvO1H0/VYQon2V5eiI/AAAAAAAAIDA/4g-YYby4R0A/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://1.bp.blogspot.com/-d-xXHOvO1H0/VYQon2V5eiI/AAAAAAAAIDA/4g-YYby4R0A/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A06.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;TR-BDF2 Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;As a comparison, the TR-BDF2 scheme does relatively well: oscillations are removed after the second step, even with the extreme ratio of time steps vs space steps used on this example so that illustrations are clearer - Crank-Nicolson would still oscillate a lot with 10 times less space steps but we would not see oscillation on the square root Crank-Nicolson and a very mild one on TR-BDF2.&lt;br /&gt;&lt;br /&gt;The LMG2 scheme (a local richardson extrapolation) does not oscillate at all on this problem but is the slowest:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-HTxKtzO8au4/VYQo8t-wkdI/AAAAAAAAIDI/oPOjJWs0SI0/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A53.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://3.bp.blogspot.com/-HTxKtzO8au4/VYQo8t-wkdI/AAAAAAAAIDI/oPOjJWs0SI0/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A53.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;LMG2 Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;The square root Crank-Nicolson is quite elegant. It can however not be applied to that many problems in practice, as often some grid times are imposed by the payoff to evaluate, for example in a case of a discrete weekly barrier. But for continuous time problems (density PDE, Vanilla, American, continuous barriers) it&amp;rsquo;s quite good.&lt;br /&gt;&lt;br /&gt;In reality, with a continuous barrier, the payoff is not discontinuous at every step, but it is only discontinuous at the first step. So Rannacher smoothing would work very well on that problem:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-A-qqKczuefQ/VYQwf2ba_MI/AAAAAAAAIDY/2cYpi_3Y_pI/s1600/Screenshot%2Bfrom%2B2015-06-19%2B17%253A08%253A35.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://4.bp.blogspot.com/-A-qqKczuefQ/VYQwf2ba_MI/AAAAAAAAIDY/2cYpi_3Y_pI/s640/Screenshot%2Bfrom%2B2015-06-19%2B17%253A08%253A35.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Rannacher Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;The somewhat interesting payoff left for the square root Crank-Nicolson is the American.&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Decoding Hagan&#39;s arbitrage free SABR PDE derivation</title>
      <link>http://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</link>
      <pubDate>Fri, 08 May 2015 16:50:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</guid>
      <description>&lt;p&gt;Here are the main steps of Hagan derivation. Let&amp;rsquo;s recall his notation for the SABR model where typically, \(C(F) = F^\beta\)&lt;/p&gt;

&lt;p&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-uq2IhJPDd7M/VUzC4Hh3xoI/AAAAAAAAH9k/sY034iAD38Y/s1600/Screenshot_2015-05-08_16-04-27.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-uq2IhJPDd7M/VUzC4Hh3xoI/AAAAAAAAH9k/sY034iAD38Y/s1600/Screenshot_2015-05-08_16-04-27.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;First, he defines the moments of stochastic volatility:
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-RxzKVIrxbl8/VUzC4HGhMNI/AAAAAAAAH9c/7FmxbMuB4kw/s1600/Screenshot_2015-05-08_16-04-53.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;60&#34; src=&#34;http://2.bp.blogspot.com/-RxzKVIrxbl8/VUzC4HGhMNI/AAAAAAAAH9c/7FmxbMuB4kw/s320/Screenshot_2015-05-08_16-04-53.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Then he integrates the Fokker-Planck equation over all A, to obtain
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-mDsC-Zd6FMA/VUzC4L9dF_I/AAAAAAAAH9g/cAvRNw1VTkg/s1600/Screenshot_2015-05-08_16-05-36.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-mDsC-Zd6FMA/VUzC4L9dF_I/AAAAAAAAH9g/cAvRNw1VTkg/s1600/Screenshot_2015-05-08_16-05-36.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;On the backward Komolgorov equation, he applies a Lamperti transform like change of variable:
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-QiYromr1SDU/VUzE5q_OfjI/AAAAAAAAH94/nfbr14tEnj0/s1600/Screenshot_2015-05-08_16-10-33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-QiYromr1SDU/VUzE5q_OfjI/AAAAAAAAH94/nfbr14tEnj0/s1600/Screenshot_2015-05-08_16-10-33.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And then makes another change of variable so that the PDE has the same initial conditions for all moments:
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-1C8j58UD1lA/VUzE5hZJ95I/AAAAAAAAH-I/541DaJBFbAU/s1600/Screenshot_2015-05-08_16-12-34.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-1C8j58UD1lA/VUzE5hZJ95I/AAAAAAAAH-I/541DaJBFbAU/s1600/Screenshot_2015-05-08_16-12-34.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&amp;nbsp;This leads to
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/--wj3nYsi9g8/VUzE5glQ0VI/AAAAAAAAH98/NlVSXIc2NDI/s1600/Screenshot_2015-05-08_16-13-32.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;107&#34; src=&#34;http://3.bp.blogspot.com/--wj3nYsi9g8/VUzE5glQ0VI/AAAAAAAAH98/NlVSXIc2NDI/s320/Screenshot_2015-05-08_16-13-32.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;It turns out that there is a magical symmetry for k=0 and k=2.
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-OStv8D0OSw0/VUzGJ6HKqDI/AAAAAAAAH-Y/jk85U57eHnY/s1600/Screenshot_2015-05-08_16-19-45.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;32&#34; src=&#34;http://1.bp.blogspot.com/-OStv8D0OSw0/VUzGJ6HKqDI/AAAAAAAAH-Y/jk85U57eHnY/s320/Screenshot_2015-05-08_16-19-45.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-67k-xTphi7Q/VUzGJwbwSWI/AAAAAAAAH-U/Faz235QNpKs/s1600/Screenshot_2015-05-08_16-20-09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;32&#34; src=&#34;http://3.bp.blogspot.com/-67k-xTphi7Q/VUzGJwbwSWI/AAAAAAAAH-U/Faz235QNpKs/s320/Screenshot_2015-05-08_16-20-09.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Note that in the second equation, the second derivative applies to the whole.
Because of this, he can express \(Q^{(2)}\) in terms of \(Q^{(0)}\):
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-k-3DQd9MuIQ/VUzHl5TSuAI/AAAAAAAAH-s/CjTFtlX4upw/s1600/Screenshot_2015-05-08_16-25-36.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;29&#34; src=&#34;http://1.bp.blogspot.com/-k-3DQd9MuIQ/VUzHl5TSuAI/AAAAAAAAH-s/CjTFtlX4upw/s320/Screenshot_2015-05-08_16-25-36.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And he plugs that back to the integrated Fokker-Planck equation to obtain the arbitrage free SABR PDE:
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-aEwTOxulsWI/VUzHl3qfZAI/AAAAAAAAH-o/qQZPb8Vvz7o/s1600/Screenshot_2015-05-08_16-25-48.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;29&#34; src=&#34;http://1.bp.blogspot.com/-aEwTOxulsWI/VUzHl3qfZAI/AAAAAAAAH-o/qQZPb8Vvz7o/s320/Screenshot_2015-05-08_16-25-48.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;
There is a simple more common explanation in the world of local stochastic volatility for what&amp;rsquo;s going on. For example, in the particle method paper from Guyon-Labordère, we have the following expression for the true local volatility.
&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1U/8Iv1i23oXok/s1600/Screenshot%2Bfrom%2B2013-10-16%2B15%3A51%3A46.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;127&#34; src=&#34;http://3.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1U/8Iv1i23oXok/s320/Screenshot%2Bfrom%2B2013-10-16%2B15%3A51%3A46.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;
In the first equation, the numerator is simply \(Q^{(2)}\) and the denominator \(Q^{(0)}\). Of course, the integrated Fokker-Planck equation can be rewritten as:&lt;/p&gt;

&lt;p&gt;$$ Q^{(0)}&lt;em&gt;T = \frac{1}{2}\epsilon^2 \left[C^2(F) \frac{Q^{(2)}}{Q^{(0)}} Q^{(0)}\right]&lt;/em&gt;{FF} $$&lt;/p&gt;

&lt;p&gt;Karlsmark uses that approach directly in his thesis, using the expansions of Doust for \(Q^{(k)}\). Looking a Doust expansions, the fraction reduces straightforwardly to the same expression as Hagan, and the symmetry in the equations appears a bit less coincidental.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Matching Hagan PDE SABR with the one-step Andreasen-Huge SABR</title>
      <link>http://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</link>
      <pubDate>Thu, 30 Apr 2015 17:16:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</guid>
      <description>&lt;p&gt;I looked nearly two years ago already at &lt;a href=&#34;http://chasethedevil.blogspot.co.uk/2013/05/sabr-with-new-hagan-pde-approach.html&#34;&gt;the arbitrage free SABR of Andreasen-Huge in comparison to the arbitrage free PDE of Hagan&lt;/a&gt; and showed how close the &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/12/arbitrage-free-sabr-another-view-on.html&#34;&gt;ideas were&lt;/a&gt;: Andreasen-Huge relies on the normal Dupire forward PDE using a slightly simpler local vol (no time dependent exponential term) while Hagan works directly on the Fokker-Planck PDE (you can think of it as the Dupire Forward PDE for the density) and uses an expansion of same order as the original SABR formula (which leads to an additional exponential term in the local volatility).&lt;br /&gt;&lt;br /&gt;One clever idea from Andreasen-Huge is the use of a single step. It turns out that their idea is not completely new. Daniel Duffy sent me some old papers from Shishkin around fitted schemes (&lt;a href=&#34;http://oai.cwi.nl/oai/asset/10209/10209A.pdf&#34;&gt;here is one&lt;/a&gt;). This is very much the same thing, except Shishkin concern is about a good handling of discontinuity in the initial condition, and therefore makes the association step function =&amp;gt; cumulative density to fit the diffusion parameter. Andreasen-Huge work directly with the call prices as this is what they solve.&lt;br /&gt;&lt;br /&gt;One drawback of Andreasen-Huge one step method is the inability to match the standard SABR smile: the parameters don&amp;rsquo;t have exactly the same meaning. It turns out that by just shifting proportionally the local volatility by a constant factor, Andreasen Huge matches Hagan PDE vols quite closely.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-BNmlkwTWu7Y/VUJGeLtiKII/AAAAAAAAH88/5ZeorrS4WRg/s1600/Screenshot_2015-04-30_17-12-43.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-BNmlkwTWu7Y/VUJGeLtiKII/AAAAAAAAH88/5ZeorrS4WRg/s1600/Screenshot_2015-04-30_17-12-43.png&#34; height=&#34;395&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Implied Black volatilities&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-rIUYFj3y7ro/VUJF6HOwyHI/AAAAAAAAH80/aOJrEeI1p9U/s1600/Screenshot_2015-04-30_17-09-53.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-rIUYFj3y7ro/VUJF6HOwyHI/AAAAAAAAH80/aOJrEeI1p9U/s1600/Screenshot_2015-04-30_17-09-53.png&#34; height=&#34;395&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Probability density&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;While this is interesting in itself, it&amp;rsquo;s still not so simple to backup this factor without solving for it (and then the method looses appeal).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modern Programming Language for Monte-Carlo</title>
      <link>http://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</link>
      <pubDate>Sat, 18 Apr 2015 22:58:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</guid>
      <description>&lt;p&gt;A few recent programming languages sparked my interest:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;&lt;a href=&#34;http://julialang.org/&#34;&gt;Julia&lt;/a&gt;: because of the wide coverage of mathematical functions, and great attention to quality of the implementations. It has also some interesting web interface.&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;https://www.dartlang.org/&#34;&gt;Dart&lt;/a&gt;: because it&amp;rsquo;s a language focused purely on building apps for the web, and has a supposedly good VM.&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;http://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;: it&amp;rsquo;s the latest fad. It has interesting concepts around concurrency and a focus on being low level all the while being simpler than C.&lt;/li&gt;&lt;/ul&gt;I decided to see how well suited they would be on a simple Monte-Carlo simulation of a forward start option under the Black model. I am no expert at all in any of the languages, so this is a beginner&amp;rsquo;s test. I compared the runtime for executing 16K simulations times a multiplier.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Multipl. Scala&amp;nbsp; Julia&amp;nbsp; JuliaA&amp;nbsp; Dart&amp;nbsp; Python&amp;nbsp; Rust&lt;br /&gt;1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.03&amp;nbsp;&amp;nbsp; 0.08&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.09&amp;nbsp;&amp;nbsp; 0.03&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.4&amp;nbsp; 0.004 &lt;br /&gt;10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.07&amp;nbsp;&amp;nbsp; 0.02 &amp;nbsp;&amp;nbsp; 0.06&amp;nbsp;&amp;nbsp; 0.11&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.9&amp;nbsp; 0.04&lt;br /&gt;100&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.51&amp;nbsp;&amp;nbsp; 0.21 &amp;nbsp;&amp;nbsp; 0.40&amp;nbsp;&amp;nbsp; 0.88&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.23&lt;br /&gt;1000&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 4.11&amp;nbsp;&amp;nbsp; 2.07 &amp;nbsp;&amp;nbsp; 4.17&amp;nbsp;&amp;nbsp; 8.04&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.01&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;About performance&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;I am quite impressed at Dart performance versus Scala (or vs. Java, as it has the same performance as Scala) given that it is much less strict about types and its focus is not at all on this kind of stuff.&lt;br /&gt;&lt;br /&gt;Julia performance is great, that is if one is careful about types. Julia is very &lt;a href=&#34;http://julia.readthedocs.org/en/latest/manual/performance-tips/#man-performance-tips&#34;&gt;finicky about casting and optimizations&lt;/a&gt;, fortunately @time helps spotting the issues (often an inefficient cast will lead to copy and thus high allocation). JuliaA is my first attempt, with an implicit badly performing conversion of MersenneTwister to AbstractRNG. It is slower first, as the JIT costs is reflected on the first run, very much like in Java (although it appears to be even worse).&lt;br /&gt;&lt;br /&gt;Rust is the most impressive. I had to add the &amp;ndash;release flag to the cargo build tool to produce a properly optimized binary, otherwise the performance is up to 7x worse.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;About the languages&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;My Python code is not vectorized, just like any of the other implementations. While the code looks relatively clean, I made the most errors compared to Julia or Scala. Python numpy isn&amp;rsquo;t always great: norm.ppf is very slow, slower than my hand coded python implementation of AS241.&lt;br /&gt;&lt;br /&gt;Dart does not have fixed arrays: everything is a list. It also does not have strict 64 bit int: they can be arbitrarily large. The dev environment is ok but not great. &lt;br /&gt;&lt;br /&gt;Julia is a bit funny, not very OO (no object method) but more functional, although many OO concepts are there (type inheritence, type constructors). It was relatively straightforward, although I do not find intuitive the type conversion issues (eventual copy on conversion).&lt;br /&gt;&lt;br /&gt;Rust took me the most time to write, as it has quite new concepts around mutable variables, and &amp;ldquo;pointers&amp;rdquo; scope. I relied on an existing MersenneTwister64 that worked with latest Rust. It was a bit disappointing to see that some dSFMT git project did not compile with the latest Rust, likely because Rust is still a bit too young. This does not sound so positive, but I found it to be the language the most interesting to learn.&lt;br /&gt;&lt;br /&gt;I was familiar with Scala before this exercise. I used a non functional approach, with while loops in order to make sure I had maximum performance. This is something I find a bit annoying in Scala, I always wonder if for performance I need to do a while instead of a for, when the classic for makes much more sense (that and the fact that the classic for leads to some annoying delegation in runtime errors/on debug).&lt;br /&gt;&lt;br /&gt;I relied on the default RNG for Dart but MersenneTwister for Scala, Julia, Python, Rust. All implementations use a hand coded AS241 for the inverse cumulative normal.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update&amp;nbsp;&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Using FastMath.exp instead of Math.exp leads a slightly better performance for Scala:&lt;br /&gt;&lt;br /&gt;1 0.06&lt;br /&gt;10 0.05&lt;br /&gt;100 0.39&lt;br /&gt;1000 2.66&lt;br /&gt;&lt;br /&gt;I did not expect that this would still be true in 2015 with Java 8 Oracle JVM.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Volatility Swap vs Variance Swap Replication - Truncation</title>
      <link>http://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication---truncation/</link>
      <pubDate>Mon, 16 Mar 2015 14:39:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication---truncation/</guid>
      <description>&lt;p&gt;I have looked at &lt;a href=&#34;http://chasethedevil.blogspot.fr/2015/02/jumps-impact-variance-swap-vs.html&#34;&gt;jump effects on volatility vs. variance swaps&lt;/a&gt;. There is a similar behavior on tail events, that is, on truncating the replication.&lt;br /&gt;&lt;br /&gt;One main &lt;a href=&#34;http://chasethedevil.blogspot.fr/2015/02/variance-swap-replication-discrete-or.html&#34;&gt;problem with discrete replication of variance swaps&lt;/a&gt; is the implicit domain truncation, mainly because the variance swap equivalent log payoff is far from being linear in the wings.&lt;br /&gt;&lt;br /&gt;The equivalent payoff with Carr-Lee for a volatility swap is much more linear in the wings (&lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/03/a-volatility-swap-and-straddle.html&#34;&gt;not so far of a straddle&lt;/a&gt;). So we could expect the replication to be less sensitive to the wings truncation.&lt;br /&gt;&lt;br /&gt;I have done a simple test on flat 40% volatility:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-hxdKaTQrj4w/VQbYgb7IB9I/AAAAAAAAH6o/z2NDk9wtC3w/s1600/Screenshot%2Bfrom%2B2015-03-16%2B14%3A19%3A33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-hxdKaTQrj4w/VQbYgb7IB9I/AAAAAAAAH6o/z2NDk9wtC3w/s1600/Screenshot%2Bfrom%2B2015-03-16%2B14%3A19%3A33.png&#34; height=&#34;113&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-kv6b3fbejKI/VQbYjGOv3dI/AAAAAAAAH6w/M0YfIZ1WlPc/s1600/Screenshot%2B-%2B03162015%2B-%2B02%3A19%3A47%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-kv6b3fbejKI/VQbYjGOv3dI/AAAAAAAAH6w/M0YfIZ1WlPc/s1600/Screenshot%2B-%2B03162015%2B-%2B02%3A19%3A47%2BPM.png&#34; height=&#34;72&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;As expected, the vol swap is much less sensitive, and interestingly, very much like for the jumps, it moves in the opposite direction: the truncated price is higher than the non truncated price.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arbitrage free SABR with negative rates - alternative to shifted SABR</title>
      <link>http://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates---alternative-to-shifted-sabr/</link>
      <pubDate>Wed, 11 Mar 2015 18:48:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates---alternative-to-shifted-sabr/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2557046&#34;&gt;Antonov et al.&lt;/a&gt; present an interesting view on SABR with negative rates: instead of relying on a shifted SABR to allow negative rates up to a somewhat arbitrary shift, they modify slightly the SABR model to allow negative rates directly:&lt;br /&gt;$$ dF_t = |F_t|^\beta v_t dW_F $$&lt;br /&gt;with ( v_t ) being the standard lognormal volatility process of SABR.&lt;br /&gt;&lt;br /&gt;Furthermore they derive a clever semi-analytical approximation for this model, based on low correlation, quite close to the Monte-Carlo prices in their tests. It&amp;rsquo;s however not clear if it is arbitrage-free.&lt;br /&gt;&lt;br /&gt;It turns out that it is easy to tweak Hagan SABR PDE approach to this &amp;ldquo;absolute SABR&amp;rdquo; model: one just needs to push the boundary F_min far away, and to use the absolute value in C(F).&lt;br /&gt;&lt;br /&gt;It then reproduces the same behavior as in Antonov et al. paper:&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-kKiSIo-QgAg/VQBp3sRtYFI/AAAAAAAAH34/_A9DKmA_n-E/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A13%3A45%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-kKiSIo-QgAg/VQBp3sRtYFI/AAAAAAAAH34/_A9DKmA_n-E/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A13%3A45%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;&amp;ldquo;Absolute SABR&amp;rdquo; arbitrage free PDE&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-cRLZged_Ees/VQBp7sKKTrI/AAAAAAAAH4A/PIZbUBBHP1I/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A14%3A02%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-cRLZged_Ees/VQBp7sKKTrI/AAAAAAAAH4A/PIZbUBBHP1I/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A14%3A02%2BPM.png&#34; height=&#34;250&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Antonov et al. graph&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&amp;nbsp;I obtain a higher spike, it would look much more like Antonov graph had I used a lower resolution to compute the density: the spike would be smoothed out.&lt;br /&gt;&lt;br /&gt;Interestingly, the arbitrage free PDE will also work for high beta (larger than 0.5):&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-YngRqD1ilNw/VQBroKrwKWI/AAAAAAAAH4M/3h9N7zHVjx0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A21%3A13%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-YngRqD1ilNw/VQBroKrwKWI/AAAAAAAAH4M/3h9N7zHVjx0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A21%3A13%2BPM.png&#34; height=&#34;190&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;beta = 0.75&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;It turns out to be then nearly the same as the absorbing SABR, even if prices can cross a little the 0. This is how the bpvols look like with beta = 0.75:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-J_cdl8wwUms/VQBsOst-xYI/AAAAAAAAH4U/xWRMyUpOwxA/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A24%3A18%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-J_cdl8wwUms/VQBsOst-xYI/AAAAAAAAH4U/xWRMyUpOwxA/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A24%3A18%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;red = absolute SABR, blue = absorbing SABR with beta=0.75&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;They overlap when the strike is positive.&lt;br /&gt;&lt;br /&gt;If we go back to Antonov et al. first example, the bpvols look a bit funny (very symmetric) with beta=0.1:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-DrQY0znkznc/VQBsxqF8GAI/AAAAAAAAH4g/MGMwg4sS2Zw/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A26%3A30%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-DrQY0znkznc/VQBsxqF8GAI/AAAAAAAAH4g/MGMwg4sS2Zw/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A26%3A30%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;For beta=0.25 we also reproduce Antonov bpvol graph, but with a lower slope for the left wing:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-QtPOjLCr4ts/VQBtT6hqvmI/AAAAAAAAH4o/jHLn9yC6Frk/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A28%3A55%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-QtPOjLCr4ts/VQBtT6hqvmI/AAAAAAAAH4o/jHLn9yC6Frk/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A28%3A55%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;bpvols with beta = 0.25&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;It&amp;rsquo;s interesting to see that in this case, the positive strikes bp vols are closer to the normal Hagan analytic approximation (which is not arbitrage free) than to the absorbing PDE solution.&lt;br /&gt;&lt;br /&gt;For longer maturities, the results start to be a bit different from Antonov, as Hagan PDE relies on a order 2 approximation only:&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kPhdB8qyCKI/VQBuC3w2G4I/AAAAAAAAH40/lIIp0-zSokU/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A31%3A59%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-kPhdB8qyCKI/VQBuC3w2G4I/AAAAAAAAH40/lIIp0-zSokU/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A31%3A59%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;absolute SABR PDE with 10y maturity&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-h2QCjhFGF14/VQBuTcUmlOI/AAAAAAAAH48/TarV9Gu24M0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A33%3A08%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-h2QCjhFGF14/VQBuTcUmlOI/AAAAAAAAH48/TarV9Gu24M0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A33%3A08%2BPM.png&#34; height=&#34;153&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The right wing is quite similar, except when it goes towards 0, it&amp;rsquo;s not as flat, the left wing is much lower.&lt;br /&gt;&lt;br /&gt;Another important aspect is to reproduce Hagan&amp;rsquo;s knee, the atm vols should produce a knee like curve, as different studies show (see for example &lt;a href=&#34;http://www-2.rotman.utoronto.ca/~hull/downloadablepublications/TreeBuilding.pdf&#34;&gt;this recent Hull &amp;amp; White study&lt;/a&gt; or this &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.740569&#34;&gt;other recent analysis by DeGuillaume&lt;/a&gt;). Using the same parameters as Hagan (beta=0, rho=0) leads to a nearly flat bpvol: no knee for the absolute SABR, curiously there is a bump at zero, possibly due to numerical difficulty with the spike in the density:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-H7PhpMWdy6U/VQB_m9TNZ0I/AAAAAAAAH5M/4yp7RMOwmo4/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A44%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-H7PhpMWdy6U/VQB_m9TNZ0I/AAAAAAAAH5M/4yp7RMOwmo4/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A44%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The problem is still there with beta = 0.1:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-ljZ11v0FIqw/VQB_m_hao1I/AAAAAAAAH5Q/Jpn7wgZ2Dwg/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A55%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-ljZ11v0FIqw/VQB_m_hao1I/AAAAAAAAH5Q/Jpn7wgZ2Dwg/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A55%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Overall, the idea of extending SABR to the full real line with the absolute value looks particularly simple, but it&amp;rsquo;s not clear that it makes real financial sense.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variance swaps on a foreign asset</title>
      <link>http://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</link>
      <pubDate>Tue, 24 Feb 2015 13:50:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</guid>
      <description>&lt;p&gt;There is very little information on variance swaps on a foreign asset. There can be two kinds of contracts:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;one that pays the foreign variance in a domestic currency, this is a quanto contract as the exchange rate is implicitly fixed.&lt;/li&gt;&lt;li&gt;one that pays the foreign variance, multiplied by the fx rate at maturity. This is a flexo contract, and is just about buying a variance swap from a foreign bank. The price of such a contract today is very simple, just the standard variance swap price multiplied by the fx rate today (change of measure).&lt;/li&gt;&lt;/ul&gt;For quanto contracts, it&amp;rsquo;s not so obvious a priori. If we consider a stochastic volatility model for the asset, the replication formula will not be applicable directly as the stochastic volatility will appear in the quanto drift correction. Furthermore, vanilla quanto option prices can not be computed simply as under Black-Scholes, a knowledge of the underlying model is necessary.&lt;br /&gt;&lt;br /&gt;Interestingly, under the Schobel-Zhu model, it is simple to fit an analytic formula for the quanto variance swap. The standard variance swap price is:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-gfq9trxBHhg/VOxxHi0khTI/AAAAAAAAH00/xD8NbXUSJls/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A33%3A17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-gfq9trxBHhg/VOxxHi0khTI/AAAAAAAAH00/xD8NbXUSJls/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A33%3A17.png&#34; height=&#34;50&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The quanto variance swap can be priced with the same formula using a slightly different theta:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-OwYg8Hs9qRw/VOxxMe-iifI/AAAAAAAAH08/xBeV56UEGV8/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-OwYg8Hs9qRw/VOxxMe-iifI/AAAAAAAAH08/xBeV56UEGV8/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A06.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;We can use it to assess the accuracy of a naive quanto option replication where we use the ATM quanto forward instead of the forward in the variance swap replication formula.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-ate0oZ41eoY/VOxxhwqGI_I/AAAAAAAAH1E/6LRgNiAHCJg/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A52.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-ate0oZ41eoY/VOxxhwqGI_I/AAAAAAAAH1E/6LRgNiAHCJg/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A52.png&#34; height=&#34;289&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Interestingly, the&amp;nbsp; quanto forward approximation turns out to be very accurate and the correction is important. The price without correction is the price with zero correlation, and we see it can be +/-5% off in this case.&lt;br /&gt;&lt;br /&gt;The local vol price seems a bit off, I am not sure exactly why. It could be due the discretization, the theoretical variance should be divided by (N-1) but here we divide by N where N is the number of observations. That would still lead to a skewed price but better centered around correlation 0.&lt;br /&gt;&lt;br /&gt;It&amp;rsquo;s also a bit surprising that local vol is worse than the simpler ATM quanto forward approximation: it seems that it&amp;rsquo;s extracting the wrong information to do a more precise quanto correction, likely related to the shift of stochastic volatility under the domestic measure.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jumps impact: Variance swap vs volatility swap</title>
      <link>http://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</link>
      <pubDate>Fri, 20 Feb 2015 13:24:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</guid>
      <description>&lt;p&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-9BEKyX_gWc0/VOclAJRe19I/AAAAAAAAH0g/AxbdgL48GaA/s1600/Screenshot%2B-%2B200215%2B-%2B13%3A13%3A38.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-9BEKyX_gWc0/VOclAJRe19I/AAAAAAAAH0g/AxbdgL48GaA/s1600/Screenshot%2B-%2B200215%2B-%2B13%3A13%3A38.png&#34; height=&#34;388&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Beside &lt;a href=&#34;http://chasethedevil.blogspot.fr/2015/02/variance-swap-replication-discrete-or.html&#34;&gt;the problem with the discreteness&lt;/a&gt; of the replication, variance swaps are sensitive to jumps. This is an often mentioned reason for the collapse of the single name variance swap market in 2008 as jumps are more likely on single name equities.&lt;br /&gt;&lt;br /&gt;Those graphs are the result of Monte-Carlo simulations with various jump sizes using the Bates model, and using Local Volatility implied from the Bates vanilla prices. The local volatility price will be the same price as per static replication for the variance swap, and we can see it they converge when there is no jump.&lt;br /&gt;&lt;br /&gt;The presence of jumps lead to a theoretically higher variance swap price, again, which we miss completely with the static replication. As jumps go higher, the difference is more pronounced.&lt;br /&gt;&lt;br /&gt;Volatility swaps are a bit better behaved in this regard. Interestingly, local volatility overestimate the value in this case (which for variance swaps it underestimates the value). I also noticed that the relatively &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/03/a-volatility-swap-and-straddle.html&#34;&gt;recent formula from Carr-Lee&lt;/a&gt; will underestimate jumps even more so than local volatility: it is more precise in the absence of jumps, very close to Heston, but less precise than local volatility when jumps increase in size.&lt;br /&gt;&lt;br /&gt;I have added a small section around this in &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2567398&#34;&gt;my paper on SSRN&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Variance Swap Replication : Discrete or Continuous?</title>
      <link>http://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous/</link>
      <pubDate>Thu, 19 Feb 2015 18:45:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous/</guid>
      <description>&lt;p&gt;People regularly believe that Variance swaps need to be priced by discrete replication, because the market trades only a discrete set of options.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-9dEW7QRFa7k/VOYguM8BHOI/AAAAAAAAH0Q/RPFxCeyq6nU/s1600/Screenshot%2B-%2B190215%2B-%2B18%3A36%3A26.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-9dEW7QRFa7k/VOYguM8BHOI/AAAAAAAAH0Q/RPFxCeyq6nU/s1600/Screenshot%2B-%2B190215%2B-%2B18%3A36%3A26.png&#34; height=&#34;340&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In reality, a discrete replication will misrepresent the tail, and can be quite arbitrary. It looks like the discrete replication as described in &lt;a href=&#34;http://bfi.cl/papers/Derman%201999%20-%20More%20about%20Variance%20Swaps.pdf&#34;&gt;Derman Goldman Sachs paper&lt;/a&gt; is in everybody&amp;rsquo;s mind, probably because it&amp;rsquo;s easy to grasp. Strangely, it looks like most forget the section &amp;ldquo;Practical problems with replication&amp;rdquo; on p27 of his paper, where you can understand that discrete replication is not all that practical.&lt;br /&gt;&lt;br /&gt;Reflecting on all of this, I noticed it was possible to create more accurate discrete replications easily, and that those can have vastly different hedging weights. It is a much better idea to just replicate the log payoff continuously with a decent model for interpolation and extrapolation and imply the hedge from the greeks.&lt;br /&gt;&lt;br /&gt;I wrote &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2567398&#34;&gt;a small paper around this here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monte Carlo &amp; Inverse Cumulative Normal Distribution</title>
      <link>http://chasethedevil.github.io/post/monte-carlo--inverse-cumulative-normal-distribution/</link>
      <pubDate>Tue, 03 Feb 2015 14:53:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/monte-carlo--inverse-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;In most financial Monte-Carlo simulations, there is the need of generating normally distributed random numbers. One technique is to use the inverse cumulative normal distribution function on uniform random numbers. There are several different popular numerical implementations:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Wichura AS241 (1988)&lt;/li&gt;&lt;li&gt;Moro &amp;ldquo;The full Monte&amp;rdquo; (1995)&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;http://home.online.no/~pjacklam/notes/invnorm/&#34;&gt;Acklam&lt;/a&gt; (2004)&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/0901.0638&#34;&gt;Shaw breakless formula&lt;/a&gt; optimized for GPUs (2011) &lt;/li&gt;&lt;/ul&gt;W. Shaw has an excellent overview of the accuracy of the various methods in his paper &lt;i&gt;&lt;a href=&#34;http://www.mth.kcl.ac.uk/~shaww/web_page/papers/NormalQuantile1.pdf&#34;&gt;Refinement of the normal quantile&lt;/a&gt;&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;But what about performance? In Monte-Carlo, we could accept a slighly lower accuracy for an increase in performance.&lt;br /&gt;&lt;br /&gt;I tested the various methods on the Euler full truncation scheme for Heston using a small timestep (0.01). Here are the results with Sobol quasi-rng:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;AS241&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.9186256922511046 0.42s&lt;br /&gt;MORO &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0.9186256922459066 0.38s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ACKLAM &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0.9186256922549364 0.40s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ACKLAM REFINED 0.9186256922511045 2.57s&lt;br /&gt;SHAW-HYBRID &amp;nbsp;&amp;nbsp; 0.9186256922511048 0.68s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;In practice, the most accurate algorithm, AS241, is of comparable speed as the newer but less accurate algorithms of MORO and ACKLAM. Acklam refinement to go to double precision (which AS241 is) kills its performance.&lt;br /&gt;&lt;br /&gt;What about the Ziggurat on pseudo rng only? Here are the results with Mersenne-Twister-64, and using the Doornik implementation of the Ziggurat algorithm:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;AS241&amp;nbsp; 0.9231388565879476&amp;nbsp; 0.49s&lt;br /&gt;ZIGNOR 0.9321405648313437&amp;nbsp; 0.44s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;There is a more optimized algorithm, VIZIGNOR, also from Doornik which should be a bit faster. As expected, the accuracy is quite lower than with Sobol, and the Ziggurat looks worse. This is easily visible if one plots the implied volatilities as a function of the spot for AS241 and for ZIGNOR.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-lITlDFhF-cE/VNDQfqtNbTI/AAAAAAAAHzU/zki5VJADyv4/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A43%3A10.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-lITlDFhF-cE/VNDQfqtNbTI/AAAAAAAAHzU/zki5VJADyv4/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A43%3A10.png&#34; height=&#34;321&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;AS241 implied volatility on Mersenne-Twister&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-QxKOGzNMSXE/VNDQp7dL0EI/AAAAAAAAHzc/wm1c-ymLYww/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A18%3A51.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-QxKOGzNMSXE/VNDQp7dL0EI/AAAAAAAAHzc/wm1c-ymLYww/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A18%3A51.png&#34; height=&#34;321&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;ZIGNOR implied volatility on Mersenne-Twister&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Zignor is much noisier.&lt;br /&gt;&lt;br /&gt;Note the slight bump in the scheme EULER-FT-BK that appears because the scheme, that approximates the Broadie-Kaya integrals with a trapeze (as in Andersen QE paper), does not respect martingality that well compared to the standard full truncated Euler scheme EULER-FT, and the slightly improved EULER-FT-MID where the variance integrals are approximated by a trapeze as in Van Haastrecht paper on Schobel-Zhu:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-pIO5C8vN1Es/VNDSPriO-OI/AAAAAAAAHzo/d0DUYBjiG8Q/s1600/Screenshot%2B-%2B030215%2B-%2B14%3A49%3A29.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-pIO5C8vN1Es/VNDSPriO-OI/AAAAAAAAHzo/d0DUYBjiG8Q/s1600/Screenshot%2B-%2B030215%2B-%2B14%3A49%3A29.png&#34; height=&#34;76&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;This allows to leak less correlation than the standard full truncated Euler.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Local Stochastic Volatility - Particles and Bins</title>
      <link>http://chasethedevil.github.io/post/local-stochastic-volatility---particles-and-bins/</link>
      <pubDate>Fri, 30 Jan 2015 12:03:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/local-stochastic-volatility---particles-and-bins/</guid>
      <description>&lt;p&gt;In an &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/10/local-stochastic-volatility-with-monte.html&#34;&gt;earlier post&lt;/a&gt;, I mentioned the similarities between the Guyon-Labordere &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1885032&#34;&gt;particle method&lt;/a&gt; and the Vanderstoep-Grzelak-Oosterlee &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDIQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fabstract%3D2278122&amp;amp;ei=255eUqaEDMaxhAfdqoBI&amp;amp;usg=AFQjCNF2KqSTT2ouvAyiA2J77foOFTzMKw&amp;amp;sig2=fzb4vlDPp49Hp1oT5Wja4A&amp;amp;bvm=bv.54176721,d.ZG4&#34;&gt;&amp;ldquo;bin&amp;rdquo; method&lt;/a&gt; to calibrate and price under Local Stochastic volatility. I will be a bit more precise here.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: large;&#34;&gt;&lt;b&gt;The same thing, really&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;The particle method can be seen as a generalization of the &amp;ldquo;bin&amp;rdquo; method. In deed, the bin method consists in doing the particle method using a histogram estimation of the conditional variance. The histogram estimation can be more or less seen as a very basic rectangle kernel with the appropriate bandwidth. The &amp;ldquo;bin&amp;rdquo; method is then just the particle method with another kernel (wiki link) (in the particle method, the kernel is a quartic with bandwidth defined by some slightly elaborate formula). A very good paper on this is Silverman &lt;i&gt;&lt;a href=&#34;https://ned.ipac.caltech.edu/level5/March02/Silverman/paper.pdf&#34;&gt;Density estimation for statistics and data analysis&lt;/a&gt;&lt;/i&gt;, referenced by Guyon-Labordere.&lt;br /&gt;&lt;br /&gt;In theory, the original particle method has the advantage of using a narrower bandwidth, resulting in a theoretical increase in performance as one does not have to sum over all particles, while providing a more local therefore precise estimate. In practice, the performance advantage is not so clear on my non optimized code.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: large;&#34;&gt;&lt;b&gt;Two-pass&lt;/b&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;There is an additional twist in the particle method: one can compute the expectation and the payoff evaluation in the same Monte-Carlo simulation, or in two sequential Monte-Carlo simulations.&lt;br /&gt;&lt;br /&gt;Why would we do two? Mainly because the expectation is computed across all paths, at each time step, while, usually, payoff evaluation requires a full path as it will need to store some state at each observation time for path-dependent payoffs.&lt;br /&gt;&lt;br /&gt;We can avoid recomputing the paths by just caching them at each observation time. The problem is that the size of this cache can quickly become extremely large and blow up the memory. For example a 10y daily knock-out will require 10 * 252 * 8 * 2 * MB = 40 GB for 1 million paths.&lt;br /&gt;&lt;br /&gt;A side effect of the second simulation is that one can use a Quasi-Random number generator there, while for the first simulation, this is not easy as we compute all paths, dimension by dimension.&lt;br /&gt;&lt;br /&gt;In practice, both methods work well, particle or bins, single-pass or two-pass. Here is a graph of the error in volatility, SV is a not so well calibrated Heston to market data. LVSV are the local stochastic volatility simulations, using as Vanderstoep 100 steps per year and 500K simulations with 30 bins.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-mU7ODNSv0l0/VMtk-7UmKvI/AAAAAAAAHy8/u-jq0sMMJRc/s1600/Screenshot%2Bfrom%2B2015-01-30%2B09%3A23%3A31.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-mU7ODNSv0l0/VMtk-7UmKvI/AAAAAAAAHy8/u-jq0sMMJRc/s1600/Screenshot%2Bfrom%2B2015-01-30%2B09%3A23%3A31.png&#34; height=&#34;288&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;The advantages of the particle do not show up in terms of accuracy on this example.&lt;br /&gt;I have also noticed that short expiries seem trickier, the error being larger. This might just be due to the time-step size, but interestingly the papers only show graphs of medium (min=6m) to large expiries.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>