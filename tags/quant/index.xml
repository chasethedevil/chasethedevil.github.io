<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>quant on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/quant/</link>
    <description>Recent content in quant on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Sat, 22 Aug 2015 16:13:00 +0000</lastBuildDate><atom:link href="https://chasethedevil.github.io/tags/quant/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Go for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/go-for-monte-carlo/</link>
      <pubDate>Sat, 22 Aug 2015 16:13:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/go-for-monte-carlo/</guid>
      <description>I have looked a few months ago already at Julia, Dart, Rust and Scala programming languages to see how practical they could be for a simple Monte-Carlo option pricing.
I forgot the Go language. I had tried it 1 or 2 years ago, and at that time, did not enjoy it too much. Looking at Go 1.5 benchmarks on the computer language shootout, I was surprised that it seemed so close to Java performance now, while having a GC that guarantees pauses of less 10ms and consuming much less memory.</description>
    </item>
    
    <item>
      <title>Bumping Correlations</title>
      <link>https://chasethedevil.github.io/post/bumping-correlations/</link>
      <pubDate>Sat, 25 Jul 2015 18:36:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/bumping-correlations/</guid>
      <description>In his book &#34;Monte Carlo Methods in Finance&#34;, P. Jäckel explains a simple way to clean up a correlation matrix. When a given correlation matrix is not positive semi-definite, the idea is to do a singular value decomposition (SVD), replace the negative eigenvalues by 0, and renormalize the corresponding eigenvector accordingly.
One of the cited applications is &#34;stress testing and scenario analysis for market risk&#34; or &#34;comparative pricing in order to ascertain the extent of correlation exposure for multi-asset derivatives&#34;</description>
    </item>
    
    <item>
      <title>Andreasen Huge extrapolation</title>
      <link>https://chasethedevil.github.io/post/andreasen-huge-extrapolation/</link>
      <pubDate>Mon, 13 Jul 2015 17:35:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/andreasen-huge-extrapolation/</guid>
      <description>There are not many arbitrage free extrapolation schemes. Benaim et al. extrapolation is one of the few that claims it. However, despite the paper&#39;s title, it is not truely arbitrage free. The density might be positive, but the forward is not preserved by the implied density. It can also lead to wings that don&#39;t obey Lee&#39;s moments condition.
On a Wilmott forum, P. Caspers proposed the following counter-example based on extrapolating SABR: \\( \alpha=15\%, \beta=80\%, \nu=50\%, \rho=-48\%, f=3\%, T=20.</description>
    </item>
    
    <item>
      <title>Unintuitive behavior of the Black-Scholes formula - negative volatilities in displaced diffusion extrapolation</title>
      <link>https://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula-negative-volatilities-in-displaced-diffusion-extrapolation/</link>
      <pubDate>Tue, 07 Jul 2015 16:43:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula-negative-volatilities-in-displaced-diffusion-extrapolation/</guid>
      <description>I am looking at various extrapolation schemes of the implied volatilities. An interesting one I stumbled upon is due to Kahale. Even if his paper is on interpolation, there is actually a small paragraph on using the same kind of function for extrapolation. His idea is to simply lookup the standard deviation \\( \Sigma \\) and the forward \\(f\\) corresponding to a given market volatility and slope: $$ c_{f,\Sigma} = f N(d_1) - k N(d_2)$$ with $$ d_1 = \frac{\log(f/k)+\Sigma^2 /2}{\Sigma} $$ We have simply: $$ c&#39;(k) = - N(d_2)$$ He also proves that we can always find those two parameters for any \\( k_0  c_0  0, -1 Extrapolation of the left wing with calls (blue doted line)</description>
    </item>
    
    <item>
      <title>Square Root Crank-Nicolson</title>
      <link>https://chasethedevil.github.io/post/square-root-crank-nicolson/</link>
      <pubDate>Fri, 19 Jun 2015 16:41:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/square-root-crank-nicolson/</guid>
      <description>C. Reisinger kindly pointed out to me this paper around square root Crank-Nicolson. The idea is to apply a square root of time transformation to the PDE, and discretize the resulting PDE with Crank-Nicolson. Two reasons come to mind to try this: the square root transform will result in small steps initially, where the solution is potentially not so smooth, making Crank-Nicolson behave better.&amp;nbsp;it is the natural time of the Brownian motion.</description>
    </item>
    
    <item>
      <title>Decoding Hagan&#39;s arbitrage free SABR PDE derivation</title>
      <link>https://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</link>
      <pubDate>Fri, 08 May 2015 16:50:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</guid>
      <description>Here are the main steps of Hagan derivation. Let&#39;s recall his notation for the SABR model where typically, \\(C(F) = F^\beta\\) First, he defines the moments of stochastic volatility: Then he integrates the Fokker-Planck equation over all A, to obtain On the backward Komolgorov equation, he applies a Lamperti transform like change of variable: And then makes another change of variable so that the PDE has the same initial conditions for all moments: &amp;nbsp;This leads to It turns out that there is a magical symmetry for k=0 and k=2.</description>
    </item>
    
    <item>
      <title>Matching Hagan PDE SABR with the one-step Andreasen-Huge SABR</title>
      <link>https://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</link>
      <pubDate>Thu, 30 Apr 2015 17:16:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</guid>
      <description>I looked nearly two years ago already at the arbitrage free SABR of Andreasen-Huge in comparison to the arbitrage free PDE of Hagan and showed how close the ideas were: Andreasen-Huge relies on the normal Dupire forward PDE using a slightly simpler local vol (no time dependent exponential term) while Hagan works directly on the Fokker-Planck PDE (you can think of it as the Dupire Forward PDE for the density) and uses an expansion of same order as the original SABR formula (which leads to an additional exponential term in the local volatility).</description>
    </item>
    
    <item>
      <title>Modern Programming Language for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</link>
      <pubDate>Sat, 18 Apr 2015 22:58:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</guid>
      <description>A few recent programming languages sparked my interest:
Julia: because of the wide coverage of mathematical functions, and great attention to quality of the implementations. It has also some interesting web interface.Dart: because it&#39;s a language focused purely on building apps for the web, and has a supposedly good VM.Rust: it&#39;s the latest fad. It has interesting concepts around concurrency and a focus on being low level all the while being simpler than C.</description>
    </item>
    
    <item>
      <title>Volatility Swap vs Variance Swap Replication - Truncation</title>
      <link>https://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication-truncation/</link>
      <pubDate>Mon, 16 Mar 2015 14:39:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication-truncation/</guid>
      <description>I have looked at jump effects on volatility vs. variance swaps. There is a similar behavior on tail events, that is, on truncating the replication.
One main problem with discrete replication of variance swaps is the implicit domain truncation, mainly because the variance swap equivalent log payoff is far from being linear in the wings.
The equivalent payoff with Carr-Lee for a volatility swap is much more linear in the wings (not so far of a straddle.</description>
    </item>
    
    <item>
      <title>Arbitrage free SABR with negative rates - alternative to shifted SABR</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates-alternative-to-shifted-sabr/</link>
      <pubDate>Wed, 11 Mar 2015 18:48:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates-alternative-to-shifted-sabr/</guid>
      <description>Antonov et al. present an interesting view on SABR with negative rates: instead of relying on a shifted SABR to allow negative rates up to a somewhat arbitrary shift, they modify slightly the SABR model to allow negative rates directly: $$ dF_t = |F_t|^\beta v_t dW_F $$ with \\( v\_t \\) being the standard lognormal volatility process of SABR.
Furthermore they derive a clever semi-analytical approximation for this model, based on low correlation, quite close to the Monte-Carlo prices in their tests.</description>
    </item>
    
    <item>
      <title>Variance swaps on a foreign asset</title>
      <link>https://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</link>
      <pubDate>Tue, 24 Feb 2015 13:50:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</guid>
      <description>There is very little information on variance swaps on a foreign asset. There can be two kinds of contracts:
one that pays the foreign variance in a domestic currency, this is a quanto contract as the exchange rate is implicitly fixed.one that pays the foreign variance, multiplied by the fx rate at maturity. This is a flexo contract, and is just about buying a variance swap from a foreign bank.</description>
    </item>
    
    <item>
      <title>Jumps impact: Variance swap vs volatility swap</title>
      <link>https://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</link>
      <pubDate>Fri, 20 Feb 2015 13:24:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</guid>
      <description>Beside the problem with the discreteness of the replication, variance swaps are sensitive to jumps. This is an often mentioned reason for the collapse of the single name variance swap market in 2008 as jumps are more likely on single name equities.
Those graphs are the result of Monte-Carlo simulations with various jump sizes using the Bates model, and using Local Volatility implied from the Bates vanilla prices. The local volatility price will be the same price as per static replication for the variance swap, and we can see it they converge when there is no jump.</description>
    </item>
    
    <item>
      <title>Variance Swap Replication : Discrete or Continuous?</title>
      <link>https://chasethedevil.github.io/post/variance-swap-replication-discrete-or-continuous/</link>
      <pubDate>Thu, 19 Feb 2015 18:45:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/variance-swap-replication-discrete-or-continuous/</guid>
      <description>People regularly believe that Variance swaps need to be priced by discrete replication, because the market trades only a discrete set of options.

In reality, a discrete replication will misrepresent the tail, and can be quite arbitrary. It looks like the discrete replication as described in Derman Goldman Sachs paper is in everybody&#39;s mind, probably because it&#39;s easy to grasp. Strangely, it looks like most forget the section &#34;</description>
    </item>
    
    <item>
      <title>Monte Carlo &amp; Inverse Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/monte-carlo-inverse-cumulative-normal-distribution/</link>
      <pubDate>Tue, 03 Feb 2015 14:53:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/monte-carlo-inverse-cumulative-normal-distribution/</guid>
      <description>In most financial Monte-Carlo simulations, there is the need of generating normally distributed random numbers. One technique is to use the inverse cumulative normal distribution function on uniform random numbers. There are several different popular numerical implementations:
Wichura AS241 (1988)Moro &#34;The full Monte&#34; (1995)Acklam (2004)Shaw breakless formula optimized for GPUs (2011) W. Shaw has an excellent overview of the accuracy of the various methods in his paper Refinement of the normal quantile.</description>
    </item>
    
    <item>
      <title>Local Stochastic Volatility - Particles and Bins</title>
      <link>https://chasethedevil.github.io/post/local-stochastic-volatility-particles-and-bins/</link>
      <pubDate>Fri, 30 Jan 2015 12:03:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/local-stochastic-volatility-particles-and-bins/</guid>
      <description>In an earlier post, I mentioned the similarities between the Guyon-Labordere particle method and the Vanderstoep-Grzelak-Oosterlee &#34;bin&#34; method to calibrate and price under Local Stochastic volatility. I will be a bit more precise here.
The same thing, really
The particle method can be seen as a generalization of the &#34;bin&#34; method. In deed, the bin method consists in doing the particle method using a histogram estimation of the conditional variance.</description>
    </item>
    
    <item>
      <title>Flat Volatility Surfaces &amp; Discrete Dividends</title>
      <link>https://chasethedevil.github.io/post/flat-volatility-surfaces-discrete-dividends/</link>
      <pubDate>Tue, 25 Nov 2014 13:58:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/flat-volatility-surfaces-discrete-dividends/</guid>
      <description>In papers around volatility and cash (discrete) dividends, we often encounter the example of the flat volatility surface. For example, the OpenGamma paper presents this graph:

It shows that if the Black volatility surface is fully flat, there are jumps in the pure volatility surface (corresponding to a process that includes discrete dividends in a consistent manner) at the dividend dates or equivalently if the pure volatility surface is flat, the Black volatility jumps.</description>
    </item>
    
    <item>
      <title>Machine Learning &amp; Quantitative Finance</title>
      <link>https://chasethedevil.github.io/post/machine-learning-quantitative-finance/</link>
      <pubDate>Tue, 18 Nov 2014 12:34:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/machine-learning-quantitative-finance/</guid>
      <description>There is an interesting course on Machine Learning on Coursera, it does not require much knowledge and yet manages to teach quite a lot.
I was struck by the fact that most techniques and ideas apply also to problems in quantitative finance.
Linear regression: used for example in the Longstaff-Schwartz approach to price Bermudan options with Monte-Carlo. Interestingly the teacher insists on feature normalization, something we can forget easily, especially with the polynomial features.</description>
    </item>
    
    <item>
      <title>Pseudo-Random vs Quasi-Random Numbers</title>
      <link>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</link>
      <pubDate>Wed, 12 Nov 2014 17:05:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</guid>
      <description>Quasi-Random numbers (like Sobol) are a relatively popular way in finance to improve the Monte-Carlo convergence compared to more classic Pseudo-Random numbers (like Mersenne-Twister). Behind the scenes one has to be a bit more careful about the dimension of the problem as the Quasi-Random numbers depends on the dimension (defined by how many random variables are independent from each other).
For a long time, Sobol was limited to 40 dimensions using the so called Bratley-Fox direction numbers (his paper actually gives the numbers for 50 dimensions).</description>
    </item>
    
    <item>
      <title>Integrating an oscillatory function</title>
      <link>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</link>
      <pubDate>Wed, 05 Nov 2014 16:48:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</guid>
      <description>Recently, some instabilities were noticed in the Carr-Lee seasoned volatility swap price in some situations. The Carr-Lee seasoned volatility swap price involve the computation of a double integral. The inner integral is really the problematic one as the integrand can be highly oscillating.

I&amp;nbsp; first found a somewhat stable behavior using a specific adaptive Gauss-Lobatto implementation (the one from Espelid) and a change of variable. But it was not very satisfying to see that the outer integral was stable only with another specific adaptive Gauss-Lobatto (the one from Gander &amp;amp; Gauschi, present in Quantlib).</description>
    </item>
    
    <item>
      <title>The elusive reference: the Lamperti transform</title>
      <link>https://chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</link>
      <pubDate>Mon, 03 Nov 2014 11:23:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</guid>
      <description>Without knowing that it was a well known general concept, I first noticed the use of the Lamperti transform in the Andersen-Piterbarg &#34;Interest rate modeling&#34; book p292 &#34;finite difference solutions for general phi&#34;. &amp;nbsp;Pat Hagan used that transformation for a better discretization&amp;nbsp; of the arbitrage free SABR PDE model.I then started to notice the use of this transformation in many more papers. The first one I saw naming it &#34;</description>
    </item>
    
    <item>
      <title>Barrier options under negative rates: complex numbers to the rescue</title>
      <link>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</link>
      <pubDate>Thu, 02 Oct 2014 11:58:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</guid>
      <description>I stumbled upon an unexpected problem: the one touch barrier formula can break down under negative rates. While negative rates can sound fancy, they are actually quite real on some markets. Combined with relatively low volatilities, this makes the standard Black-Scholes one touch barrier formula blow up because somewhere the square root of a negative number is taken.
At first, I had the idea to just floor the number to 0.</description>
    </item>
    
    <item>
      <title>Initial Guesses for SVI - A Summary</title>
      <link>https://chasethedevil.github.io/post/initial-guesses-for-svi-a-summary/</link>
      <pubDate>Fri, 26 Sep 2014 10:46:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/initial-guesses-for-svi-a-summary/</guid>
      <description>I have been looking at various ways of finding initial guesses for SVI calibration (Another SVI Initial Guess, More SVI Initial Guesses, SVI and long maturities issues). I decided to write a paper summarizing this. I find that the process of writing a paper makes me think more carefully about a problem.
In this case, it turns out that the Vogt initial guess method (guess via asymptotes and minimum variance) is actually very good as long as one has a good way to lookup the asymptotes (the data is not always convex, while SVI is) and as long as rho is not close to -1, that is for long maturity affine like smiles, where SVI is actually more difficult to calibrate properly due to the over-parameterisation in those cases.</description>
    </item>
    
    <item>
      <title>Asymptotic Behavior of SVI vs SABR</title>
      <link>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</link>
      <pubDate>Tue, 23 Sep 2014 12:06:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</guid>
      <description>The variance under SVI becomes linear when the log-moneyness is very large in absolute terms. The lognormal SABR formula with beta=0 or beta=1 has a very different behavior. Of course, the theoretical SABR model has actually a different asymptotic behavior.
As an illustration, we calibrate SABR (with two different values of beta) and SVI against the same implied volatility slice and look at the wings behavior. 
While the Lee moments formula implies that the variance should be at most linear, something that the SABR formula does not respect.</description>
    </item>
    
    <item>
      <title>SVI and long maturities issues</title>
      <link>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</link>
      <pubDate>Fri, 01 Aug 2014 12:51:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</guid>
      <description>On long maturities equity options, the smile is usually very much like a skew: very little curvature. This usually means that the SVI rho will be very close to -1, in a similar fashion as what can happen for the the correlation parameter of a real stochastic volatility model (Heston, SABR).
In terms of initial guess, we looked at the more usual use cases and showed that matching a parabola at the minimum variance point often leads to a decent initial guess if one has an ok estimate of the wings.</description>
    </item>
    
    <item>
      <title>More SVI Initial Guesses</title>
      <link>https://chasethedevil.github.io/post/more-svi-initial-guesses/</link>
      <pubDate>Thu, 31 Jul 2014 14:54:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/more-svi-initial-guesses/</guid>
      <description>In the previous post, I showed one could extract the SVI parameters from a best fit parabola at-the-money. It seemed to work reasonably well, but I found some real market data where it can be much less satisfying.

Sometimes (actually not so rarely) the ATM slope and curvatures can&#39;t be matched given rho and b found through the asymptotes. As a result if I force to just match the curvature and set m=0 (when the slope can&#39;t be matched), the simple ATM parabolic guess looks shifted.</description>
    </item>
    
    <item>
      <title>Another SVI Initial Guess</title>
      <link>https://chasethedevil.github.io/post/another-svi-initial-guess/</link>
      <pubDate>Tue, 29 Jul 2014 14:39:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/another-svi-initial-guess/</guid>
      <description>The SVI formula is:
$$w(k) = a + b ( \rho (k-m) + \sqrt{(k-m)^2+ \sigma^2}$$
where k is the log-moneyness, w(k) the implied variance at a given moneyness and a,b,rho,m,sigma the 5 SVI parameters.
A. Vogt described a particularly simple way to find an initial guess to fit SVI to an implied volatility slice a while ago. The idea to compute rho and sigma from the left and right asymptotic slopes.</description>
    </item>
    
    <item>
      <title>New SABR Formulae</title>
      <link>https://chasethedevil.github.io/post/new-sabr-formulae/</link>
      <pubDate>Wed, 16 Jul 2014 22:35:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/new-sabr-formulae/</guid>
      <description>In a talk at the Global Derivatives conference of Amsterdam (2014), Pat Hagan presented some new SABR formulas, supposedly close to the arbitrage free PDE behavior.
I tried to code those from the slides, but somehow that did not work out well on his example, I just had something very close to the good old SABR formulas. I am not 100% sure (only 99%) that it is due to a mistake in my code.</description>
    </item>
    
    <item>
      <title>Heston or Schobel-Zhu issues with short expiries</title>
      <link>https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/</link>
      <pubDate>Thu, 03 Jul 2014 23:28:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/</guid>
      <description>It&#39;s relatively well known that Heston does not fit the market for short expiries. Given that there are just 5 parameters to fit a full surface, it&#39;s almost logical that one part of the surface of it is not going to fit well the market.
I was more surprised to see how bad Heston or Schobel-Zhu were to fit a single short expiry volatility slice. As an example I looked at SP500 options with 1 week expiry.</description>
    </item>
    
    <item>
      <title>Moore-Penrose Inverse &amp; Gauss-Newton SABR Minimization</title>
      <link>https://chasethedevil.github.io/post/moore-penrose-inverse-gauss-newton-sabr-minimization/</link>
      <pubDate>Tue, 24 Jun 2014 15:29:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/moore-penrose-inverse-gauss-newton-sabr-minimization/</guid>
      <description>I have found a particularly nice initial guess to calibrate SABR. As it is quite close to the true best fit, it is tempting to use a very simple minimizer to go to the best fit. Levenberg-Marquardt works well on this problem, but can we shave off a few iterations?
I firstly considered the basic Newton&#39;s method, but for least squares minimization, the Hessian (second derivatives) is needed. It&#39;s possible to obtain it, even analytically with SABR, but it&#39;s quite annoying to derive it and code it without some automatic differentiation tool.</description>
    </item>
    
    <item>
      <title>On the importance of accuracy for bpvol solvers</title>
      <link>https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/</link>
      <pubDate>Thu, 12 Jun 2014 17:31:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/</guid>
      <description>While I was playing around calibrating the arbitrage free SABR model from Hagan (using the PDE on probability density approach), I noticed a misbehavior for some short maturity smiles. I thought it was due to the PDE implementation. Actually some of it was, but the remaining large error was due to the bpvol solver.
I initially took the same approach as Choi et al. in my solver, that is to work with in-the-money prices (they work with straddles) because it&#39;s nice and convenient.</description>
    </item>
    
    <item>
      <title>Two SABR for the same smile</title>
      <link>https://chasethedevil.github.io/post/two-sabr-for-the-same-smile/</link>
      <pubDate>Tue, 20 May 2014 12:08:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/two-sabr-for-the-same-smile/</guid>
      <description>While playing around with differential evolution and SABR calibration, I noticed that sometimes, several set of parameters can lead to a very similar smile, usually the good one is for relatively low vol of vol and the bad one is for relatively high vol of vol. I first looked for errors in my implementation, but it&amp;rsquo;s a real phenomenon.
 I used the normal implied volatility formula with beta=1, then converted it to lognormal (Black) volatility.</description>
    </item>
    
    <item>
      <title>Heston vs SABR slice by slice fit</title>
      <link>https://chasethedevil.github.io/post/heston-vs-sabr-slice-by-slice-fit/</link>
      <pubDate>Thu, 15 May 2014 22:06:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/heston-vs-sabr-slice-by-slice-fit/</guid>
      <description>Some people use Heston to fit one slice of a volatility surface. In this case, some parameters are clearly redundant. Still, I was wondering how it fared against SABR, which is always used to fit a slice. And what about Schobel-Zhu?
Aggregated error in fit per slice on 10 surfacesWith Heston, the calibration is actually slightly better with kappa=0, that is, without mean reversion, because the global optimization is easier and the mean reversion is fully redundant.</description>
    </item>
    
    <item>
      <title>Quadratic Spline with Knots at Mid-Points</title>
      <link>https://chasethedevil.github.io/post/quadratic-spline-with-knots-at-mid-points/</link>
      <pubDate>Wed, 14 May 2014 14:12:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quadratic-spline-with-knots-at-mid-points/</guid>
      <description>Two months ago, I looked at arbitrage free interpolation using piecewise-constant density. This is equivalent to a piecewise quadratic polynomial in call prices where each piece is centered around each call strike.
I wondered at the time what a quadratic spline would look like on this problem, as it should be very close in theory, except that we can ensure that it is C1, a condition for a good looking implied volatility.</description>
    </item>
    
    <item>
      <title>Non-linear Option Pricing</title>
      <link>https://chasethedevil.github.io/post/non-linear-option-pricing/</link>
      <pubDate>Fri, 18 Apr 2014 22:18:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/non-linear-option-pricing/</guid>
      <description>I am currently reading the book &#34;Nonlinear Option Pricing&#34; by J. Guyon and P. Henry-Labordère. It&#39;s quite interesting even if the first third is quite theoretical. For example they describe how to solve some not well defined non-linear parabolic PDE by relying on the parabolic envelope. They also explain why most problems lead to parabolic PDEs in finance. The rest is a bit more practical. I stumbled upon an good remark regarding Longstaff-Schwartz: the algorithm as Longstaff and Schwarz describe it does not necessary lead to a low-biased estimate as they use future information (the paths they regress on) in the Monte-Carlo estimate.</description>
    </item>
    
    <item>
      <title>Building a more accurate basis point volatility formula</title>
      <link>https://chasethedevil.github.io/post/building-a-more-accurate-basis-point-volatility-formula/</link>
      <pubDate>Sat, 05 Apr 2014 15:42:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/building-a-more-accurate-basis-point-volatility-formula/</guid>
      <description>P. Jaeckel has defied the limits of accuracy with hislatest Black-Scholes volatility solver, managing to also improve performance compared to his earlier solver &#34;By Implication&#34;. Out of a silly exercise, I decided to try my hand for a more accurate Normal (or basis point) volatility solver.
In reality, the problem is much simpler in the Bachelier/Normal model. A very basic analysis of Bachelier formula shows that the problem can be reduced to a single variable, as Choi et al explain in their paper.</description>
    </item>
    
    <item>
      <title>Fast and Accurate Implied Volatility Solver</title>
      <link>https://chasethedevil.github.io/post/fast-and-accurate-implied-volatility-solver/</link>
      <pubDate>Wed, 19 Mar 2014 18:10:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/fast-and-accurate-implied-volatility-solver/</guid>
      <description>The calibration of a stochastic volatility model or a volatility surface parameterization (like SVI) involves minimizing the model options volatilities against market options volatilities. Often, the model computes an option price, not an implied volatility. It is therefore useful to have a fast way to invert that option price to get back the implied volatility that corresponds to it. Furthermore during the calibration procedure, the model option price can vary widely: it is convenient to have a robust implied volatility solver.</description>
    </item>
    
    <item>
      <title>Arbitrage Free Interpolation of Option Prices using Piecewise Constant Density</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-interpolation-of-option-prices-using-piecewise-constant-density/</link>
      <pubDate>Mon, 17 Mar 2014 15:25:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/arbitrage-free-interpolation-of-option-prices-using-piecewise-constant-density/</guid>
      <description>Tension splines can produce in some cases arbitrage free C2 interpolation of options, but unfortunately this is not guaranteed. It turns out that, on some not so nice looking data, where the discrete probability density is not monotone but only positive, all previously considered interpolation fail (spline in volatility or variance, tension spline in log prices, harmonic spline on prices).
K &amp;nbsp; &amp;nbsp; vol&amp;nbsp;&amp;nbsp; put&amp;nbsp;&amp;nbsp; b-slope&amp;nbsp; b-convexity
300.0 0.682 0.</description>
    </item>
    
    <item>
      <title>C2 Arbitrage Free Interpolation with Tension Splines</title>
      <link>https://chasethedevil.github.io/post/c2-arbitrage-free-interpolation-with-tension-splines/</link>
      <pubDate>Tue, 11 Mar 2014 17:05:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/c2-arbitrage-free-interpolation-with-tension-splines/</guid>
      <description>In a previous post, I have explored the arbitrage free wiggles in the volatility surface that P. Jaeckel found in his paper. I showed that interpolating in log prices instead of prices was enough to remove the wiggles, but then, it appears that the interpolation is not guaranteed to be arbitrage free, even though it often is. On another example from P. Jaeckel paper, that I reproduced inaccurately but well enough, it is not.</description>
    </item>
    
    <item>
      <title>Bachelier and Black-Scholes Fits of the Volatility Surface, what about SABR?</title>
      <link>https://chasethedevil.github.io/post/bachelier-and-black-scholes-fits-of-the-volatility-surface-what-about-sabr/</link>
      <pubDate>Fri, 07 Mar 2014 15:31:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/bachelier-and-black-scholes-fits-of-the-volatility-surface-what-about-sabr/</guid>
      <description>I always wondered if Bachelier was really worse than Black-Scholes in practice. As an experiment I fit various implied volatility surfaces with Bachelier and Black-Scholes and look at the average error in implied volatility by slice.
In theory Bachelier is appealing because slightly simpler: log returns are a bit more challenging to think about than returns. And it also takes indirectly into account the fact that OTM calls are less likely than OTM puts because of default risk, if you assume absorbing probability at strike 0.</description>
    </item>
    
    <item>
      <title>Arbitrage Free Wiggles</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-wiggles/</link>
      <pubDate>Mon, 03 Mar 2014 17:13:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/arbitrage-free-wiggles/</guid>
      <description>Peter Jaeckel, in a recent paper (pdf), shows that something that sounds like a reasonable arbitrage free interpolation can produce wiggles in the implied volatility slice.
The interpolation in question is using some convexity preserving spline on call and put option prices directly and in strike, assuming those input prices are arbitrage free. This is very similar to Kahale interpolation (pdf).
It seemed too crazy for me so I had to try out his example.</description>
    </item>
    
    <item>
      <title>Adjoint Delta for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/adjoint-delta-for-monte-carlo/</link>
      <pubDate>Tue, 25 Feb 2014 18:37:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/adjoint-delta-for-monte-carlo/</guid>
      <description>In an earlier post, I have been quickly exploring adjoint differentiation in the context of analytical Black-Scholes. Today, I tried to mix it in a simple Black-Scholes Monte-Carlo as described in L. Capriotti paper, and measured the performance to compute delta compared to a numerical single sided finite difference delta.
I was a bit surprised that even on a single underlying, without any real optimization, adjoint delta was faster by a factor of nearly 40%.</description>
    </item>
    
    <item>
      <title>SVI on top of SABR</title>
      <link>https://chasethedevil.github.io/post/svi-on-top-of-sabr/</link>
      <pubDate>Thu, 20 Feb 2014 18:36:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/svi-on-top-of-sabr/</guid>
      <description>Several papers show that the limit for large strikes of Heston is SVI.
Interestingly, I stumbled onto a surface where the Hagan SABR fit was perfect as well as the SVI fit, while the Heston fit was not.
Originally, I knew that, on this data, the SVI fit was perfect. Until today, I just never tried to fit a lognormal SABR on the same data. I did a small test with random values of the SABR parameters alpha, rho, nu, and found out that in deed, the SVI fit is always perfect on SABR.</description>
    </item>
    
    <item>
      <title>Smart Initial Guess for Schobel-Zhu</title>
      <link>https://chasethedevil.github.io/post/smart-initial-guess-for-schobel-zhu/</link>
      <pubDate>Wed, 19 Feb 2014 18:57:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/smart-initial-guess-for-schobel-zhu/</guid>
      <description>With a small time expansion, it is easy to derive a reasonable initial guess, without resorting to some global minimizer.
Like Forde &amp;amp; al did for Heston, one can find the 5 Schobel-Zhu parameters through 5 points at coordinates (0,0), (x0,t1), (-x0,t1), (x0,t2), (-x0,t2), where x0 is a chosen the log-moneyness, for example, 0.1 and t1, t2 relatively short expiries (for example, 0.1, 0.25).
We can truncate the small time expansion so that the polynomial in (x,t) is fully captured by those 5 points.</description>
    </item>
    
    <item>
      <title>A Look at Small Time Expansions for Heston</title>
      <link>https://chasethedevil.github.io/post/a-look-at-small-time-expansions-for-heston/</link>
      <pubDate>Wed, 12 Feb 2014 13:13:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-look-at-small-time-expansions-for-heston/</guid>
      <description>Small time expansions for Heston can be useful during the calibration of the implied volatility surface, in order to find an initial guess for a local minimizer (for example, Levenberg-Marquardt). Even if they are not so accurate, they capture the dynamic of the model parameters, and that is often enough.
In 2011, Forde et al. proposed a second order small time expansion around the money, which I found to work well for calibration.</description>
    </item>
    
    <item>
      <title>A Small-Time Schobel-Zhu Expansion</title>
      <link>https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion/</link>
      <pubDate>Mon, 10 Feb 2014 18:30:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion/</guid>
      <description>The paper implied vol for any local stochastic vol model from Lorig et al. presents a very generic and simple formula to compute implied volatility expansions up to order-2 (there is actually an order-3 formula available in their Mathematica CDF file).I tried it on the Schobel-Zhu stochastic volatility model. This model is an interesting alternative to Heston. I found that, in practice, the implied volatility surface fit was as good, while the simulation under the QE scheme is quite faster (and simpler) than Heston.</description>
    </item>
    
    <item>
      <title>Brownian Bridge or Not with Heston Quadratic Exponential QMC</title>
      <link>https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston-quadratic-exponential-qmc/</link>
      <pubDate>Fri, 24 Jan 2014 19:35:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston-quadratic-exponential-qmc/</guid>
      <description>At first I did not make use of the Brownian Bridge technique in Heston QMC, because the variance process is not simulated like a Brownian Motion under the Quadratic Exponential algorithm from Andersen.

It is, however, perfectly possible to use the Brownian Bridge on the asset process. Does it make a difference? In my small test, it does not seem to make a difference. An additional question would be, is it better to take first N for the asset and next N for the variance or vice versa or intertwined?</description>
    </item>
    
    <item>
      <title>Adjoint Algorithmic Differentiation for Black-Scholes</title>
      <link>https://chasethedevil.github.io/post/adjoint-algorithmic-differentiation-for-black-scholes/</link>
      <pubDate>Tue, 21 Jan 2014 13:03:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/adjoint-algorithmic-differentiation-for-black-scholes/</guid>
      <description>Adjoint algorithmic differentiation is particularly interesting in finance as we often encounter the case of a function that takes many input (the market data) and returns one output (the price) and we would like to also compute sensitivities (greeks) to each input.
As I am just starting around it, to get a better grasp, I first tried to apply the idea to the analytic knock out barrier option formula, by hand, only to find out I was making way too many errors by hand to verify anything.</description>
    </item>
    
    <item>
      <title>Placing the Strike on the Grid and Payoff Smoothing in Finite Difference Methods for Vanilla Options</title>
      <link>https://chasethedevil.github.io/post/placing-the-strike-on-the-grid-and-payoff-smoothing-in-finite-difference-methods-for-vanilla-options/</link>
      <pubDate>Sun, 12 Jan 2014 16:27:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/placing-the-strike-on-the-grid-and-payoff-smoothing-in-finite-difference-methods-for-vanilla-options/</guid>
      <description>Pooley et al., in Convergence Remedies for non-smooth payoffs in option pricing suggest that placing the strike on the grid for a Vanilla option is good enough:

At the same time, Tavella and Randall show in their book that numerically, placing the strike in the middle of two nodes leads to a more accurate result. My own numerical experiments confirm Tavella and Randall suggestion.
In reality, what Pooley et al.</description>
    </item>
    
    <item>
      <title>Coordinate Transform of the Andreasen Huge SABR PDE &amp; Spline Interpolation</title>
      <link>https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde-spline-interpolation/</link>
      <pubDate>Wed, 08 Jan 2014 18:51:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde-spline-interpolation/</guid>
      <description>Recently, I noticed how close are the two PDE based approaches from Andreasen-Huge and Hagan for an arbitrage free SABR. Hagan gives a local volatility very close to the one Andreasen-Huge use in the forward PDE in call prices. A multistep Andreasen-Huge (instead of their one step PDE method) gives back prices and densities nearly equal to Hagan density based approach.
Hagan proposed in some unpublished paper a coordinate transformation for two reasons: the ideal range of strikes for the PDE can be very large, and concentrating the points where it matters should improve stability and accuracy.</description>
    </item>
    
    <item>
      <title>Levenberg Marquardt &amp; Constraints by Domain Transformation</title>
      <link>https://chasethedevil.github.io/post/levenberg-marquardt-constraints-by-domain-transformation/</link>
      <pubDate>Tue, 17 Dec 2013 15:27:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/levenberg-marquardt-constraints-by-domain-transformation/</guid>
      <description>The Fortran minpack library has a good Levenberg-Marquardt minimizer, so good, that it has been ported to many programming languages. Unfortunately it does not support contraints, even simple bounds.
One way to achieve this is to transform the domain via a bijective function. For example, \(a+\frac{b-a}{1+e^{-\alpha t}}\) will transform \(]-\infty, +\infty[\) to ]a,b[. Then how should one choose \(\alpha\)?
A large \(\alpha\) will make tiny changes in \(t\) appear large.</description>
    </item>
    
    <item>
      <title>Arbitrage Free SABR - Another View on Hagan Approach</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-sabr-another-view-on-hagan-approach/</link>
      <pubDate>Sat, 14 Dec 2013 00:56:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/arbitrage-free-sabr-another-view-on-hagan-approach/</guid>
      <description>Several months ago, I took a look attwo interesting recent ways to price under SABR with no arbitrage:
One way is due to Andreasen and Huge, where they find an equivalent local volatility expansion, and then use a one-step finite difference technique to price.The other way is due to Hagan himself, where he numerically solves an approximate PDE in the probability density, and then price with options by integrating on this density.</description>
    </item>
    
    <item>
      <title>American Option on Forward/Futures</title>
      <link>https://chasethedevil.github.io/post/american-option-on-forwardfutures/</link>
      <pubDate>Thu, 21 Nov 2013 11:17:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/american-option-on-forwardfutures/</guid>
      <description>Prices of a Future contract and a Forward contract are the same under the Black-Scholes assumptions (deterministic rates) but the price of options on Futures or options on Forwards might still differ. I did not find this obvious at first.
For example, when the underlying contract expiration date (Futures, Forward) is different from the option expiration date. For a Future Option, the Black-76 formula can be used, the discounting is done from the option expiry date, because one receives the cash on expiration due to the margin account.</description>
    </item>
    
    <item>
      <title>Spikes in Heston/Schobel-Zhu Local Volatility</title>
      <link>https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/</link>
      <pubDate>Wed, 20 Nov 2013 13:33:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/</guid>
      <description>Using precise vanilla option pricing engine for Heston or Schobel-Zhu, like the Cos method with enough points and a large enough truncation can still lead to spikes in the Dupire local volatility (using the variance based formula).
Local Volatility
Implied Volatility
The large spikes in the local volatility 3d surface are due to constant extrapolation, but there are spikes even way before the extrapolation takes place at longer maturities. Even if the Cos method is precise, it seems to be not enough, especially for large strikes so that the second derivative over the strike combined with the first derivative over time can strongly oscillate.</description>
    </item>
    
    <item>
      <title>Local Stochastic Volatility with Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/</link>
      <pubDate>Wed, 16 Oct 2013 16:14:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/</guid>
      <description>I always imagined local stochastic volatility to be complicated, and thought it would be very slow to calibrate.
After reading a bit about it, I noticed that the calibration phase could just consist in calibrating independently a Dupire local volatility model and a stochastic volatility model the usual way.
One can then choose to compute on the fly the local volatility component (not equal the Dupire one, but including the stochastic adjustment) in the Monte-Carlo simulation to price a product.</description>
    </item>
    
    <item>
      <title>Heston, Schobel-Zhu, Bates, Double-Heston Fit</title>
      <link>https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit/</link>
      <pubDate>Mon, 07 Oct 2013 19:35:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit/</guid>
      <description>I did some experiments fitting Heston, Schobel-Zhu, Bates and Double-Heston to a real world equity index implied volatility surface. I used a global optimizer (differential evolution).
To my surprise, the Heston fit is quite good: the implied volatility error is less than 0.42% on average. Schobel-Zhu fit is also good (0.47% RMSE), but a bit worse than Heston. Bates improves quite a bit on Heston although it has 3 more parameters, we can see the fit is better for short maturities (0.</description>
    </item>
    
    <item>
      <title>Second Cumulant of Heston</title>
      <link>https://chasethedevil.github.io/post/second-cumulant-of-heston/</link>
      <pubDate>Thu, 03 Oct 2013 17:27:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/second-cumulant-of-heston/</guid>
      <description>I recently stumbled upon an error in the various papers related to the Heston Cos method regarding the second cumulant. It is used to define the boundaries of the Cos method.
Letting phi be Heston characteristic function, the cumulant generating function is:
$$g(u) = \log(\phi(-iu))$$
And the second cumulant is defined a: $$c_2 = g&#39;&#39;(0)$$
Compared to a numerical implementation, the c_2 from the paper is really off in many use cases.</description>
    </item>
    
    <item>
      <title>Maxima for Symbolic Calculus</title>
      <link>https://chasethedevil.github.io/post/maxima-for-symbolic-calculus/</link>
      <pubDate>Wed, 02 Oct 2013 15:06:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/maxima-for-symbolic-calculus/</guid>
      <description>A few years ago, I found an interesting open source symbolic calculus software called Xcas. It can however be quickly limited, for example, it does not seem to work well to compute Taylor expansions with several embedded functions.
Google pointed me to another popular open source package, Maxima. It looks a bit rudimentary (command like interface), but formulas can actually be very easily exported to latex with the tex command.</description>
    </item>
    
    <item>
      <title>Making Classic Heston Integration Faster than the Cos Method</title>
      <link>https://chasethedevil.github.io/post/making-classic-heston-integration-faster-than-the-cos-method/</link>
      <pubDate>Thu, 05 Sep 2013 17:35:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/making-classic-heston-integration-faster-than-the-cos-method/</guid>
      <description>A coworker pointed to me that Andersen and Piterbarg book &#34;Interest Rate Modeling&#34; had a chapter on Fourier integration applied to Heston. The authors rely on the Lewis formula to price vanilla call options under Heston.
Lewis formulaMore importantly, they strongly advise the use of a Black-Scholes control variate. I had read about that idea before, and actually tried it in the Cos method, but it did not improve anything for the Cos method.</description>
    </item>
    
    <item>
      <title>Attari, Lord-Kahl &amp; Cos Methods Comparison on Heston</title>
      <link>https://chasethedevil.github.io/post/attari-lord-kahl-cos-methods-comparison-on-heston/</link>
      <pubDate>Wed, 28 Aug 2013 17:54:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/attari-lord-kahl-cos-methods-comparison-on-heston/</guid>
      <description>I recently wrote about the Cos method. While rereading the various papers on Heston semi-analytical pricing, especially the nice summary by Schmelzle, it struck me how close were the Attari/Bates methods and the Cos method derivations. I then started wondering if Attari was really much worse than the Cos method or not.
I noticed that Attari method accuracy is directly linked to the underlying Gaussian quadrature method accuracy. I found that the doubly adaptive Newton-Cotes quadrature by Espelid (coteda) was the most accurate/fastest on this problem (compared to Gauss-Laguerre/Legendre/Extrapolated Simpson/Lobatto).</description>
    </item>
    
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>I just stumbled upon Julia, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.
Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library.</description>
    </item>
    
    <item>
      <title>The COS method for Heston</title>
      <link>https://chasethedevil.github.io/post/the-cos-method-for-heston/</link>
      <pubDate>Fri, 02 Aug 2013 14:19:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-cos-method-for-heston/</guid>
      <description>Fang, in his thesis, has the idea of the COS method and applies it to Heston. There are several published papers around it to price options under various models that have a known characteristic function, as well as to price more exotic options like barriers or bermudans.
The COS method is very close to the more standard Heston quasi analytic formula&amp;nbsp;(use transform of characteristic function for the density and integrates the payoff with the density, exchanging summation), except that the more simple Fourier series are used instead of the standard Fourier transform.</description>
    </item>
    
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>I was used to Scilab for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.
Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the Arbitrage Free SABR problem.</description>
    </item>
    
    <item>
      <title>The CUDA Performance Myth II</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</link>
      <pubDate>Fri, 12 Jul 2013 15:23:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</guid>
      <description>This is a kind of following to the CUDA performance myth. There is a recent news on the java concurrent mailing list about SplittableRandom class proposed for JDK8. It is a new parallel random number generator a priori usable for Monte-Carlo simulations.
It seems to rely on some very recent algorithm. There are some a bit older ones: the ancestor, L&#39;Ecuyer MRG32k3a that can be parallelized through relatively costless skipTo methods, a Mersenne Twister variant MTGP, and even the less rigourous XorWow popularized by NVidia CUDA.</description>
    </item>
    
    <item>
      <title>Bessel and Harmonic Kinks in the Forward</title>
      <link>https://chasethedevil.github.io/post/bessel-and-harmonic-kinks-in-the-forward/</link>
      <pubDate>Tue, 02 Jul 2013 15:44:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/bessel-and-harmonic-kinks-in-the-forward/</guid>
      <description>As Bessel (sometimes called Hermite) spline interpolation is only C1, like the Harmonic spline from Fritsch-Butland, the forward presents small kinks compared to a standard cubic spline. Hyman filtering also creates a kink where it fixes the monotonicity. Those are especially visible with a log scale in time. Here is how it looks on the Hagan-West difficult curve.</description>
    </item>
    
    <item>
      <title>The Finite Difference Theta Scheme Optimal Theta</title>
      <link>https://chasethedevil.github.io/post/the-finite-difference-theta-scheme-optimal-theta/</link>
      <pubDate>Tue, 18 Jun 2013 15:02:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-finite-difference-theta-scheme-optimal-theta/</guid>
      <description>The theta finite difference scheme is a common generalization of Crank-Nicolson. In finance, the book from Wilmott, a paper from A. Sepp, one from Andersen-Ratcliffe present it. Most of the time, it&#39;s just a convenient way to handle implicit \(\theta=1\), explicit \(\theta=0\) and Crank-Nicolson \(\theta=0.5\) with the same algorithm.
Wilmott makes an interesting remark: one can choose a theta that will cancel out higher order terms in the local truncation error and therefore should lead to increased accuracy.</description>
    </item>
    
    <item>
      <title>Akima for Yield Curve Interpolation ?</title>
      <link>https://chasethedevil.github.io/post/akima-for-yield-curve-interpolation-/</link>
      <pubDate>Mon, 03 Jun 2013 00:07:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/akima-for-yield-curve-interpolation-/</guid>
      <description>On my test of yield curve interpolations, focusing on parallel delta versus sequential delta, Akima is the worst of the lot. I am not sure why this interpolation is still popular when most alternatives seem much better. Hyman presented some of the issues with Akima in his paper in 1983. In the following graph, a higher value is a higher parallel-vs-sequential difference. That plus the Hagan-West example of a tricky curve looks a bit convoluted with it (although it does not have any negative forward).</description>
    </item>
    
    <item>
      <title>2 Ways for an Accurate Barrier with Finite Difference </title>
      <link>https://chasethedevil.github.io/post/2-ways-for-an-accurate-barrier-with-finite-difference/</link>
      <pubDate>Sun, 02 Jun 2013 00:46:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/2-ways-for-an-accurate-barrier-with-finite-difference/</guid>
      <description>I had explored the issue of pricing a barrier using finite difference discretization of the Black-Scholes PDE a few years ago. Briefly, for explicit schemes, one just need to place the barrier on the grid and not worry about much else, but for implicit schemes, either the barrier should be placed on the grid and the grid&amp;nbsp; truncated at the barrier, or a fictitious point should be introduced to force the correct price at the barrier level (0, typically).</description>
    </item>
    
    <item>
      <title>SABR with the new Hagan PDE Approach</title>
      <link>https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</link>
      <pubDate>Tue, 28 May 2013 15:56:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</guid>
      <description>At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it&#39;s just like a regular PDE with proper boundary conditions.
I was wondering how it compared to Andreasen Huge results.

My first implementation was quite slow.</description>
    </item>
    
    <item>
      <title>SABR with Andreasen-Huge</title>
      <link>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</link>
      <pubDate>Fri, 24 May 2013 14:17:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</guid>
      <description>I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.
Around 2 years ago, I saw a presentation from Andreasen and Huge about how they were able to price/calibrate SABR by a one-step finite difference technique.</description>
    </item>
    
    <item>
      <title>Large Steps in Schobel-Zhu/Heston the Lazy Way</title>
      <link>https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/</link>
      <pubDate>Fri, 17 May 2013 12:46:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/</guid>
      <description>Van Haastrecht, Lord and Pelsser present an effective way to price derivatives by Monte-Carlo under the Schobel-Zhu model (as well as under the Schobel-Zhu-Hull-White model). It&#39;s quite similar to Andersen QE scheme for Heston in spirit.
In their paper they evolve the (log) asset process together with the volatility process, using the same discretization times. A while ago, when looking at&amp;nbsp; Joshi and Chan large steps for Heston, I noticed that, inspired by Broadie-Kaya exact Heston scheme, they present the idea to evolve the variance process using small steps and the asset process using large steps (depending on the payoff) using the integrated variance value computed by small steps.</description>
    </item>
    
    <item>
      <title>Exact Forward in Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</link>
      <pubDate>Mon, 13 May 2013 17:58:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</guid>
      <description>Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.
In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &#34;The Equity Option Volatility Smile: a Finite Difference Approach&#34; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices.</description>
    </item>
    
    <item>
      <title>Quasi Monte Carlo in Finance</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/</link>
      <pubDate>Mon, 13 May 2013 13:16:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/</guid>
      <description>I have been wondering if there was any better alternative than the standard Sobol (+ Brownian Bridge) quasi random sequence generator for the Monte Carlo simulations of finance derivatives.
Here is what I found:
Scrambled Sobol. The idea is to rerandomize the quasi random numbers slightly. It can provide better uniformity properties and allows for a real estimate of the standard error. There are many ways to do that. The simple Cranley Patterson rotation consisting in adding a pseudo random number modulo 1, Owen scrambling (permutations of the digits) and simplifications of it to achieve a reasonable speed.</description>
    </item>
    
    <item>
      <title>Upper Bounds in American Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</link>
      <pubDate>Tue, 30 Apr 2013 17:05:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</guid>
      <description>Glasserman and Yu (GY) give a relatively simple algorithm to compute lower and upper bounds of a the price of a Bermudan Option through Monte-Carlo.
I always thought it was very computer intensive to produce an upper bound, and that the standard Longstaff Schwartz algorithm was quite precise already. GY algorithm is not much slower than the Longstaff-Schwartz algorithm, but what&#39;s a bit tricky is the choice of basis functions: they have to be Martingales.</description>
    </item>
    
    <item>
      <title>Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo-longstaff-schwartz-american-option-price/</link>
      <pubDate>Mon, 22 Apr 2013 18:00:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo-longstaff-schwartz-american-option-price/</guid>
      <description>In the book Monte Carlo Methods in Financial Engineering, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.
However Longstaff and Schwartz seem to just reuse the paths in their paper, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.</description>
    </item>
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.
Can we improve the speed of exp over the JDK one?</description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part III)</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii/</link>
      <pubDate>Fri, 12 Apr 2013 13:41:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii/</guid>
      <description>I forgot two important points in my previous post about Lord-Kahl method to compute the Heston call price:
- Scaling: scaling the call price appropriately allows to increase the maximum precision significantly, because the Carr-Madan formula operates on log(Forward) and log(Strike) directly, but not the ratio, and alpha is multiplied by the log(Forward). I simply scale by the spot, the call price is \(S_0*max(S/S_0-K/S0)\). Here are the results for Lord-Kahl, Kahl-Jaeckel (the more usual way limited to machine epsilon accuracy), Forde-Jacquier-Lee ATM implied volatility without scaling for a maturity of 1 day:</description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>In my previous post, I explored the Lord-Kahl method to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise.</description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</link>
      <pubDate>Tue, 09 Apr 2013 19:49:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</guid>
      <description>I just tried to implement Lord Kahl algorithm to compute the Heston call price. The big difficulty of their method is to find the optimal alpha. That&#39;s what make it work or break. The tricky part is that the function of alpha we want to minimize has multiple discontinuities (it&#39;s periodic in some ways). This is why the authors rely on the computation of an alpha_max: bracketing is very important, otherwise your optimizer will jump the discontinuity without even noticing it, while you really want to stay in the region before the first discontinuity.</description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>Marsaglia in his paper on Normal Distribution made the same mistake I initially did while trying to verify the accuracy of the normal density.
In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.</description>
    </item>
    
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>In my previous post, I stated that some library (SPECFUN by W.D. Cody) computes $$e^{-\frac{x^2}{2}}$$ the following way:
xsq = fint(x * 1.6) / 1.6;
del = (x - xsq) * (x + xsq);
result = exp(-xsq * xsq * 0.5) * exp(-del *&amp;nbsp;0.5);
where fint(z) computes the floor of z.
1. Why 1.6?
An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.</description>
    </item>
    
    <item>
      <title>A Double Precision Puzzle with the Gaussian</title>
      <link>https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/</link>
      <pubDate>Wed, 20 Mar 2013 17:50:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/</guid>
      <description>Some library computes $$e^{-\frac{x^2}{2}}$$ the following way:
xsq = fint(x * 1.6) / 1.6;
del = (x - xsq) * (x + xsq);
result = exp(-xsq * xsq * 0.5) * exp(-del * 0.5);
where fint(z) computes the floor of z.
Basically, x*x is rewritten as xsq*xsq+del. I have seen that trick once before, but I just can&#39;t figure out where and why (except that it is probably related to high accuracy issues).</description>
    </item>
    
    <item>
      <title>A Seasoned Volatility Swap</title>
      <link>https://chasethedevil.github.io/post/a-seasoned-volatility-swap/</link>
      <pubDate>Thu, 14 Mar 2013 19:55:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-seasoned-volatility-swap/</guid>
      <description>This is very much what&#39;s in the Carr-Lee paper &#34;Robust Replication of Volatility Derivatives&#34;, but it wasn&#39;t so easy to obtain in practice:
The formulas as written in the paper are not usable as is: they can be simplified (not too difficult, but intimidating at first)The numerical integration is not trivial: a simple Gauss-Laguerre is not precise enough (maybe if I had an implementation with more points), a Gauss-Kronrod is not either (maybe if we split it in different regions).</description>
    </item>
    
    <item>
      <title>A Volatility Swap and a Straddle</title>
      <link>https://chasethedevil.github.io/post/a-volatility-swap-and-a-straddle/</link>
      <pubDate>Tue, 12 Mar 2013 21:36:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-volatility-swap-and-a-straddle/</guid>
      <description>A Volatility swap is a forward contract on future realized volatility. The pricing of such a contract used to be particularly challenging, often either using an unprecise popular expansion in the variance, or a model specific way (like Heston or local volatility with Jumps). Carr and Lee have recently proposed a way to price those contracts in a model independent way in their paper &#34;robust replication of volatility derivatives&#34;. Here is the difference between the value of a synthetic volatility swap payoff at maturity (a newly issued one, with no accumulated variance) and a straddle.</description>
    </item>
    
    <item>
      <title>Local Volatility Delta &amp; Dynamic</title>
      <link>https://chasethedevil.github.io/post/local-volatility-delta-dynamic/</link>
      <pubDate>Thu, 29 Nov 2012 12:30:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/local-volatility-delta-dynamic/</guid>
      <description>This will be a very technical post, I am not sure that it will be very understandable by people not familiar with the implied volatility surface.

Something one notices when computing an option price under local volatility using a PDE solver, is how different is the Delta from the standard Black-Scholes Delta, even though the price will be very close for a Vanilla option. In deed, the Finite difference grid will have a different local volatility at each point and the Delta will take into account a change in local volatility as well.</description>
    </item>
    
    <item>
      <title>GPU computing in Finance</title>
      <link>https://chasethedevil.github.io/post/gpu-computing-in-finance/</link>
      <pubDate>Mon, 15 Oct 2012 16:14:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/gpu-computing-in-finance/</guid>
      <description>Very interesting presentation from Murex about their GPU computing. Some points were:
- GPU demand for mostly exotics pricing &amp;amp; greeks
- Local vol main model for EQD exotics. Local vol calibrated via PDE approach.
- Markov functional model becoming main model for IRD.
- Use of local regression instead of Longstaff Schwartz (or worse CVA like sim of sim).
- philox RNG from DE Shaw. But the presenter does not seem to know RNGs very well (recommended Brownian Bridge for Mersenne Twister!</description>
    </item>
    
    <item>
      <title>Binary Voting</title>
      <link>https://chasethedevil.github.io/post/binary-voting/</link>
      <pubDate>Fri, 07 Sep 2012 17:21:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/binary-voting/</guid>
      <description>How many reports have you had to fill up with a number of stars to choose? How much useless time is spent on figuring the this number just because it is always very ambiguous?
Some blogger wrote an interesting entry on Why I Hate Five Stars Reviews. Basically he advocates binary voting instead via like/dislike. Maybe a ternary system via like/dislike/don&#39;t care would be ok too.
One coworker used to advocate the same for a similar reason: people reading those reports only pay attention to the extremes: the 5 stars or the 0 stars.</description>
    </item>
    
    <item>
      <title>Adaptive Quadrature for Pricing European Option with Heston</title>
      <link>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</link>
      <pubDate>Mon, 25 Jun 2012 12:50:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</guid>
      <description>The Quantlib code to evaluate the Heston integral for European options is quite nice. It proposes Kahl &amp;amp; Jaeckel method as well as Gatheral method for the complex logarithm. It also contains expansions where it matters so that the resulting code is very robust. One minor issue is that it does not integrate both parts at the same time, and also does not propose Attari method for the Heston integral that is supposed to be more stable.</description>
    </item>
    
    <item>
      <title>Gnome Shell more stable than Unity on Ubuntu 12.04</title>
      <link>https://chasethedevil.github.io/post/gnome-shell-more-stable-than-unity-on-ubuntu-12.04/</link>
      <pubDate>Thu, 14 Jun 2012 12:01:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/gnome-shell-more-stable-than-unity-on-ubuntu-12.04/</guid>
      <description>Regularly, the unity dock made some applications inaccessible: clicking on the app icon did not show or start the app anymore, a very annoying bug. This is quite incredible given that this version of Ubuntu is supposed to be long term support. So I decided to give one more chance to Gnome Shell. Installing it on Ubuntu 12.04 is simple with this guide. To my surprise it is very stable so far.</description>
    </item>
    
    <item>
      <title>Why primitive arrays matter in Java</title>
      <link>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</link>
      <pubDate>Wed, 29 Feb 2012 10:01:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</guid>
      <description> In the past, I have seen that one could greatly improve performance of some Monte-Carlo simulation by using as much as possible double[][] instead of arrays of objects.
It was interesting to read this blog post explaining why that happens: it is all about memory access. </description>
    </item>
    
    <item>
      <title>KDE 4.8 finally has a dock</title>
      <link>https://chasethedevil.github.io/post/kde-4.8-finally-has-a-dock/</link>
      <pubDate>Fri, 27 Jan 2012 13:38:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/kde-4.8-finally-has-a-dock/</guid>
      <description>KDE 4.8 finally has a dock: you just have to add the plasma icon tasks. Also the flexibility around ALT+TAB is welcome. With Krusader as file manager, Thunderbird and Firefox for email and web, it is becoming a real nice desktop, but it took a while since the very bad KDE 4.0 release.
It is easy to install under ubuntu 11.10 through the backports and seems very stable so far.</description>
    </item>
    
    <item>
      <title>Generating random numbers following a given discrete probability distribution</title>
      <link>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</link>
      <pubDate>Mon, 09 Jan 2012 00:14:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</guid>
      <description>I have never really thought very much about generating random numbers according to a precise discrete distribution, for example to simulate an unfair dice. In finance, we are generally interested in continuous distributions, where there is typically 2 ways: the inverse transform (usually computed in a numerical way), and the acceptance-rejection method (typically the ziggurat). The inverse transform is often preferred, because it&#39;s usable method for Quasi Monte-Carlo simulations while the acceptance rejection is not.</description>
    </item>
    
    <item>
      <title>Quant Interview &amp; Education</title>
      <link>https://chasethedevil.github.io/post/quant-interview-education/</link>
      <pubDate>Wed, 21 Dec 2011 17:37:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/quant-interview-education/</guid>
      <description>Recently, I interviewed someone for a quant position. I was very surprised to find out that someone who did one of the best master in probabilities and finance in France could not solve a very basic probability problem:
This is accessible to someone with very little knowledge of probabilities When I asked this problem around to co-workers (who have all at least a master in a scientific subject), very few could actually answer it properly.</description>
    </item>
    
    <item>
      <title>Gnome 3 not so crap after all</title>
      <link>https://chasethedevil.github.io/post/gnome-3-not-so-crap-after-all/</link>
      <pubDate>Wed, 30 Nov 2011 18:11:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/gnome-3-not-so-crap-after-all/</guid>
      <description>In a previous post, I was complaining how bad Gnome 3 was. Since I have installed a real dock: docky, it is now much more usable. I can easily switch / launch applications without an annoying full screen change.
In addition I found out that it had a good desktop search (tracker). The ALT+F2 also does some sort of completion, too bad it can not use tracker here as well.</description>
    </item>
    
    <item>
      <title>Good &amp; Popular Algorithms are Simple</title>
      <link>https://chasethedevil.github.io/post/good-popular-algorithms-are-simple/</link>
      <pubDate>Thu, 17 Nov 2011 12:28:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/good-popular-algorithms-are-simple/</guid>
      <description>I recently tried to minimise a function according to some constraints. One popular method to minimise a function in several dimensions is Nelder-Mead Simplex. It is quite simple, so simple that I programmed it in Java in 1h30, including a small design and a test. It helped that the original paper from Nelder-Mead is very clear:
However the main issue is that it works only for unconstrained problems. Nelder and Mead suggested to add a penalty, but in practice this does not work so well.</description>
    </item>
    
    <item>
      <title>exp(y*log(x)) Much Faster than Math.pow(x,y)</title>
      <link>https://chasethedevil.github.io/post/expylogx-much-faster-than-math.powxy/</link>
      <pubDate>Fri, 08 Apr 2011 23:03:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/expylogx-much-faster-than-math.powxy/</guid>
      <description> Today I found out that replacing Math.pow(x,y) by Math.exp(y*Math.log(x)) made me gain 50% performance in my program. Of course, both x and y are double in my case. I find this quite surprising, I expected better from Math.pow. </description>
    </item>
    
    <item>
      <title>SIMD and Mersenne-Twister</title>
      <link>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</link>
      <pubDate>Sat, 05 Feb 2011 13:18:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</guid>
      <description>Since 2007, there is a new kind of Mersenne-Twister (MT) that exploits SIMD architecture, the SFMT. The Mersenne-Twister has set quite a standard in random number generation for Monte-Carlo simulations, even though it has flaws.
I was wondering if SFMT improved the performance over MT for a Java implementation. There is actually on the same page a decent Java port of the original algorithm. When I ran it, it ended up slower by more than 20% than the classical Mersenne-Twister (32-bit) on a 64-bit JDK 1.</description>
    </item>
    
    <item>
      <title>XORWOW L&#39;ecuyer TestU01 Results</title>
      <link>https://chasethedevil.github.io/post/xorwow-lecuyer-testu01-results/</link>
      <pubDate>Wed, 12 Jan 2011 20:26:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/xorwow-lecuyer-testu01-results/</guid>
      <description>Nvidia uses XorWow random number generator in its CURAND library. It is a simple and fast random number generator with a reasonably long period. It can also be parallelized relatively easily. Nvidia suggests it passes L&#39;Ecuyer TestU01, but is not very explicit about it. So I&#39;ve decided to see how it performed on TestU01.I found very simple to test a new random number generator on TestU01, the documentation is great and the examples helpful.</description>
    </item>
    
    <item>
      <title>The CUDA Performance Myth</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth/</link>
      <pubDate>Mon, 03 Jan 2011 16:07:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth/</guid>
      <description>There is an interesting article on how to generate efficiently the inverse of the normal cumulative distribution on the GPU. This is useful for Monte-Carlo simulations based on normally distributed variables.
Another result of the paper is a method (breakless algorithm) to compute it apparently faster than the very good Wichura&#39;s AS241 algorithm on the CPU as well keeping a similar precision. The key is to avoid branches (if-then) at the cost of not avoiding log() calls.</description>
    </item>
    
    <item>
      <title>Another Look at Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</link>
      <pubDate>Mon, 29 Nov 2010 12:45:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</guid>
      <description>A while ago, I was already looking for a good Java Matrix library, complaining that there does not seem any real good one where development is still active: the 2 best ones are in my opinion Jama and Colt.
Recently I tried to price options via RBF (radial basis functions) based on TR-BDF2 time stepping. This is a problem where one needs to do a few matrix multiplications and inverses (or better, LU solve) in a loop.</description>
    </item>
    
    <item>
      <title>Street Fighting Mathematics Book</title>
      <link>https://chasethedevil.github.io/post/street-fighting-mathematics-book/</link>
      <pubDate>Wed, 28 Jul 2010 14:25:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/street-fighting-mathematics-book/</guid>
      <description> The&amp;nbsp;MIT has a downloadable book on basic mathematics: Street Fighting Mathematics. I liked the part focused on the geometrical approach. It reminded me of the early greek mathematics.
Overall it does look like a very American approach to Maths: answering a multiple choices questions test by elimination. But it is still an interesting book. </description>
    </item>
    
    <item>
      <title>double[][] Is Fine</title>
      <link>https://chasethedevil.github.io/post/double-is-fine/</link>
      <pubDate>Thu, 26 Nov 2009 14:51:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/double-is-fine/</guid>
      <description>In my previous post, I suggest that keeping a double[] performs better than keeping a double[][] if you do matrix multiplications and other operations.
This is actually not true. I benchmarked 3 libraries, Colt (uses double[]), Apache Commons Math (uses double[][]) and Jama (uses double[][] cleverly). At first it looks like Jama has a similar performance as Colt (they avoid [][] slow access by a clever algorithm). But once hotspot hits, the difference is crazy and Jama becomes the fastest (Far ahead).</description>
    </item>
    
    <item>
      <title>The Pain of Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries/</link>
      <pubDate>Thu, 26 Nov 2009 09:17:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries/</guid>
      <description>Looking for a good Java Matrix (and actually also math) library, I was a bit surprised to find out there does not seem to be any really serious one still maintained.
Sure, there is Apache Commons Math, but it is still changing a lot, and it is not very performance optimized yet, while it has been active for several years already. There is also Java3D, it does Matrix through GMatrix, but not much linear algebra and if you look at their implementation, it is very basic, not performance oriented.</description>
    </item>
    
  </channel>
</rss>
