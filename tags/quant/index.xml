<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Quant on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/quant/</link>
    <description>Recent content in Quant on Chase the Devil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Tue, 22 Apr 2025 12:56:42 +0100</lastBuildDate>
    <atom:link href="https://chasethedevil.github.io/tags/quant/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A New Asian Basket Spread Option Approximation</title>
      <link>https://chasethedevil.github.io/post/new_spread_option_approximation/</link>
      <pubDate>Tue, 22 Apr 2025 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/new_spread_option_approximation/</guid>
      <description>&lt;p&gt;Around 10 years ago, while reading the excellent paper of Etore and Gobet on &lt;a href=&#34;https://hal.science/hal-00507787v1/document&#34;&gt;stochastic Taylor expansions for the pricing of vanilla options with discrete (cash) dividends&lt;/a&gt;, I had the idea of a small improvement, by using &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;a more precise proxy&lt;/a&gt; for the Taylor expansion.&lt;/p&gt;&#xA;&lt;p&gt;More recently, I applied the idea to &lt;a href=&#34;https://arxiv.org/abs/2402.17684v1&#34;&gt;approximate arithmetic Asian options prices&lt;/a&gt; by using the geometric Asian option price as a proxy (with some adjustments). This worked surprisingly well, and is competitive with the best implementations of Curran approach to Asian options pricing. I quickly noticed that I could apply the same idea to &lt;a href=&#34;https://arxiv.org/abs/2402.17684&#34;&gt;approximate Basket option prices&lt;/a&gt;, and from it obtain another approximation for vanilla options with cash dividends through the &lt;a href=&#34;https://arxiv.org/abs/2106.12971&#34;&gt;mapping described by J. Healy&lt;/a&gt;. Interestingly the resulting approximation is the most accurate amongst all other approximations for cash dividends.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Calibrating Heston to Variance Swaps - a bad idea?</title>
      <link>https://chasethedevil.github.io/post/heston_variance_swap_calibration/</link>
      <pubDate>Tue, 11 Feb 2025 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/heston_variance_swap_calibration/</guid>
      <description>&lt;p&gt;An interesting idea to calibrate the Heston model in a more stable manner and reduce the calibration time is to make use of variance swap prices. Indeed, there is a simple formula for the theoretical price of a variance swap in the Heston model.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/heston_varswap_formula.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;It is not perfect since it approximates the variance swap price by the expectation of the integrated variance process over time. In particular it does not take into account eventual jumps (obviously), finiteness of replication, and discreteness of observations. But it may be good enough. Thanks to this formula, we can calibrate three parameters of the Heston model: the initial variance, the long-term mean variance, and the speed of mean reversion to the term-structure of variance swaps. We do not need market prices of variance swaps, we may simply use a replication based on market vanilla options prices, such as the model-free replication of Fukasawa.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monotonicity of the Black-Scholes Option Prices in Practice</title>
      <link>https://chasethedevil.github.io/post/vol_monotonicity_in_practice/</link>
      <pubDate>Sun, 29 Sep 2024 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/vol_monotonicity_in_practice/</guid>
      <description>&lt;p&gt;It is well known that vanilla option prices must increase when we increase the implied volatility. Recently, a post on the Wilmott forums wondered about the true accuracy of Peter Jaeckel implied volatility solver, whether it was truely IEEE 754 compliant. In fact, the author noticed some inaccuracy in the option price itself. Unfortunately I can not reply to the forum, its login process does not seem to be working anymore, and so I am left to blog about it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Copilot vs ChatGPT on the Optimal Finite Difference Step-Size</title>
      <link>https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/</link>
      <pubDate>Thu, 25 Jul 2024 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/</guid>
      <description>&lt;p&gt;When computing the derivative of a function by finite difference, which step size is optimal? The answer depends on the kind of difference (forward, backward or central), and the degree of the derivative (first or second typically for finance).&lt;/p&gt;&#xA;&lt;p&gt;For the first derivative, the result is very quick to find (it&amp;rsquo;s on &lt;a href=&#34;https://en.wikipedia.org/wiki/Numerical_differentiation&#34;&gt;wikipedia&lt;/a&gt;). For the second derivative, it&amp;rsquo;s more challenging. The &lt;a href=&#34;https://paulklein.ca/newsite/teaching/Notes_NumericalDifferentiation.pdf&#34;&gt;Lecture Notes&lt;/a&gt; of Karen Kopecky provide an answer. I wonder if Copilot or ChatGPT would find a good solution to the question:&lt;/p&gt;</description>
    </item>
    <item>
      <title>News on the COS Method Truncation</title>
      <link>https://chasethedevil.github.io/post/cos_method_truncation/</link>
      <pubDate>Mon, 13 May 2024 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/cos_method_truncation/</guid>
      <description>&lt;p&gt;The COS method is a fast way to price vanilla European options under stochastic volatility models with a known characteristic function. There are alternatives, explored in &lt;a href=&#34;https://chasethedevil.github.io/post/the-cos-method-for-heston/&#34;&gt;previous&lt;/a&gt; &lt;a href=&#34;https://chasethedevil.github.io/post/attari-lord-kahl--cos-methods-comparison-on-heston/&#34;&gt;blog&lt;/a&gt; &lt;a href=&#34;https://chasethedevil.github.io/post/making-classic-heston-integration-faster-than-the-cos-method/&#34;&gt;posts&lt;/a&gt;. A main advantage of the COS method is its simplicity. But this comes at the expense of finding the correct values for the truncation level and the (associated) number of terms.&lt;/p&gt;&#xA;&lt;p&gt;A related issue of the COS method, or its more fancy wavelet cousin the SWIFT method, is to require a huge (&amp;gt;65K) number of points to reach a reasonable accuracy for some somewhat extreme choices of Heston parameters. I provide an example in &lt;a href=&#34;https://arxiv.org/abs/2401.01758&#34;&gt;a recent paper&lt;/a&gt; (see Section 5).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variance Swap Term-Structure under Schobel-Zhu</title>
      <link>https://chasethedevil.github.io/post/unrealistic_variance_swaps_under_schobel_zhu/</link>
      <pubDate>Tue, 26 Mar 2024 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/unrealistic_variance_swaps_under_schobel_zhu/</guid>
      <description>&lt;p&gt;I never paid too much attention to it, but the term-structure of variance swaps is not always realistic under the Schobel-Zhu stochastic volatility model.&lt;/p&gt;&#xA;&lt;p&gt;This is not fundamentally the case with the Heston model, the Heston model is merely extremely limited to produce either a flat shape or a downward sloping exponential shape.&lt;/p&gt;&#xA;&lt;p&gt;Under the Schobel-Zhu model, the price of a newly issued variance swap reads&#xA;$$&#x9;V(T) = \left[\left(v_0-\theta\right)^2-\frac{\eta^2}{2\kappa}\right]\frac{1-e^{-2\kappa T}}{2\kappa T}+2\theta(v_0-\theta)\frac{1-e^{-\kappa T}}{\kappa T}+\theta^2+\frac{\eta^2}{2\kappa},,$$&#xA;where \( \eta \) is the vol of vol.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Basket Expansions and Cash Dividends</title>
      <link>https://chasethedevil.github.io/post/new_basket_approximation_and_cash_dividends/</link>
      <pubDate>Sat, 23 Mar 2024 09:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/new_basket_approximation_and_cash_dividends/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://chasethedevil.github.io/post/new_asian_approximation/&#34;&gt;previous post&lt;/a&gt;, I presented a &lt;a href=&#34;https://arxiv.org/abs/2106.12971&#34;&gt;new stochastic expansion&lt;/a&gt; for the prices of Asian options. The stochastic expansion is generalized to basket options in the paper, and &lt;a href=&#34;https://arxiv.org/abs/2106.12971&#34;&gt;can thus be applied&lt;/a&gt; on the problem of pricing vanilla options with cash dividends.&lt;/p&gt;&#xA;&lt;p&gt;I have updated the paper with comparisons to more direct stochastic expansions for pricing vanilla options with cash dividends, such as the one of &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1687590&#34;&gt;Etoré and Gobet&lt;/a&gt;, and &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;my own refinement&lt;/a&gt; on it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>New Approximations for the Prices of Asian and basket Options</title>
      <link>https://chasethedevil.github.io/post/new_asian_approximation/</link>
      <pubDate>Sun, 17 Mar 2024 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/new_asian_approximation/</guid>
      <description>&lt;p&gt;Many years ago, I had &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;applied the stochastic expansion&lt;/a&gt; technique of &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1687590&#34;&gt;Etore and Gobet&lt;/a&gt; to a refined proxy, in order to produce more accurate prices for vanilla options with cash dividends under the Black-Scholes model with deterministic jumps at the dividend dates. Any approximation for vanilla basket option prices can also be applied on this problem, and the sophisticated Curran geometric conditioning was found to be particularly competitive in &lt;a href=&#34;https://arxiv.org/abs/2106.12971&#34;&gt;The Pricing of Vanilla Options with Cash Dividends as a Classic Vanilla Basket Option Problem&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Easy Mistake With the Log-Euler Discretization On Black-Scholes</title>
      <link>https://chasethedevil.github.io/post/logeuler_not_exact/</link>
      <pubDate>Mon, 11 Mar 2024 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/logeuler_not_exact/</guid>
      <description>&lt;p&gt;In the Black-Scholes model with a term-structure of volatilities, the Log-Euler Monte-Carlo scheme is not necessarily exact.&lt;/p&gt;&#xA;&lt;p&gt;This happens if you have two assets \(S_1\) and \(S_2\), with two different time varying volatilities \(\sigma_1(t), \sigma_2(t) \). The covariance from the Ito isometry from \(t=t_0\) to \(t=t_1\) reads $$ \int_{t_0}^{t_1} \sigma_1(s)\sigma_2(s) \rho ds, $$ while a naive log-Euler discretization may use&#xA;$$ \rho  \bar\sigma_1(t_0) \bar\sigma_2(t_0)  (t_1-t_0). $$&#xA;In practice, the \( \bar\sigma_i(t_0) \) are calibrated such that the vanilla option prices are exact, meaning&#xA;$$ \bar{\sigma}_i^2(t_0)(t_1-t_0) = \int_{t_0}^{t_1} \sigma_i^2(s) ds.$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Roughness of Pure Jumps</title>
      <link>https://chasethedevil.github.io/post/roughness_of_jumps/</link>
      <pubDate>Mon, 18 Dec 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/roughness_of_jumps/</guid>
      <description>&lt;p&gt;In my previous &lt;a href=&#34;https://chasethedevil.github.io/post/roughness_of_stochastic_volatility/&#34;&gt;blog post&lt;/a&gt;, I looked at the roughness of the SVCJ stochastic volatility model with jumps (in the volatility). In this model, the jumps occur randomly, but at discrete times. And with typical parameters used in the litterature, the jumps are not so frequent. It is thus more interesting to look at the roughness of pure jump processes, such as the &lt;a href=&#34;https://engineering.nyu.edu/sites/default/files/2018-09/CarrJournalofBusiness2002.pdf&#34;&gt;CGMY process&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The CGMY process is more challenging to simulate. I used the approach based on the characteristic function described in &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1983134&#34;&gt;Simulating Levy Processes from Their Characteristic Functions and Financial Applications&lt;/a&gt;. Ballota and Kyriakou add some variations based on FFT pricing of the characteristic function in &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1951537&#34;&gt;Monte Carlo simulation of the CGMY process and option pricing&lt;/a&gt; and pay much care about a proper truncation range. Indeed, I found that the truncation range was key to simulate the process properly and not always trivial to set up especially for \(Y \in (0,1) \). I however did not  implement any automated range guess as I am merely interested in very specific use cases, and I used the COS method instead of FFT.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Roughness of Stochastic Volatility with Jumps</title>
      <link>https://chasethedevil.github.io/post/roughness_of_stochastic_volatility/</link>
      <pubDate>Thu, 07 Dec 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/roughness_of_stochastic_volatility/</guid>
      <description>&lt;p&gt;I was wondering if adding jumps to stochastic volatility, as is done in the SVCJ model of Duffie, Singleton and Pan &lt;em&gt;&amp;ldquo;Transform Analysis and Asset Pricing for Affine Jump-Diffusion&amp;rdquo;&lt;/em&gt; also in Broadie and Kaya &lt;em&gt;&amp;ldquo;Exact simulation of stochastic volatility and other affine jump diffusion processes&amp;rdquo;&lt;/em&gt;, would lead to rougher paths, or if it would mislead the roughness estimators.&lt;/p&gt;&#xA;&lt;p&gt;The answer to the first question can almost be answered visually:&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/svcj_variance_path.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Measuring Roughness with Julia</title>
      <link>https://chasethedevil.github.io/post/measuring_roughness_with_julia/</link>
      <pubDate>Tue, 07 Nov 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/measuring_roughness_with_julia/</guid>
      <description>&lt;p&gt;I received a few e-mails asking me for the code I used to measure roughness in my preprint on the &lt;a href=&#34;https://chasethedevil.github.io/post/implied_volatility_roughness&#34;&gt;roughness of the implied volatility&lt;/a&gt;. Unfortunately, the code I wrote for this paper is not in a &lt;em&gt;good&lt;/em&gt; state, it&amp;rsquo;s all in one long file line by line, not necessarily in order of execution, with comments that are only meaningful to myself.&lt;/p&gt;&#xA;&lt;p&gt;In this post I will present the code relevant to measuring the oxford man institute roughness with Julia. I won&amp;rsquo;t go into generating Heston or rough volatility model implied volatilities, and focus only on the measure on roughness based on some CSV like input. I downloaded the oxfordmanrealizedvolatilityindices.csv from the Oxford Man Institute website (unfortunately now discontinued, data bought by Refinitiv but still available in some github repos) to my home directory&lt;/p&gt;</description>
    </item>
    <item>
      <title>Black with Bachelier</title>
      <link>https://chasethedevil.github.io/post/black_with_bachelier/</link>
      <pubDate>Tue, 03 Oct 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/black_with_bachelier/</guid>
      <description>&lt;p&gt;I was experimenting with the recent &lt;a href=&#34;https://www.researchgate.net/publication/348192007_SABR_for_Baskets&#34;&gt;SABR basket approximation&lt;/a&gt; of Hagan. The approximation only works&#xA;for the normal SABR model, meaning beta=0 in SABR combined with the Bachelier option formula.&lt;/p&gt;&#xA;&lt;p&gt;I was wondering how good the approximation would be for two flat smiles (in terms of Black volatilities). I then noticed something that escaped me before: the normal SABR model is able to fit the pure Black model (with constant vols) extremely well. A calibration near the money stays valid very out-of-the-money and the error in Black volatilities is very small.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Clenshaw-Curtis Quadrature Implementation by FFT in Practice</title>
      <link>https://chasethedevil.github.io/post/clenshaw_fft_implementation/</link>
      <pubDate>Wed, 27 Sep 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/clenshaw_fft_implementation/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Clenshaw%E2%80%93Curtis_quadrature&#34;&gt;Clenshaw-Curtis quadrature&lt;/a&gt; is known to be competitive with Gauss quadratures. It has several advantages:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the weights are easy and fast to compute.&lt;/li&gt;&#xA;&lt;li&gt;adaptive / doubling quadratures are possible with when the Chebyshev polynomial of the second kind is used for the quadrature.&lt;/li&gt;&#xA;&lt;li&gt;the Chebyshev nodes may also be used to interpolate some costly function.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The wikipedia article has a relatively detailed description on how to compute the quadrature weights corresponding to the Chebyshev polynomial of the second kind (where the points -1 and 1 are included), via a type-I DCT. It does not describe the weights corresponding to the Chebyshev polynomials of the first kind (where -1 and 1 are excluded, like the Gauss quadratures). &lt;a href=&#34;https://numbersandshapes.net/posts/high_precision_clenshaw_curtis/&#34;&gt;The numbersandshapes blog post&lt;/a&gt; describes it very nicely. There are some publications around computation of Clenshaw-Curtis or Fejer rules, a recent one is &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S089812211200689X&#34;&gt;Fast construction of Fejér and Clenshaw–Curtis rules for general weight functions&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ghost Vacations</title>
      <link>https://chasethedevil.github.io/post/ghost_vacations/</link>
      <pubDate>Sun, 20 Aug 2023 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/ghost_vacations/</guid>
      <description>&lt;p&gt;During my vacation, I don&amp;rsquo;t know why, but I looked at some stability issue with ghost points and the explicit method. I was initially trying out ghost points with the explicit runge kutta Chebyshev/Legendre/Gegenbauer technique and noticed some explosion in some cases.&lt;/p&gt;&#xA;&lt;p&gt;I cornered it down to a stability issue of the standard explicit Euler method with ghost (or fictitious) points. The technique is described in the book &amp;ldquo;Paul Wilmott on Quantitative Finance&amp;rdquo; (also in Paul Wilmott introduces quantitative finance), which I find quite good, although I have some friends who are not much fond of it. The technique may be used to compute the price of a continuously monitored barrier option when the barrier does not fall on the grid, or more generally for time-dependent barriers. I however look at it in the simple context of a constant barrier in time.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maximum Implied Variance Slope</title>
      <link>https://chasethedevil.github.io/post/maximum_implied_variance_slope/</link>
      <pubDate>Mon, 22 May 2023 23:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/maximum_implied_variance_slope/</guid>
      <description>&lt;p&gt;The paper &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=2ahUKEwim__mNp4v_AhUPI0QIHfICCjEQFnoECBEQAQ&amp;amp;url=http%3A%2F%2Fmath.uchicago.edu%2F~rogerlee%2Fmoment.pdf&amp;amp;usg=AOvVaw36Aps9YuK7f3sovbO-KAOW&#34;&gt;The Moment Formula for Implied Volatility at Extreme Strikes&lt;/a&gt; by Roger Lee redefined how practioners extrapolate the implied volatility, by showing that the total implied variance can be at most linear in the wings, with a slope below 2.&lt;/p&gt;&#xA;&lt;p&gt;Shortly after, the SVI model of Jim Gatheral, with its linear wings, started to become popular.&lt;/p&gt;&#xA;&lt;p&gt;In a recent paper in collaboration with Winfried Koller, we show that the asymptotic bounds are usually overly optimistic. This is somewhat expected, as the bound only holds as the log-moneyness goes to infinity. What is less expected, is that, even at very large strikes (nearly up to the limit of what SVI allow numerically), we may happen to have arbitrages, even though the bounds are respected.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Return of the Arbitrage in the Perfect Volatility Surface</title>
      <link>https://chasethedevil.github.io/post/the_return_of_the_arbitrage/</link>
      <pubDate>Wed, 29 Mar 2023 23:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/the_return_of_the_arbitrage/</guid>
      <description>&lt;p&gt;In a Wilmott article from 2018 (Wilmott magazine no. 97) titled &amp;ldquo;Arbitrage in the perfect volatility surface&amp;rdquo;, Uwe Wystup points out some interesting issues on seemingly innocuous FX volatility surfaces:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a cubic spline tends to produce artificial peaks/modes in the density.&lt;/li&gt;&#xA;&lt;li&gt;SVI not arbitrage-free even on seemingly trivial input.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The examples provided are indeed great and the remarks very valid. There is more to it however:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;a cubic spline on strikes or log-moneyness does not produce the artificial peak.&lt;/li&gt;&#xA;&lt;li&gt;SVI with a=0 is arbitrage-free on this example.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;For the first point, the denominator of the Dupire formula in terms of the representation of total variance as a function of logmoneyness gives some clues as it constitues a vega scaled version of the probability density&#xA;with direct link to the total variance and its derivatives. In particular it is a  simple function of its value, first and second derivatives, without involving any non-linear function and the second derivative only appears as a linear term. As such a low order polynomial representation of the variance in log-moneyness may be adequate.&#xA;In contrast, the delta based representation introduces a strong non-linearity with the cumulative normal distribution function.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/audnzd_delta_spline_dens.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Splines on AUD/NZD 1w options.&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Princeton Fintech and Quant conference of December 2022</title>
      <link>https://chasethedevil.github.io/post/princeton_fintech_conference/</link>
      <pubDate>Sun, 04 Dec 2022 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/princeton_fintech_conference/</guid>
      <description>&lt;p&gt;I recently presented my latest published paper &lt;a href=&#34;https://wilmott.com/wilmott-magazine-november-2022-issue/&#34;&gt;On the Bachelier implied volatility at extreme strikes&lt;/a&gt; at the Princeton Fintech and Quant conference.&#xA;The presenters were of quite various backgrounds. The first presentations were much more business oriented with lots of AI keywords, but relatively little technical content while the last presentation was&#xA;about parallel programming. Many were a pitch to recruit to employees.&lt;/p&gt;&#xA;&lt;p&gt;The diversity was interesting: it was refreshing to hear about quantitative finance from vastly different perspectives. The presentation from Morgan Stanley about &lt;a href=&#34;https://github.com/morganstanley/optimus-cirrus&#34;&gt;their scala annotation framework&lt;/a&gt; to ease up&#xA;parallel programming was enlightening. The main issue they were trying to solve is the necessity for all the boilerplate code to handle concurrency, caching, robustness, which obfuscates significantly the business logic in the code.&#xA;This is an old problem. Decades ago, rule engines were the trend for similar reasons. Using the example of pricing bonds, the presenters put very well forward the issues in evidence, issues that I found&#xA;very relevant.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Roughness of the Implied Volatility</title>
      <link>https://chasethedevil.github.io/post/implied_volatility_roughness/</link>
      <pubDate>Sat, 09 Jul 2022 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/implied_volatility_roughness/</guid>
      <description>&lt;p&gt;This is a follow up of my &lt;a href=&#34;https://chasethedevil.github.io/post/rough-volatility-or-not-a-review/&#34;&gt;previous post&lt;/a&gt; on rough volatility. I recently tried to reproduce the results of the paper &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4065951&#34;&gt;Rough Volatility: Fact or Artefact?&lt;/a&gt; as I was curious to apply the technique using different inputs. The 5-minutes SPX realized volatility is freely available in CSV format at the &lt;a href=&#34;https://realized.oxford-man.ox.ac.uk/data&#34;&gt;&#xA;Oxford-Man Institute of Quantitative Finance&lt;/a&gt; and it is thus relatively straightforward to reproduce the numbers presented in the paper.&lt;/p&gt;&#xA;&lt;p&gt;Using a sampling of K=75 and L=75*75, I obtain an rounghness index H=0.181. The paper uses K=70 and L = 70x70, and their Figure 19 of the paper states 0.187.&#xA;It turns out that there are more than L observations in the time-series, and, with K=70, I end up with a roughness index H=0.222 when I start from the first observation (year 2000), up to the observation L+1. But it is possible to slide this window and compute the roughness index at each starting point. The results are enlightening.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/roughness_spx_K70_oxford.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;SPX historical vol roughness index sliding estimate with K=70.&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/roughness_density_spx_K70_oxford.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;density of SPX historical vol roughness index estimate with K=70. The mean is 0.158 with a standard deviation of 0.034&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Volatility: Rough or Not? A Short Review</title>
      <link>https://chasethedevil.github.io/post/rough-volatility-or-not-a-review/</link>
      <pubDate>Tue, 10 May 2022 17:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/rough-volatility-or-not-a-review/</guid>
      <description>&lt;p&gt;It is well-known that the assumption of constant volatility in the Black-Scholes model for pricing financial contracts is wrong and may lead&#xA;to serious mispricing, especially for any exotic derivative contracts.&#xA;A classic approach is to use a deterministic local volatility model to take into account the variation both in the time dimension and in the underlying asset price dimension. But&#xA;the model is still not realistic in terms of forward smile (the implied volatilities of forward starting options). A stochastic volatility component must be added to correct for it.&#xA;More recently, the concept of rough volatility emerged in many academic papers. Instead of using a classic Brownian motion for the stochastic volatility process,&#xA;a fractional Brownian motion is used. The idea of using a fractional Brownian motion for financial time-series can be traced back to Mandelbrot, but it is only relatively recently that it has been proposed for the volatility process (and not the stock process).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monte-Carlo Parallelization: to vectorize or not?</title>
      <link>https://chasethedevil.github.io/post/monte-carlo-vectorization-or-not/</link>
      <pubDate>Sat, 09 Apr 2022 21:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/monte-carlo-vectorization-or-not/</guid>
      <description>&lt;p&gt;When writing a Monte-Carlo simulation to price financial derivative contracts, the most straightforward is to code a loop over the number of paths, in which each path is fully calculated. Inside the loop, a payoff function takes this path to compute the present value of the contract on the given path. The present values are recorded to lead to the Monte-Carlo statistics (mean, standard deviation).&#xA;I ignore here any eventual callability of the payoff which may still be addressed with some work-arounds in this setup. The idea can be schematized by the following go code:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#007020;font-weight:bold&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#666&#34;&gt;:=&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;0&lt;/span&gt;; i &amp;lt; numSimulations; i&lt;span style=&#34;color:#666&#34;&gt;++&lt;/span&gt; {&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;pathGenerator.&lt;span style=&#34;color:#06287e&#34;&gt;ComputeNextPath&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;path) &lt;span style=&#34;color:#60a0b0;font-style:italic&#34;&gt;//path contains an array of n time-steps of float64.&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#60a0b0;font-style:italic&#34;&gt;&lt;/span&gt;&#x9;pathEvaluator.&lt;span style=&#34;color:#06287e&#34;&gt;Evaluate&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;path, output)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&#x9;statistics.&lt;span style=&#34;color:#06287e&#34;&gt;RecordValue&lt;/span&gt;(output)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&#xA;A python programmer would likely not write a simulation this way, as the python code inside the large loop will not be fast. Instead, the python programmer will write a vectorized simulation, generating all paths at the same time.&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-go&#34; data-lang=&#34;go&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;pathGenerator.&lt;span style=&#34;color:#06287e&#34;&gt;ComputeAllPaths&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;paths) &lt;span style=&#34;color:#60a0b0;font-style:italic&#34;&gt;//paths contains an array of n time-steps of vectors of size numSimulations &#xA;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#60a0b0;font-style:italic&#34;&gt;&lt;/span&gt;pathEvaluator.&lt;span style=&#34;color:#06287e&#34;&gt;EvaluateAll&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;&amp;amp;&lt;/span&gt;paths, output)&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;statistics.&lt;span style=&#34;color:#06287e&#34;&gt;RecordValues&lt;/span&gt;(output)&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>More Automatic Differentiation Awkwardness</title>
      <link>https://chasethedevil.github.io/post/more-automatic-differentiation-awkwardness/</link>
      <pubDate>Tue, 04 Jan 2022 21:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/more-automatic-differentiation-awkwardness/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://quantsrus.github.io/post/exp_b_spline_collocation_autodiff/&#34;&gt;This blog post&lt;/a&gt; from Jherek Healy presents some not so obvious behavior of automatic differentiation, when a function is decomposed&#xA;into the product of two parts where one part goes to infinity and the other to zero, and we know the overall result must go to zero (or to some other specific number).&#xA;This decomposition may be relatively simple to handle for the value of the function, but becomes far less trivial to think of in advance, at the derivative level.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quadprog in Julia</title>
      <link>https://chasethedevil.github.io/post/quadprog-in-julia/</link>
      <pubDate>Sun, 21 Nov 2021 13:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/quadprog-in-julia/</guid>
      <description>&lt;p&gt;As described &lt;a href=&#34;https://en.wikipedia.org/wiki/Quadratic_programming&#34;&gt;on wikipedia&lt;/a&gt;, a quadratic programming problem with &lt;em&gt;n&lt;/em&gt; variables and &lt;em&gt;m&lt;/em&gt; constraints is of the  form&#xA;$$ \min(-d^T x + 1/2 x^T D x) $$ with the&#xA;constraints \( A^T x \geq b_0 \), were \(D\) is a \(n \times n\)-dimensional real symmetric matrix, \(A\) is a \(n \times m\)-dimensional real matrix, \( b_0 \) is a \(m\)-dimensional vector of constraints, \( d \) is a \(n\)-dimensional vector, and the variable \(x\) is a \(n\)-dimensional vector.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Positive Stochastic Collocation</title>
      <link>https://chasethedevil.github.io/post/positive_stochastic_collocation/</link>
      <pubDate>Tue, 31 Aug 2021 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/positive_stochastic_collocation/</guid>
      <description>&lt;p&gt;In the context of my thesis, I &lt;a href=&#34;https://arxiv.org/abs/2109.02405&#34;&gt;explored&lt;/a&gt; the use of stochastic collocation to capture the marginal densities of a positive asset.&#xA;Indeed, most financial asset prices must be non-negative. But the classic stochastic collocation towards the normally distributed random variable, is not.&lt;/p&gt;&#xA;&lt;p&gt;A simple tweak, proposed early on by Grzelak, is to assume absorption and use the put-call parity to price put options (which otherwise depend on the left tail).&#xA;This sort of works most of the time, but a priori, there is no guarantee that we will end up with a positive put option price.&#xA;As an extreme example, we may consider the case where the collocation price formula leads to  \(V_{\textsf{call}}(K=0) &amp;lt; f \) where  \(f \) is the forward price to maturity.&#xA;The put-call parity relation applied at  \(K=0 \) leads to  \(V_{\textsf{put}}(K=0) = V_{\textsf{call}}(K=0)-f &amp;lt; 0 \). This means that for some strictly positive strike, the put option price will be negative, which is non-sensical.&#xA;In reality, it thus implies that absorption must happen earlier, not at  \(S=0 \), but at some strictly positive asset price. And then it is not so obvious to chose the right value in advance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Remarkable Coincidences, Bad Book?</title>
      <link>https://chasethedevil.github.io/post/reghai_remarkable_coincidences/</link>
      <pubDate>Sat, 21 Nov 2020 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/reghai_remarkable_coincidences/</guid>
      <description>&lt;p&gt;I stumbled upon a new  short book &lt;a href=&#34;https://link.springer.com/book/10.1007%2F978-3-030-57496-3&#34;&gt;Financial Models in Production&lt;/a&gt; from O. Kettani and A. Reghai. A page attracted my attention&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/reghai_coincidences.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;A page from Kettani and Reghai&amp;#39;s book.&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;This is the same example as I used on &lt;a href=&#34;https://chasethedevil.github.io/post/implied-volatility-from-black-scholes-price/&#34;&gt;my blog&lt;/a&gt;, where I also present the Li&amp;rsquo;s SOR method combined with the good initial guess from Stefanica. The idea has also been expanded on in &lt;a href=&#34;https://jherekhealy.github.io&#34;&gt;Jherek Healy&amp;rsquo;s book&lt;/a&gt;. What is shocking is that, beside reusing my example, &lt;strong&gt;they reuse my timing&lt;/strong&gt; for Jäckel and my implementation is in &lt;a href=&#34;https://golang.org&#34;&gt;Google Go&lt;/a&gt;, with a timing done on some older laptop. The numbers given are thus highly inconsistent. Of course, none of this is mentioned anywhere, and the book does not reference my blog.&lt;/p&gt;</description>
    </item>
    <item>
      <title>More on random number generators</title>
      <link>https://chasethedevil.github.io/post/more-on-random-number-generators/</link>
      <pubDate>Sat, 10 Oct 2020 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/more-on-random-number-generators/</guid>
      <description>&lt;p&gt;My &lt;a href=&#34;https://chasethedevil.github.io/post/war-of-the-random-number-generators/&#34;&gt;previous post&lt;/a&gt; described the recent view on random number generators, with a focus on the Mersenne-Twister war.&lt;/p&gt;&#xA;&lt;p&gt;Since, I have noticed another front in the war of the random number generators:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1806.05243&#34;&gt;An example in dimension 121 from K Savvidy&lt;/a&gt; where L&amp;rsquo;Ecuyer MRG32k3a fails to compute the correct result, regardless of the seed. This is a manufactured example, such that the vector, used in the example, falls in the dual lattice of the generator. Similar examples can be constructed for other variants.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://pubsonline.informs.org/doi/abs/10.1287/ijoc.2018.0878&#34;&gt;Spectral Analysis of the MIXMAX Random Number Generator&lt;/a&gt; (by L&amp;rsquo;Ecuyer et al.) shows defects in MIXMAX, for some parameters that were advised in earlier papers of K. Savvidy. MIXMAX is a RNG popularized by K. Savvidy, used &lt;a href=&#34;https://cdcvs.fnal.gov/redmine/projects/g4/wiki/RNDM-Geant4104&#34;&gt;in CLHEP at Fermilab&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Also, I found interesting that Monte-Carlo simulations run at the Los Alamos National Laboratory relied on a relatively simple linear congruential generator (LCG) producing 24- or 48-bits integers &lt;a href=&#34;https://mcnp.lanl.gov/pdf_files/la-ur-11-04859.pdf&#34;&gt;for at least 40 years&lt;/a&gt;. LCGs are today touted as some of the worst random number generators, exhibiting strong patterns in 2D projections. Also the period chosen was very small by today&amp;rsquo;s standards: 7E13.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The war of the random number generators</title>
      <link>https://chasethedevil.github.io/post/war-of-the-random-number-generators/</link>
      <pubDate>Thu, 17 Sep 2020 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/war-of-the-random-number-generators/</guid>
      <description>&lt;p&gt;These days, there seems to be some sort of small war to define what is a modern good random number generators to advise for simulations.&#xA;Historically, the Mersenne-Twister (MT thereafter) won this war. It is used by default in many scientific libraries and software, even if there has been a few issues with it:&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;A bad initial seed may make it generate a sequence of low quality for at least as many as 700K numbers.&lt;/li&gt;&#xA;&lt;li&gt;It is slow to jump-ahead, making parallelization not so practical.&lt;/li&gt;&#xA;&lt;li&gt;It fails some TestU01 Bigcrush tests, mostly related to the F2 linear algebra, the algebra of the Mersenne-Twister.&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;It turns out, that before MT (1997), a lot of the alternatives were much worse, except, perhaps, &lt;a href=&#34;https://arxiv.org/abs/hep-lat/9309020&#34;&gt;RANLUX&lt;/a&gt; (1993), which is quite slow due to the need of skipping many points of the generated sequence.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sobol with 64-bits integers</title>
      <link>https://chasethedevil.github.io/post/sobol-64-bits/</link>
      <pubDate>Wed, 09 Sep 2020 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/sobol-64-bits/</guid>
      <description>&lt;p&gt;A while ago, I wondered how to make some implementation of Sobol support 64-bits integers (long) and double floating points. &lt;a href=&#34;https://en.wikipedia.org/wiki/Sobol_sequence&#34;&gt;Sobol&lt;/a&gt; is the most used&#xA;quasi random number generator (QRNG) for (quasi) Monte-Carlo simulations.&lt;/p&gt;&#xA;&lt;p&gt;The standard Sobol algorithms are all coded with 32-bits integers and lead to double floating point numbers which can not be smaller than&#xA;\( 2^{-31} \). I was recently looking back at the internals at Sobol generators, and noticed that generating with 64-bits integers would not help much.&lt;/p&gt;</description>
    </item>
    <item>
      <title>March 9, 2020 crash - where will CAC40 go?</title>
      <link>https://chasethedevil.github.io/post/mar9-cac40-crash/</link>
      <pubDate>Mon, 09 Mar 2020 21:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/mar9-cac40-crash/</guid>
      <description>&lt;p&gt;The stock market crashed by more than 7% on March 9, 2020. It is one of the most important drop since September 2001.&#xA;I looked at BNP warrant prices on the CAC40 French index, with a maturity of March 20, 2020 , to see what they would tell about the market direction on the day of the crash. This is really a not-so-scientific experiment.&lt;/p&gt;&#xA;&lt;p&gt;The quotes I got were quite noisy. I applied a few different techniques to imply the probability density from the option prices:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Numba, Pypy Overrated?</title>
      <link>https://chasethedevil.github.io/post/python-numba-overrated/</link>
      <pubDate>Tue, 12 Feb 2019 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/python-numba-overrated/</guid>
      <description>&lt;p&gt;Many benchmarks show impressive performance gains with the use&#xA;of &lt;a href=&#34;https://numba.pydata.org/&#34;&gt;Numba&lt;/a&gt; or &lt;a href=&#34;https://www.pypy.org/&#34;&gt;Pypy&lt;/a&gt;. Numba allows to compile just-in-time some specific methods, while Pypy takes&#xA;the approach of compiling/optimizing the full python program: you use it just like the standard&#xA;python runtime. From those benchmarks, I imagined that those  tools would improve my 2D Heston PDE solver&#xA;performance easily. The initialization part of my program contains embedded for loops over several 10Ks elements.&#xA;To my surprise, numba did not improve anything (and I had to isolate the code, as it would&#xA;not work on 2D numpy arrays manipulations that are vectorized). I surmise it does not play well&#xA;with scipy sparse matrices.&#xA;Pypy did not behave better, the solver became actually slower than with the standard python&#xA;interpreter, up to twice as slow, for example, in the case of the main solver loop which only does matrix multiplications and LU solves sparse systems. I did not necessarily expect any performance improvement in this specific loop, since it only consists in a few calls to expensive scipy calculations. But I did not expect a 2x performance drop either.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fixing NaNs in Quadprog</title>
      <link>https://chasethedevil.github.io/post/quadprog-nans/</link>
      <pubDate>Sun, 07 Oct 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/quadprog-nans/</guid>
      <description>&lt;p&gt;Out of curiosity, I tried &lt;a href=&#34;https://github.com/cran/quadprog&#34;&gt;quadprog&lt;/a&gt; as &lt;a href=&#34;https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/&#34;&gt;open-source quadratic programming convex optimizer&lt;/a&gt;, as it is looks fast, and the code stays relatively simple. I however stumbled on cases where the algorithm would return NaNs even though my inputs seemed straighforward. Other libraries such as CVXOPT did not have any issues with those inputs.&lt;/p&gt;&#xA;&lt;p&gt;Searching on the web, I found that I was not the only one to stumble on this kind of issue with quadprog. In particular, in 2014, Benjamen Tyner &lt;a href=&#34;http://r.789695.n4.nabble.com/quadprog-solve-QP-sometimes-returns-NaNs-td4697548.html&#34;&gt;gave a simple example in R&lt;/a&gt;, where solve.QP returns NaNs while the input is very simple: an identity matrix with small perturbations out of the diagonal. Here is a copy of his example:&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Probability of a Netflix Stock Crash</title>
      <link>https://chasethedevil.github.io/post/nflx-stock-crash-probability/</link>
      <pubDate>Thu, 12 Jul 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/nflx-stock-crash-probability/</guid>
      <description>&lt;p&gt;This is a follow up to my &lt;a href=&#34;https://chasethedevil.github.io/post/tsla-stock-crash-probability&#34;&gt;previous post&lt;/a&gt; where I explore the probability&#xA;of a TSLA stock crash, reproducing the &lt;a href=&#34;https://www.linkedin.com/pulse/options-market-thinks-16-chance-tesla-exist-january-2020-klassen/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BaOfn2Xf6RIum6%2F9ddKS9fA%3D%3D&#34;&gt;results of Timothy Klassen&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;According to the implied cumulative probability density, TSLA has around 15% chance of crashing below $100. Is this really&#xA;high compared to other stocks? or is it the interpretation of the data erroneous?&lt;/p&gt;&#xA;&lt;p&gt;Here I take a look at NFLX (Netflix). Below is the implied volatility according to three different models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>On the Probability of a TSLA Stock Crash</title>
      <link>https://chasethedevil.github.io/post/tsla-stock-crash-probability/</link>
      <pubDate>Wed, 11 Jul 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/tsla-stock-crash-probability/</guid>
      <description>&lt;p&gt;Timothy Klassen had an &lt;a href=&#34;https://www.linkedin.com/pulse/options-market-thinks-16-chance-tesla-exist-january-2020-klassen/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BaOfn2Xf6RIum6%2F9ddKS9fA%3D%3D&#34;&gt;interesting post&lt;/a&gt; on linkedin recently, with the title &amp;ldquo;the options market thinks there is a 16% chance that Tesla will not exist in January 2020&amp;rdquo;.&#xA;As I was also recently looking at the TSLA options, I was a bit intrigued. I looked at the option chain on July 10th,&#xA;and implied the European volatility from the American option prices. I then fit a few of my favorite models: Andreasen-Huge with Tikhonov regularization, the lognormal mixture, and a polynomial collocation of degree 7.&#xA;This results in the following graph&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Fourth Moment of the Normal SABR Model</title>
      <link>https://chasethedevil.github.io/post/normal-sabr-fourth-moment/</link>
      <pubDate>Mon, 11 Jun 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/normal-sabr-fourth-moment/</guid>
      <description>&lt;p&gt;I was wondering if I could use the SABR moments to calibrate a model to SABR parameters directly. It turns out that the SABR moments have relatively&#xA;simple expressions when \(\beta=0\), that is, for the normal SABR model (with no absorption). This is for the pure SABR stochatic volatility model, not the Hagan approximation.&#xA;For the Hagan approximation, we would need to use the replication by vanilla options to compute the moments.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implying the Probability Density from Market Option Prices (Part 2)</title>
      <link>https://chasethedevil.github.io/post/implying-the-probability-density-from-market-option-prices-ii/</link>
      <pubDate>Sun, 27 May 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/implying-the-probability-density-from-market-option-prices-ii/</guid>
      <description>&lt;p&gt;This is a follow-up to my posts on the implied risk-neutral density (RND) of the SPW options before and after the big volatility change that happened in early February with two different techniques:&#xA;&lt;a href=&#34;https://chasethedevil.github.io/post/spx500_bets_after_rates_hike/&#34;&gt;a smoothing spline on the implied volatilities&lt;/a&gt; and a &lt;a href=&#34;https://chasethedevil.github.io/post/implying-the-probability-density-from-market-option-prices/&#34;&gt;Gaussian kernel approach&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The Gaussian kernel (as well as to some extent the smoothing spline) let us believe that there are multiple modes in the distribution (multiple peaks in the density). In reality,&#xA;Gaussian kernel approaches will, by construction, tend to exhibit such modes. It is not so obvious to know if those are real or artificial. There are other ways to apply the Gaussian kernel,&#xA;for example by optimizing the nodes locations and the standard deviation of each Gaussian. The resulting density with those is very similar looking.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implying the Probability Density from Market Option Prices</title>
      <link>https://chasethedevil.github.io/post/implying-the-probability-density-from-market-option-prices/</link>
      <pubDate>Tue, 13 Feb 2018 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/implying-the-probability-density-from-market-option-prices/</guid>
      <description>&lt;p&gt;In the &lt;a href=&#34;https://chasethedevil.github.io/post/spx500_bets_after_rates_hike/&#34;&gt;previous post&lt;/a&gt;, I showed a plot of the probability implied from SPW options before and after the big volatility change of last week.&#xA;I created it from a least squares spline fit of the market mid implied volatilities (weighted by the inverse of the bid-ask spread). While it looks reasonable, the underlying&#xA;technique is not very robust. It is particularly sensitive to the number of options strikes used as spline nodes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Where is the S&amp;P 500 going to end?</title>
      <link>https://chasethedevil.github.io/post/spx500_bets_after_rates_hike/</link>
      <pubDate>Tue, 06 Feb 2018 19:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/spx500_bets_after_rates_hike/</guid>
      <description>&lt;p&gt;Yesterday the American stocks went a bit crazy along with the VIX that jumped from 17.50 to 38. It&amp;rsquo;s not exactly clear why, the news mention that the Fed might raise its interest rates, the bonds yield have been recently increasing substantially, and the market self-correcting&#xA;after stocks grew steadily for months in a low VIX environment.&lt;/p&gt;&#xA;&lt;p&gt;I don&amp;rsquo;t exactly follow the SPX/SPW options daily. But I had taken a snapshot two weeks ago when the market was quiet. We can imply the probability density from the market option prices.&#xA;It&amp;rsquo;s not an exact science. Here I do this with a least-squares spline on the implied volatilities (the least squares smoothes out the noise). I will show another approach in a subsequent post.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Discrete Sine Transform via the FFT</title>
      <link>https://chasethedevil.github.io/post/discrete_sine_transform_fft/</link>
      <pubDate>Mon, 05 Feb 2018 13:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/discrete_sine_transform_fft/</guid>
      <description>&lt;p&gt;Several months ago, I had a quick look at &lt;a href=&#34;https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2585529&#34;&gt;a recent paper&lt;/a&gt; describing how to use&#xA;Wavelets to price options under stochastic volatility models with a known characteristic function.&#xA;The more classic method is to use some numerical quadrature directly on the Fourier integral as described &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2362968&#34;&gt;in this paper&lt;/a&gt; for example.&#xA;When I read the paper, I was skeptical about the Wavelet approach, since it looked complicated, and with many additional parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quantitative Finance Books Citing My Papers</title>
      <link>https://chasethedevil.github.io/post/quantitative_finance_books/</link>
      <pubDate>Sat, 09 Dec 2017 13:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/quantitative_finance_books/</guid>
      <description>&lt;p&gt;I would have never really expected that when I started writing papers, but little by little there is a growing list of books citing &lt;a href=&#34;https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1514784&#34;&gt;my papers&lt;/a&gt;:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Applied-Quantitative-Finance-Equity-Derivatives/dp/1977557872/ref=sr_1_1?ie=UTF8&amp;amp;qid=1512751819&amp;amp;sr=8-1&amp;amp;keywords=jherek+healy&#34;&gt;Applied Quantitative Finance for Equity Derivatives&lt;/a&gt; by &lt;a href=&#34;https://jherekhealy.github.io&#34;&gt;Jherek Healy&lt;/a&gt;: the most recent book on equity derivatives refers to several of my papers. In contrast with many other books, the author goes beyond and provides additional insights on the papers.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Interest-Rate-Derivatives-Explained-Engineering/dp/1137360186/ref=sr_1_1?ie=UTF8&amp;amp;qid=1512825081&amp;amp;sr=8-1&amp;amp;keywords=Interest+Rate+Derivatives+Explained%3A+Volume+2%3A+Term+Structure+and+Volatility+Modelling&#34;&gt;Interest Rate Derivatives Explained: Volume 2: Term Structure and Volatility Modelling&lt;/a&gt; by Jörg Kienitz and Peter Casper. It refers to the paper &amp;ldquo;finite difference techniques for arbitrage-free SABR&amp;rdquo;, written in collaboration with Gary Kennedy.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Interest-Rate-Derivatives-Explained-Engineering/dp/1137360062/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1512824387&amp;amp;sr=1-1&#34;&gt;Interest Rate Derivatives Explained: Volume 1: Products and Markets&lt;/a&gt; by Jörg Kienitz. It refers to my paper on curve interpolation (there is a mistake in the actual reference given inside the book,about arbitrage-free SABR, which unrelated to the text). I like how this book gives real world market data related to the products considered.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.amazon.com/Interest-Rate-Modelling-Multi-Curve-Framework/dp/1137374659/ref=sr_1_1?s=books&amp;amp;ie=UTF8&amp;amp;qid=1512825258&amp;amp;sr=1-1&amp;amp;keywords=henrard+interest&#34;&gt;Interest Rate Modelling in the Multi-Curve Framework: Foundations, Evolution and Implementation&lt;/a&gt; by &lt;a href=&#34;http://multi-curve-framework.blogspot.fr/&#34;&gt;Marc Henrard&lt;/a&gt;. It refers to the paper about yield curve interpolation. This is one of the rare books to present curve construction in depth.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;There are also some Springer books which are typically a collection of papers on a specific subject (which I find less interesting).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Blogs on Quantitative Finance</title>
      <link>https://chasethedevil.github.io/post/quantitative_finance_blogs/</link>
      <pubDate>Wed, 21 Jun 2017 23:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/quantitative_finance_blogs/</guid>
      <description>&lt;p&gt;There are not many blogs on quantitative finance that I read. Blogs are not so popular anymore with the advent of the various social networks (facebook, stackoverflow, google plus, reddit, &amp;hellip;). Here is a small list:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.clarusft.com/blog/&#34;&gt;Clarus FT&lt;/a&gt;: often interesting statistics on the swap market, clearing, plus the &lt;a href=&#34;https://www.clarusft.com/author/gary/&#34;&gt;more technical articles from Gary&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://quantsrus.github.io/&#34;&gt;Quants R Us&lt;/a&gt;: A relatively new blog with a promising starting post analyzing &lt;a href=&#34;https://quantsrus.github.io/post/andreasen_huge_spline/&#34;&gt;Andreasen-Huge one-step local-volatility algorithm with a Spline&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://quantlib.wordpress.com/author/petercaspers/&#34;&gt;Fooling around with Quantlib&lt;/a&gt;: the blog from Peter Caspers, also relevant  to non-Quantlib professionals has original insights such as &lt;a href=&#34;https://quantlib.wordpress.com/2015/09/19/smile-dynamics-by-densities/&#34;&gt;Smile dynamics by densities&lt;/a&gt; or the &lt;a href=&#34;https://quantlib.wordpress.com/2015/08/23/supernatural-libor-coupons/&#34;&gt;Supernatural Libor Coupons&lt;/a&gt;. Unfortunately it is not so active anymore.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.implementingquantlib.com&#34;&gt;Implementing Quantlib&lt;/a&gt;: the blog from Luigi Ballabio, which explains many of the design decisions in Quantlib. Very interesting for developer of financial libraries, see for example &lt;a href=&#34;http://www.implementingquantlib.com/2017/04/fd-solvers.html&#34;&gt;the fd solvers&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://hpcquantlib.wordpress.com/&#34;&gt;HPC Quantlib&lt;/a&gt; from Klaus Spanderen. Yes lots of quantlib blogs, but this one is actually not much focused on quantlib. It goes into great details about some numerical techniques, see for example the &lt;a href=&#34;https://hpcquantlib.wordpress.com/2017/05/07/newer-semi-analytic-heston-pricing-algorithms/&#34;&gt;analysis of Heston pricing algorithm&lt;/a&gt;&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://oxfordstrat.com/rd-blog/&#34;&gt;Oxford Strat&lt;/a&gt;. It is a different kind of subject: too many trading strategies but some interesting data, for example &lt;a href=&#34;http://oxfordstrat.com/data/global-market-correlations/&#34;&gt;global market correlations&lt;/a&gt; and &lt;a href=&#34;http://oxfordstrat.com/ideas/sharpe-ratio/&#34;&gt;ideas&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://forum.wilmott.com/&#34;&gt;Wilmott forums&lt;/a&gt; not a blog, but it sometimes (not often) has interesting discussions and can be a good way to connect.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Another way to find out what&amp;rsquo;s going on in the quantitative finance world is to scan regularly recent papers on &lt;a href=&#34;https://arxiv.org/list/q-fin/recent&#34;&gt;arxiv&lt;/a&gt;, &lt;a href=&#34;http://www.ssrn.com&#34;&gt;SSRN&lt;/a&gt; or the suggestions of &lt;a href=&#34;http://scholar.google.com&#34;&gt;Google scholar&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Typo in Hyman non-negative constraint - 28 years later</title>
      <link>https://chasethedevil.github.io/post/typo-in-hyman-non-negative-constraint/</link>
      <pubDate>Tue, 23 May 2017 23:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/typo-in-hyman-non-negative-constraint/</guid>
      <description>&lt;p&gt;In their paper &lt;a href=&#34;http://www.ams.org/journals/mcom/1989-52-186/S0025-5718-1989-0962209-1/S0025-5718-1989-0962209-1.pdf&#34;&gt;&amp;ldquo;Nonnegativity-, Monotonicity-, or Convexity-Preserving Cubic and Quintic Hermite Interpolation&amp;rdquo;&lt;/a&gt;, Dougherty, Edelman and Hyman present a simple filter on the first derivatives to maintain positivity of a cubic spline interpolant.&lt;/p&gt;&#xA;&lt;p&gt;Unfortunately, in their main formula for non-negativity, they made a typo: the equation (3.3) is not consistent with the equation (3.1): the  \( \Delta x_{i-1/2} \)  is interverted with  \( \Delta x_{i+1/2} \).&lt;/p&gt;&#xA;&lt;p&gt;It was not obvious to find out which equation was wrong since there is no proof in the paper. Fortunately, the proof is in the reference paper &lt;a href=&#34;http://epubs.siam.org/doi/abs/10.1137/0722023&#34;&gt;&amp;ldquo;Monotone piecewise bicubic interpolation&amp;rdquo;&lt;/a&gt; from Carlson and Fritsch and it is clear then that equation (3.1) is the correct one.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Implied Volatility from Black-Scholes price</title>
      <link>https://chasethedevil.github.io/post/implied-volatility-from-black-scholes-price/</link>
      <pubDate>Sun, 02 Apr 2017 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/implied-volatility-from-black-scholes-price/</guid>
      <description>&lt;p&gt;Dan Stefanica and Rados Radoicic propose a quite good initial guess in their very recent paper &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2908494&#34;&gt;An Explicit Implied Volatility Formula&lt;/a&gt;. Their formula is simple, fast to compute and results in an implied volatility guess with a relative error of less than 10%.&lt;/p&gt;&#xA;&lt;p&gt;It is more robust than the rational fraction from &lt;a href=&#34;https://mpra.ub.uni-muenchen.de/6867/1/MPRA_paper_6867.pdf&#34;&gt;Minquiang Li&lt;/a&gt;: his rational fraction is only valid for a fixed range of strikes and maturities. The new approximation is mathematically proved accurate across all strikes and all maturities. There is only the need to be careful in the numerical implementation with the case where the price is very small (a Taylor expansion of the variable C will be useful in this case).&lt;/p&gt;</description>
    </item>
    <item>
      <title>The VIX starts smiling</title>
      <link>https://chasethedevil.github.io/post/vix-starts-smiling/</link>
      <pubDate>Tue, 21 Mar 2017 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/vix-starts-smiling/</guid>
      <description>&lt;p&gt;The VIX implied volatilities used to look like a logarithmic function of the strikes. I don&amp;rsquo;t look at them often, but today, I noticed that the VIX had the start of a smile shape.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/vix_smile.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;1m VIX implied volatilities on March 21, 2017 with strictly positive volume.&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Very few strikes trades below the VIX future level (12.9). All of this is likely because the VIX is unusually low: not many people are looking to trade it much lower.&lt;/p&gt;</description>
    </item>
    <item>
      <title>When SVI Breaks Down</title>
      <link>https://chasethedevil.github.io/post/when-svi-breaks-down/</link>
      <pubDate>Thu, 16 Mar 2017 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/when-svi-breaks-down/</guid>
      <description>&lt;p&gt;In order to fit the implied volatility smile of equity options, one of the most popular parameterization is Jim Gatheral&amp;rsquo;s SVI, which I have written about before &lt;a href=&#34;https://chasethedevil.github.io/post/another-svi-initial-guess/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;It turns out that in the current market conditions, SVI does not work well for short maturities. &lt;a href=&#34;http://www.optionistics.com/quotes/stock-option-chains/SPX&#34;&gt;SPX options&lt;/a&gt; expiring on March 24, 2017 (one week) offer a good example. I paid attention to include only options with non zero volume, that is options that are actually traded.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brownian Bridge and Discrete Random Variables</title>
      <link>https://chasethedevil.github.io/post/brownian-bridge-and-discrete-sampling/</link>
      <pubDate>Thu, 26 Jan 2017 14:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/brownian-bridge-and-discrete-sampling/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://chasethedevil.github.io/post/a-new-scheme-for-heston&#34;&gt;new Heston discretisation scheme&lt;/a&gt; I wrote about a few weeks ago makes use&#xA;a discrete random variable matching the first five moments of the normal distribution instead of the usual&#xA;normally distributed random variable, computed via the inverse cumulative distribution function. Their discrete random&#xA;variable is:&#xA;$$\xi =&#x9;\sqrt{1-\frac{\sqrt{6}}{3}} \quad \text{ if } U_1 &amp;lt; 3,,$$&#xA;$$&#x9;\xi =-\sqrt{1-\frac{\sqrt{6}}{3}} \quad \text{ if } U_1 &amp;gt; 4,,$$&#xA;$$\xi =&#x9;\sqrt{1+\sqrt{6}} \quad \text{ if } U_1 = 3,,$$&#xA;$$\xi =&#x9;-\sqrt{1+\sqrt{6}} \quad \text{ if } U_1 = 4,,$$&#xA;with \(U_1 \in \{0,1,&amp;hellip;,7\}\)&lt;/p&gt;</description>
    </item>
    <item>
      <title>A new scheme for Heston - Part 2</title>
      <link>https://chasethedevil.github.io/post/a-new-scheme-for-heston_part2/</link>
      <pubDate>Mon, 23 Jan 2017 07:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/a-new-scheme-for-heston_part2/</guid>
      <description>&lt;p&gt;A couple weeks ago, I wrote about &lt;a href=&#34;https://chasethedevil.github.io/post/a-new-scheme-for-heston&#34;&gt;a new Heston discretisation scheme&lt;/a&gt; which was at least as accurate as Andersen QE scheme and faster, called DVSS2.&lt;/p&gt;&#xA;&lt;p&gt;It turns out that it does not behave very well on the following Vanilla forward start option example (which is quite benign).&#xA;The Heston parameters comes from a calibration to the market and are&lt;/p&gt;&#xA;&lt;p&gt;$$v_0= 0.0718, \kappa= 1.542, \theta= 0.0762, \sigma= 0.582, \rho= -0.352$$&lt;/p&gt;&#xA;&lt;p&gt;with a maturity of one year.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Equivalence between floating-strike and fixed-strike Asian options</title>
      <link>https://chasethedevil.github.io/post/floating_strike_fixed_strike_asian_equivalence/</link>
      <pubDate>Wed, 18 Jan 2017 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/floating_strike_fixed_strike_asian_equivalence/</guid>
      <description>&lt;p&gt;Many papers present formulae to price Asian options in the Black-Scholes world only for the fixed strike Asian case, that is&#xA;a contract that pays \( \max(A-K,0)\) at maturity \(T\) where \(A = \sum_{i=0}^{n-1} w_i S(t_i) \) is the Asian average.&lt;/p&gt;&#xA;&lt;p&gt;More generally, this can be seen as the payoff of a Basket option where the underlyings are just the same asset but at different times.&#xA;And any Basket option formula can actually be used to price fixed-strike Asian options by letting the correlation correspond to the correlation between the asset at the averaging times&#xA;and the variances correspond to the variance at each averaging time. The basket approach allows then naturally for a term-structure of rates, dividends and volatilities.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bachelier Normal Volatility Asymptotics</title>
      <link>https://chasethedevil.github.io/post/normal_volatility_asymptotics/</link>
      <pubDate>Tue, 17 Jan 2017 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/normal_volatility_asymptotics/</guid>
      <description>&lt;p&gt;It is relatively well known that the Black-Scholes volatility can not grow faster than \(\sqrt{\ln(K)}\).&#xA;The rule is also sometimes simply stated as &amp;ldquo;the implied variance can not grow faster than linear&amp;rdquo; (in log-moneyness).&#xA;The proof comes from Roger Lee &lt;a href=&#34;http://math.uchicago.edu/~rogerlee/moment.pdf&#34;&gt;&amp;ldquo;The moment formula for implied volatility at extreme strikes&amp;rdquo;&lt;/a&gt; but the rule was suggested&#xA;earlier, for example in Hodge&amp;rsquo;s paper from 1996 &lt;a href=&#34;http://www.iijournals.com/doi/pdfplus/10.3905/jod.1996.407950&#34;&gt;&amp;ldquo;Arbitrage bounds of the implied volatility strike and term structures of European-style options&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A new scheme for Heston</title>
      <link>https://chasethedevil.github.io/post/a-new-scheme-for-heston/</link>
      <pubDate>Fri, 06 Jan 2017 07:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/a-new-scheme-for-heston/</guid>
      <description>&lt;p&gt;I stumbled recently upon a new Heston discretisation scheme, in the spirit of Alfonsi, not more complex and more accurate.&lt;/p&gt;&#xA;&lt;p&gt;My first attempt at coding the scheme resulted in a miserable failure even though the described algorithm looked&#xA;not too difficult. I started wondering if &lt;a href=&#34;http://gs.elaba.lt/object/elaba:18270166/18270166.pdf&#34;&gt;the paper&lt;/a&gt;, from a little known Lithuanian mathematical journal, was any good.&#xA;Still, the math in it is very well written, with a great emphasis on the settings for each proposition.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Andreasen-Huge interpolation - Don&#39;t stay flat</title>
      <link>https://chasethedevil.github.io/post/dont-stay-flat-with-andreasen-huge-interpolation/</link>
      <pubDate>Tue, 13 Dec 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/dont-stay-flat-with-andreasen-huge-interpolation/</guid>
      <description>&lt;p&gt;Jesper Andreasen and Brian Huge propose an arbitrage-free interpolation method&#xA;based on a single-step forward Dupire PDE solution in their paper &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1694972&#34;&gt;Volatility interpolation&lt;/a&gt;.&#xA;To do so, they consider a piecewise constant representation of the local volatility in maturity time and strike&#xA;where the number of constants matches the number of market option prices.&lt;/p&gt;&#xA;&lt;p&gt;An interesting example that shows some limits to the technique as described in Jesper Andreasen and Brian Huge paper comes from&#xA;Nabil Kahale paper on &lt;a href=&#34;https://www.researchgate.net/profile/Nabil_Kahale/publication/228872089_An_Arbitrage-free_Interpolation_of_Volatilities/links/0c96053b56097decd5000000.pdf&#34;&gt;an arbitrage-free interpolation of volatilities&lt;/a&gt;.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/kahale_spx500_1995.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;option volatilities for the SPX500 in October 1995.&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Put-Call parity and the log-transformed Black-Scholes PDE</title>
      <link>https://chasethedevil.github.io/post/put_call_parity_with_log_transformed_pde/</link>
      <pubDate>Mon, 05 Dec 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/put_call_parity_with_log_transformed_pde/</guid>
      <description>&lt;p&gt;We will assume zero interest rates and no dividends on the asset \(S\) for clarity.&#xA;The results can be easily generalized to the case with non-zero interest rates and dividends.&#xA;Under those assumptions, the Black-Scholes PDE is:&#xA;$$&#x9;\frac{\partial V}{\partial t} + \frac{1}{2}\sigma^2 S^2 \frac{\partial^2 V}{\partial S^2} = 0.$$&lt;/p&gt;&#xA;&lt;p&gt;An implicit Euler discretisation on a uniform grid in \(S\) of width \(h\) with linear boundary conditions (zero Gamma) leads to:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Benaim et al. extrapolation does not work on equities</title>
      <link>https://chasethedevil.github.io/post/issues_with_bdk_extrapolation/</link>
      <pubDate>Tue, 04 Oct 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/issues_with_bdk_extrapolation/</guid>
      <description>&lt;p&gt;We have seen &lt;a href=&#34;https://chasethedevil.github.io/mystic_parabola.md&#34;&gt;earlier&lt;/a&gt; that a simple parabola allows to capture the smile of AAPL 1m options surprisingly well. For very high and very low strikes,&#xA;the parabola does not obey Lee&amp;rsquo;s moments formula (the behavior in the wings needs to be at most linear in variance/log-moneyness).&lt;/p&gt;&#xA;&lt;p&gt;Extrapolating the volatility smile in the low or high strikes in a smooth \(C^2\) fashion is however not easy.&#xA;A surprisingly popular so called &amp;ldquo;arbitrage-free&amp;rdquo;&#xA;method is the &lt;a href=&#34;http://www.quarchome.org/RiskTailsPaper_v5.pdf&#34;&gt;extrapolation of Benaim, Dodgson and Kainth&lt;/a&gt; developed to remedy the negative density of SABR in interest rates as&#xA;well as to give more control over the wings.&lt;/p&gt;</description>
    </item>
    <item>
      <title>AES for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/aes_for_monte_carlo/</link>
      <pubDate>Wed, 17 Aug 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/aes_for_monte_carlo/</guid>
      <description>&lt;p&gt;In finance, and also in science, the &lt;a href=&#34;https://en.wikipedia.org/wiki/Mersenne_Twister&#34;&gt;Mersenne-Twister&lt;/a&gt; is the de-factor pseudo-random number generator (PRNG) for Monte-Carlo simulations.&#xA;By the way, there is a &lt;a href=&#34;http://arxiv.org/abs/1301.5435&#34;&gt;recent 64-bit maximally equidistributed version&lt;/a&gt; called MEMT19937 with 53-bit double precision floating point numbers in mind.&lt;/p&gt;&#xA;&lt;p&gt;D.E. Shaw paper &lt;a href=&#34;https://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwiyl5Lg8cjOAhVMahoKHVhPCPQQFggmMAA&amp;amp;url=http%3A%2F%2Fwww.thesalmons.org%2Fjohn%2Frandom123%2Fpapers%2Frandom123sc11.pdf&amp;amp;usg=AFQjCNEZ5I7JeeDSELDJBDjLU84tXKmI3w&amp;amp;sig2=UqLBNOlLjkHsMMncABKkIg&amp;amp;bvm=bv.129759880,d.d2s&#34;&gt;Parallel Random Numbers: As easy as 1, 2, 3&lt;/a&gt;&#xA;makes a bold remark: since &lt;a href=&#34;https://en.wikipedia.org/wiki/AES_instruction_set&#34;&gt;specific AES instructions&lt;/a&gt; have&#xA;been available since 2010 in most x86 processors, why not use them?&lt;/p&gt;&#xA;&lt;p&gt;Historicaly, counter-based PRNGs based on cryptographic standards such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Advanced_Encryption_Standard&#34;&gt;AES&lt;/a&gt;&#xA;were historically slow, which motivated the development of sequential PRNGs with good statistical properties,&#xA;yet not cryptographically strong like the Mersenne Twister for Monte-Carlo simulations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Number of regressors in a BSDE</title>
      <link>https://chasethedevil.github.io/post/number_of_regressors_in_bdse/</link>
      <pubDate>Tue, 26 Jul 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/number_of_regressors_in_bdse/</guid>
      <description>&lt;p&gt;Last year, I was kindly invited at the workshop on Models and Numerics in Financial Mathematics at the Lorentz center.&#xA;It was surprinsgly interesting on many different levels. Beside the relatively large gap between academia and the industry, which this&#xA;workshop was trying to address, one thing that struck me is how difficult it was for people of slightly different specialties to communicate.&lt;/p&gt;&#xA;&lt;p&gt;It seemed that mathematicians of different countries working on different subjects related to backward stochastic differential equations (BSDEs) would not truly understand each other. I know this is&#xA;very subjective, and maybe those mathematicians did not feel this at all. One concrete example is related to the number of regressors needed to solve a BSDE on 10 different variables.&#xA;Solving a BSDE on 10 variables for EDF was given as an illustration at the end of an otherwise excellent presentation.&#xA;Someone in the audience asked how possibly they could do that in practice since it would involve \(10^{10}\) regression factors. The answer of the speaker was more or less that it was what they do, with no particular trick but with a large computing power, as if \(10^{10}\) was not so big.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shooting arbitrage - part II</title>
      <link>https://chasethedevil.github.io/post/shooting_arbitrage2/</link>
      <pubDate>Tue, 05 Jul 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/shooting_arbitrage2/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/shooting_arbitrage/&#34;&gt;previous post&lt;/a&gt;, I looked at de-arbitraging volatilities of options of a specific maturity with the shooting method.&#xA;In reality it is not so practical. While the local volatility will be continuous at the given expiry \(T\), it won&amp;rsquo;t be so at the times \( t \lt T \)&#xA;because of the interpolation or extrapolation in time. If we consider a single market expiry at time \(T\),&#xA;it is standard practice to extrapolate the implied volatility flatly for \(t \lt T\), that is, \(w(y,t) = v_T(y) t\)&#xA;where the variance at time \(T\) is defined as \(v_T(y)= \frac{1}{T}w(y,T)\).&#xA;Plugging this into the local variance formula leads to&#xA;$$\sigma^{\star 2}\left(y, t\right) = \frac{ v_T(y)}{1 - \frac{y}{v_T}\frac{\partial v_T}{\partial y}&#x9;+ \frac{1}{4}\left(-\frac{t^2}{4}-\frac{t}{v_T}+\frac{y^2}{v_T^2}\right)\left(\frac{\partial v_T}{\partial y}\right)^2 + \frac{t}{2}\frac{\partial^{2} v_T}{\partial y^2}}$$&#xA;for \(t\leq T\). In particular, for \(t=0\), we have&#xA;$$\sigma^{\star 2}\left(y, 0\right) = \frac{ v_T(y)}&#xA;{1 - \frac{y}{v_T}\frac{\partial v_T}{\partial y}&#x9;+ \frac{1}{4}\left(\frac{y^2}{v_T^2}\right)\left(\frac{\partial v_T}{\partial y}\right)^2}$$&#xA;But the first derivative is not continuous, and jumps at \(y=y_0\) and \(y=y_1\). The local volatility will jump as well around those points. Thus, in practice, the technique can not be used for pricing under local volatility.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Shooting arbitrage - part I</title>
      <link>https://chasethedevil.github.io/post/shooting_arbitrage/</link>
      <pubDate>Wed, 22 Jun 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/shooting_arbitrage/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/damghani_dearbitraging_a_weak_smile_on_svi/&#34;&gt;previous post&lt;/a&gt;, I looked at de-arbitraging volatilities of options of a specific maturity with SVI (re-)calibration.&#xA;The penalty method can be used beyond SVI. For example I interpolate here with a cubic spline on 11 equidistant nodes the original volatility slice that contains arbitrages and then minimize with Levenberg-Marquardt&#xA;and the negative local variance denominator penalty on 51 equidistant points. This results in a quite small adjustment to the original volatilities:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dearbitraging a weak smile on SVI with Damghani&#39;s method</title>
      <link>https://chasethedevil.github.io/post/damghani_dearbitraging_a_weak_smile_on_svi/</link>
      <pubDate>Wed, 15 Jun 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/damghani_dearbitraging_a_weak_smile_on_svi/</guid>
      <description>&lt;p&gt;Yesterday, I wrote about some &lt;a href=&#34;https://chasethedevil.github.io/post/svi_zeliade_arbitrage/&#34;&gt;calendar spread arbitrages with SVI&lt;/a&gt;. Today I am looking at the famous example of butterfly spread arbitrage from Axel Vogt.&#xA;$$(a, b, m, \rho, \sigma) = (−0.0410, 0.1331, 0.3586, 0.3060, 0.4153)$$&#xA;The parameters obey the weak no-arbitrage constraint of Gatheral, and yet produce a negative density, or equivalently, a negative denominator in the local variance Dupire formula.&#xA;Those parameters are mentioned in Jim Gatheral and Antoine Jacquier paper on &lt;a href=&#34;http://ssrn.com/abstract=2033323&#34;&gt;arbitrage free SVI volatility surfaces&lt;/a&gt; and also in Damghani&amp;rsquo;s paper &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2428532&#34;&gt;dearbitraging a weak smile&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Arbitrage in Zeliade&#39;s SVI example</title>
      <link>https://chasethedevil.github.io/post/svi_zeliade_arbitrage/</link>
      <pubDate>Tue, 14 Jun 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/svi_zeliade_arbitrage/</guid>
      <description>&lt;p&gt;Zeliade wrote an &lt;a href=&#34;http://www.zeliade.com/whitepapers/zwp-0005.pdf&#34;&gt;excellent paper&lt;/a&gt; about the calibration of the SVI parameterization for the volatility surface in 2008. I just noticed recently&#xA;that their example calibration actually contained strong calendar spread arbitrages. This is not too surprising if you look at the parameters,&#xA;they vary wildly between the first and the second expiry.&lt;/p&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;T&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;a&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;b&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;rho&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;m&lt;/th&gt;&#xA;          &lt;th style=&#34;text-align: center&#34;&gt;s&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.082&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.027&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.234&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.068&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.100&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.028&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.16&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.030&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.125&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;-1.0&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.074&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.050&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.26&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.032&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.094&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;-1.0&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.093&lt;/td&gt;&#xA;          &lt;td style=&#34;text-align: center&#34;&gt;0.041&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;The calendar spread arbitrage is very visible in total variance versus log-moneyness graph:&#xA;in those coordinates if lines crosses, there is an arbitrage. This is because the total variance should be increasing with the expiry time.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/svi_zeliade_arb.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Arbitrage in Zeliade&amp;#39;s example&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dupire Local Volatility with Cash Dividends Part 2</title>
      <link>https://chasethedevil.github.io/post/dupire_cash_dividend_part2/</link>
      <pubDate>Sun, 29 May 2016 17:01:00 +0200</pubDate>
      <guid>https://chasethedevil.github.io/post/dupire_cash_dividend_part2/</guid>
      <description>&lt;p&gt;I had a look at how to price under Local Volatility with Cash dividends in &lt;a href=&#34;https://chasethedevil.github.io/post/dupire_cash_dividend/&#34;&gt;my previous post&lt;/a&gt;. I still had a somewhat large error in my FDM price. After too much time, I managed to find the culprit, it was the extrapolation of the prices when applying the jump continuity condition \(V(S,t_\alpha^-) = V(S-\alpha, t_\alpha^+) \) for an asset \(S\) with a cash dividend of amount \(\alpha\) at \( t_\alpha \).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Dupire Local Volatility with Cash Dividends</title>
      <link>https://chasethedevil.github.io/post/dupire_cash_dividend/</link>
      <pubDate>Thu, 19 May 2016 17:01:00 +0200</pubDate>
      <guid>https://chasethedevil.github.io/post/dupire_cash_dividend/</guid>
      <description>&lt;p&gt;The Dupire equation for local volatility has been derived under the assumption of Martingality, that means no dividends or interest rates.&#xA;The extension to continuous dividend yield is described in many papers or books:&lt;/p&gt;&#xA;&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;With cash dividends however, the Black-Scholes formula is not valid anymore if we suppose that the asset jumps at the dividend date of the dividend amount. There are various relatively accurate&#xA;approximations available to price an option supposing a constant (spot) volatility and jumps, for example, &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SVI, SABR, or parabola on AAPL?</title>
      <link>https://chasethedevil.github.io/post/svi_sabr_or_parabola/</link>
      <pubDate>Thu, 12 May 2016 19:32:42 +0200</pubDate>
      <guid>https://chasethedevil.github.io/post/svi_sabr_or_parabola/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://chasethedevil.github.io/post/least_squares_spline&#34;&gt;previous post&lt;/a&gt;, I took a look at least squares spline and parabola fits on AAPL 1m options market volatilities. I would have imagined SVI to fit even  better since it has 5 parameters, and SABR to do reasonably well.&lt;/p&gt;&#xA;&lt;p&gt;It turns out that the simple parabola has the lowest RMSE, and SVI is not really better than SABR on that example.&lt;/p&gt;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/svi_sabr_parabola.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;SVI, SABR, least squares parabola fitted to AAPL 1m options&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Note that this is just one single example, unlikely to be representative of anything, but I thought this was interesting that in practice, a simple parabola can compare favorably to more complex models.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adaptive Filon quadrature for stochastic volatility models</title>
      <link>https://chasethedevil.github.io/post/filon_for_heston/</link>
      <pubDate>Thu, 12 May 2016 19:08:18 +0200</pubDate>
      <guid>https://chasethedevil.github.io/post/filon_for_heston/</guid>
      <description>&lt;p&gt;A while ago, I have applied a relatively simple adaptive Filon quadrature to the problem of &lt;a href=&#34;http://papers.ssrn.com/abstract=2620166&#34;&gt;volatility swap pricing&lt;/a&gt;. The &lt;a href=&#34;https://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwi18brlh9XMAhWIVhoKHdnGA2UQFggdMAA&amp;amp;url=http%3A%2F%2Fwww.ams.org%2Fmcom%2F1968-22-101%2FS0025-5718-1968-0225485-5%2FS0025-5718-1968-0225485-5.pdf&amp;amp;usg=AFQjCNEQvSMm6vOaXIX2MqAJ-GQt79QRiA&amp;amp;sig2=HLHd-rc74qnCuo5yp1Q13A&#34;&gt;Filon quadrature&lt;/a&gt; is an old quadrature from 1928 that allows to integrate oscillatory integrand like \(f(x)\cos(k x) \) or \(f(x)\sin(k x) \). It turns out that combined with an adaptive Simpson like method, it has many advantages over more generic adaptive quadrature methods like Gauss-Lobatto, which is often used on similar problems.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Least Squares Rational Function</title>
      <link>https://chasethedevil.github.io/post/rational_fit/</link>
      <pubDate>Thu, 21 Apr 2016 16:37:24 +0200</pubDate>
      <guid>https://chasethedevil.github.io/post/rational_fit/</guid>
      <description>&lt;p&gt;In my paper &lt;a href=&#34;http://ssrn.com/abstract=2420757&#34;&gt;&amp;ldquo;Fast and Accurate Analytic Basis Point Volatility&amp;rdquo;&lt;/a&gt;,&#xA;I use a table of Chebyshev polynomials to provide an accurate representation of some function. This is&#xA;an idea I first saw in the &lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package&#34;&gt;Faddeeva package&lt;/a&gt; to&#xA;represent the cumulative normal distribution with high accuracy, and high performance. It is also&#xA;simple to find out the Chebyshev polynomials, and which intervals are the most appropriate for those, which&#xA;makes this technique quite appealing.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Least Squares Spline for Volatility Interpolation</title>
      <link>https://chasethedevil.github.io/post/least_squares_spline/</link>
      <pubDate>Fri, 19 Feb 2016 18:29:33 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/least_squares_spline/</guid>
      <description>&lt;p&gt;I am experimenting a bit with least squares splines. Existing algorithms (for example from the NSWC Fortran library) usually work&#xA;with B-splines, a relatively simple explanation of how it works is given in &lt;a href=&#34;http://www.geometrictools.com/Documentation/BSplineCurveLeastSquaresFit.pdf&#34;&gt;this paper&lt;/a&gt; (I think this is how De Boor coded it in the NSWC library).&#xA;Interestingly there is &lt;a href=&#34;http://educ.jmu.edu/~lucassk/Papers/Spline3.pdf&#34;&gt;an equivalent formulation in terms of standard cubic splines&lt;/a&gt;, although it seems that the&#xA;pseudo code on that paper has errors.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Mystic Parabola</title>
      <link>https://chasethedevil.github.io/post/mystic_parabola/</link>
      <pubDate>Tue, 16 Feb 2016 22:13:53 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/mystic_parabola/</guid>
      <description>&lt;p&gt;I recently had some fun trying to work directly with the option chain from the &lt;a href=&#34;http://www.nasdaq.com/symbol/aapl/option-chain&#34;&gt;Nasdaq website&lt;/a&gt;.&#xA;The data there is quite noisy, but a simple parabola can still give an amazing fit. I will consider the options of maturity two years as illustration.&#xA;I also relied on a simple implied volatility algorithm that can be summarized in the following steps:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Compute a rough guess for the forward price by using interest, borrow curves and by extrapolating the dividends.&lt;/li&gt;&#xA;&lt;li&gt;Imply the forward from the European Put-Call parity relationship on the mid prices of the two strikes closes to the rough forward guess. A simple linear interpolation between the two strikes can be used to compute the forward.&lt;/li&gt;&#xA;&lt;li&gt;Compute the Black implied volatilities as if the option were European using P. Jaeckel algorithm.&lt;/li&gt;&#xA;&lt;li&gt;Calibrate the proportional dividend amount or the growth rate by minimizing, for example with a Levenberg-Marquardt minimizer, the difference between model and mid-option prices corresponding to the three strikes closest to the forward. The parameters in this case are effectively the dividend amount and the volatilities for Put and Call options (the same volatility is used for both options). The initial guess stems directly from the two previous steps. American option prices are computed by the finite difference method.&lt;/li&gt;&#xA;&lt;li&gt;Solve numerically the volatilities one by one with the TOMS748 algorithm so that the model prices match the market mid out-of-the-money option prices.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;Then I just fit a least squares parabola in variance on log-moneyness, using options trading volumes as weights and obtain the following figure:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Yahoo Finance Implied Volatility</title>
      <link>https://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</link>
      <pubDate>Wed, 03 Feb 2016 16:45:58 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://finance.yahoo.com/q/op?s=GOOG&amp;amp;date=1457049600&#34;&gt;option chain&lt;/a&gt; on Yahoo finance shows an implied volatility number for each call or put option in the last column.&#xA;I was wondering a bit how they computed that number. I did not exactly find out their methodology, especially since we don&amp;rsquo;t even know the daycount convention used, but&#xA;I did find that it was likely just garbage.&lt;/p&gt;&#xA;&lt;p&gt;A red-herring is for example the large discrepancy between put vols and call vols. For example strike 670, call vol=50%, put vol=32%.&#xA;This suggests that the two are completely decoupled, and they use some wrong forward (spot price?) to obtain those numbers. If I compute&#xA;the implied volatilities using put-call parity close to the money to find out the implied forward price, I end up with ask vols of 37% and 34% or call and put mid vols of 33%.&#xA;By considering the put-call parity, I assume European option prices, which is not correct in this case. It turns out however, that with the low interest rates we live in, there is nearly zero additional value due to the American early exercise.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Linear and Flat forward interpolation with cash dividends</title>
      <link>https://chasethedevil.github.io/post/linear_flat_forward_interpolation/</link>
      <pubDate>Tue, 19 Jan 2016 09:55:32 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/linear_flat_forward_interpolation/</guid>
      <description>&lt;p&gt;When the dividend curve is built from discrete cash dividends, the dividend yield is discontinuous at the dividend time as the asset price jumps from the dividend amount.&#xA;This can be particularly problematic for numerical schemes like finite difference methods. In deed, a finite difference grid&#xA;will make use of the forward yield (eventually adjusted to the discretisation scheme), which explodes then.&#xA;Typically, if one is not careful about this, then increasing the number of time steps does not increase accuracy anymore, as&#xA;the spike just becomes bigger on a smaller time interval. A simple work-around is to limit the resolution to one day.&#xA;This means that intraday, we interpolate the dividend yield.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Go for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/go-for-monte-carlo/</link>
      <pubDate>Sat, 22 Aug 2015 16:13:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/go-for-monte-carlo/</guid>
      <description>&lt;p&gt;I have &lt;a href=&#34;https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/&#34;&gt;looked&lt;/a&gt; a few months ago already at Julia, Dart, Rust and Scala programming languages to see how practical they could be for a simple Monte-Carlo option pricing.&lt;/p&gt;&#xA;&lt;p&gt;I forgot &lt;a href=&#34;https://golang.org/&#34;&gt;the Go language&lt;/a&gt;. I had tried it 1 or 2 years ago, and at that time, did not enjoy it too much. Looking at Go 1.5 benchmarks on the &lt;a href=&#34;http://benchmarksgame.alioth.debian.org/&#34;&gt;computer language shootout&lt;/a&gt;, I was surprised that it seemed so close to Java performance now, while having a GC that guarantees pauses of less 10ms and consuming much less memory.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bumping Correlations</title>
      <link>https://chasethedevil.github.io/post/bumping-correlations/</link>
      <pubDate>Sat, 25 Jul 2015 18:36:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/bumping-correlations/</guid>
      <description>In his book &#34;&lt;i&gt;Monte Carlo Methods in Finance&lt;/i&gt;&#34;, P. Jäckel explains a simple way to clean up a correlation matrix. When a given correlation matrix is not positive semi-definite, the idea is to do a &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular value decomposition&lt;/a&gt; (SVD), replace the negative eigenvalues by 0, and renormalize the corresponding eigenvector accordingly.&lt;br /&gt;&lt;br /&gt;One of the cited applications is &#34;&lt;i&gt;stress testing and scenario analysis for market risk&lt;/i&gt;&#34; or &#34;&lt;i&gt;comparative pricing in order to ascertain the extent of correlation exposure for multi-asset derivatives&lt;/i&gt;&#34;, saying that &#34;&lt;i&gt;In many of these cases we end up with a matrix that is no longer positive semi-definite&lt;/i&gt;&#34;.&lt;br /&gt;&lt;br /&gt;It turns out that if one bumps an invalid correlation matrix (the input), that is then cleaned up automatically, the effect can be a very different bump. Depending on how familiar you are with SVD, this could be more or less obvious from the procedure,&lt;br /&gt;&lt;br /&gt;As a simple illustration I take the matrix representing 3 assets A, B, C with rho_ab = -0.6, rho_ac = rho_bc = -0.5.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.60000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.60000&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.50000&amp;nbsp; -0.50000&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;For those rho_ac and rho_bc, the correlation matrix is not positive definite unless rho_ab in in the range (-0.5, 1). One way to verify this is to use the fact that positive definiteness is equivalent to a positive determinant. The determinant will be 1 - 2*0.25 - rho_ab^2 + 2*0.25*rho_ab.&lt;br /&gt;&lt;br /&gt;After using P. Jaeckel procedure, we end up with: &lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56299&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.56299&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.46745&amp;nbsp; -0.46745&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;If we bump now rho_bc by 1% (absolute), we end up after cleanup with:&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56637&amp;nbsp; -0.47045&lt;br /&gt;&amp;nbsp; -0.56637&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46081&lt;br /&gt;&amp;nbsp; -0.47045&amp;nbsp; -0.46081 &amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;It turns out that rho_bc has changed by only 0.66% and rho_ac by -0.30%, rho_ab by -0.34%. So our initial bump (0,0,1) has been translated to a bump (-0.34, -0.30, 0.66). In other words, it does not work to compute sensitivities.&lt;br /&gt;&lt;br /&gt;One can optimize to obtain the nearest correlation matrix in some norm. Jaeckel proposes a hypersphere decomposition based optimization, using as initial guess the SVD solution. &lt;a href=&#34;https://nickhigham.wordpress.com/2013/02/13/the-nearest-correlation-matrix/&#34;&gt;Higham proposed a specific algorithm&lt;/a&gt; just for that purpose. It turns out that on this example, they will converge to the same solution (if we use the same norm). I tried out of curiosity to see if that would lead to some improvement. The first matrix becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56435&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.56435&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.46672&amp;nbsp; -0.46672&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;And the bumped one becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56766&amp;nbsp; -0.46984&lt;br /&gt;&amp;nbsp; -0.56766&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46002&lt;br /&gt;&amp;nbsp; -0.46984&amp;nbsp; -0.46002&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;We find back the same issue, rho_bc has changed by only 0.67%, rho_ac by -0.31% and rho_ab by -0.33%. We also see that the SVD correlation or the real near correlation matrix are quite close, as noticed by P. Jaeckel.&lt;br /&gt;&lt;br /&gt;Of course, one should apply the bump directly to the cleaned up matrix, in which case it will actually work as expected, unless our bump produces another non positive definite matrix, and then we would have correlation leaking a bit everywhere. It&#39;s not entirely clear what kind of meaning the risk figures would have then.</description>
    </item>
    <item>
      <title>Andreasen Huge extrapolation</title>
      <link>https://chasethedevil.github.io/post/andreasen-huge-extrapolation/</link>
      <pubDate>Mon, 13 Jul 2015 17:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/andreasen-huge-extrapolation/</guid>
      <description>&lt;p&gt;There are not many arbitrage free extrapolation schemes. Benaim et al. extrapolation is one of the few that claims it. However, despite the paper&amp;rsquo;s title, it is not truely arbitrage free. The density might be positive, but the forward is not preserved by the implied density. It can also lead to wings that don&amp;rsquo;t obey Lee&amp;rsquo;s moments condition.&lt;/p&gt;&#xA;&lt;p&gt;On a Wilmott forum, &lt;a href=&#34;http://www.wilmott.com/messageview.cfm?catid=4&amp;amp;threadid=95309&#34;&gt;P. Caspers proposed&lt;/a&gt; the following counter-example based on extrapolating SABR: \( \alpha=15%, \beta=80%, \nu=50%, \rho=-48%, f=3%, T=20.0 \). He cut this smile at 2.5% and 6% and used the BDK extrapolation scheme with mu=nu=1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Unintuitive behavior of the Black-Scholes formula - negative volatilities in displaced diffusion extrapolation</title>
      <link>https://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</link>
      <pubDate>Tue, 07 Jul 2015 16:43:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</guid>
      <description>&lt;p&gt;I am looking at various extrapolation schemes of the implied volatilities. An interesting one I stumbled upon is due to Kahale. Even if &lt;a href=&#34;http://nkahale.free.fr/papers/Interpolation.pdf&#34;&gt;his paper&lt;/a&gt; is on interpolation, there is actually a small paragraph on using the same kind of function for extrapolation. His idea is to simply lookup the standard deviation \( \Sigma \) and the forward \(f\) corresponding to a given market volatility and slope:&#xA;$$ c_{f,\Sigma} = f N(d_1) - k N(d_2)$$&#xA;with&#xA;$$ d_1 = \frac{\log(f/k)+\Sigma^2 /2}{\Sigma} $$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Square Root Crank-Nicolson</title>
      <link>https://chasethedevil.github.io/post/square-root-crank-nicolson/</link>
      <pubDate>Fri, 19 Jun 2015 16:41:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/square-root-crank-nicolson/</guid>
      <description>C. Reisinger kindly pointed out to me &lt;a href=&#34;http://arxiv.org/abs/1210.5487&#34;&gt;this paper around square root Crank-Nicolson&lt;/a&gt;. The idea is to apply a square root of time transformation to the PDE, and discretize the resulting PDE with Crank-Nicolson. Two reasons come to mind to try this: &lt;br /&gt;&lt;ul&gt;&lt;li&gt;the square root transform will result in small steps initially, where the solution is potentially not so smooth, making Crank-Nicolson behave better.&lt;/li&gt;&lt;li&gt;&amp;nbsp;it is the natural time of the Brownian motion.&lt;/li&gt;&lt;/ul&gt;Interestingly, it has nicer properties than what those reasons may suggest. On the Fokker-Planck density PDE, it does not oscillate under some very mild conditions and &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2605160&#34;&gt;preserves density positivity at the peak&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Out of curiosity I tried it to price a one touch barrier option. Of course there is an analytical solution in my test case (Black-Scholes assumptions), but as soon as rates are assumed not constant or local volatility is used, there is no other solution than a numerical method. In the later case, finite difference methods are quite good in terms of performance vs accuracy.&lt;br /&gt;&lt;br /&gt;The classic Crank-Nicolson gives a reasonable price, but the strong oscillations near the barrier, at every time step are not very comforting.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-XNr7vE77Dfo/VYQoBELx2KI/AAAAAAAAICw/FVVYrehW39Y/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A24%253A59.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;337&#34; src=&#34;http://3.bp.blogspot.com/-XNr7vE77Dfo/VYQoBELx2KI/AAAAAAAAICw/FVVYrehW39Y/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A24%253A59.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Crank-Nicolson Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Moving to square root of time removes nearly all oscillations on this problem, even with a relatively low number of time steps compared to the number of space steps.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-JXVRuhLrMOQ/VYQodTFBDGI/AAAAAAAAIC4/hrsSdQbA5Wo/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A14.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://1.bp.blogspot.com/-JXVRuhLrMOQ/VYQodTFBDGI/AAAAAAAAIC4/hrsSdQbA5Wo/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A14.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Square Root Crank-Nicolson Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;We can see that the second step prices are a bit higher than the third step (the lines cross), which looks like a small numerical oscillation in time, even if there is no oscillation is space.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-d-xXHOvO1H0/VYQon2V5eiI/AAAAAAAAIDA/4g-YYby4R0A/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://1.bp.blogspot.com/-d-xXHOvO1H0/VYQon2V5eiI/AAAAAAAAIDA/4g-YYby4R0A/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A06.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;TR-BDF2 Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;As a comparison, the TR-BDF2 scheme does relatively well: oscillations are removed after the second step, even with the extreme ratio of time steps vs space steps used on this example so that illustrations are clearer - Crank-Nicolson would still oscillate a lot with 10 times less space steps but we would not see oscillation on the square root Crank-Nicolson and a very mild one on TR-BDF2.&lt;br /&gt;&lt;br /&gt;The LMG2 scheme (a local richardson extrapolation) does not oscillate at all on this problem but is the slowest:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-HTxKtzO8au4/VYQo8t-wkdI/AAAAAAAAIDI/oPOjJWs0SI0/s1600/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A53.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://3.bp.blogspot.com/-HTxKtzO8au4/VYQo8t-wkdI/AAAAAAAAIDI/oPOjJWs0SI0/s640/Screenshot%2Bfrom%2B2015-06-19%2B16%253A25%253A53.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;LMG2 Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;The square root Crank-Nicolson is quite elegant. It can however not be applied to that many problems in practice, as often some grid times are imposed by the payoff to evaluate, for example in a case of a discrete weekly barrier. But for continuous time problems (density PDE, Vanilla, American, continuous barriers) it&#39;s quite good.&lt;br /&gt;&lt;br /&gt;In reality, with a continuous barrier, the payoff is not discontinuous at every step, but it is only discontinuous at the first step. So Rannacher smoothing would work very well on that problem:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-A-qqKczuefQ/VYQwf2ba_MI/AAAAAAAAIDY/2cYpi_3Y_pI/s1600/Screenshot%2Bfrom%2B2015-06-19%2B17%253A08%253A35.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;338&#34; src=&#34;http://4.bp.blogspot.com/-A-qqKczuefQ/VYQwf2ba_MI/AAAAAAAAIDY/2cYpi_3Y_pI/s640/Screenshot%2Bfrom%2B2015-06-19%2B17%253A08%253A35.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Rannacher Prices near the Barrier. Each line is a different time.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;The somewhat interesting payoff left for the square root Crank-Nicolson is the American.&lt;br /&gt;</description>
    </item>
    <item>
      <title>Decoding Hagan&#39;s arbitrage free SABR PDE derivation</title>
      <link>https://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</link>
      <pubDate>Fri, 08 May 2015 16:50:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/decoding-hagans-arbitrage-free-sabr-pde-derivation/</guid>
      <description>Here are the main steps of Hagan derivation. Let&#39;s recall his notation for the SABR model where typically, \\(C(F) = F^\beta\\)&#xA;&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-uq2IhJPDd7M/VUzC4Hh3xoI/AAAAAAAAH9k/sY034iAD38Y/s1600/Screenshot_2015-05-08_16-04-27.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-uq2IhJPDd7M/VUzC4Hh3xoI/AAAAAAAAH9k/sY034iAD38Y/s1600/Screenshot_2015-05-08_16-04-27.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;First, he defines the moments of stochastic volatility:&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-RxzKVIrxbl8/VUzC4HGhMNI/AAAAAAAAH9c/7FmxbMuB4kw/s1600/Screenshot_2015-05-08_16-04-53.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;60&#34; src=&#34;http://2.bp.blogspot.com/-RxzKVIrxbl8/VUzC4HGhMNI/AAAAAAAAH9c/7FmxbMuB4kw/s320/Screenshot_2015-05-08_16-04-53.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Then he integrates the Fokker-Planck equation over all A, to obtain&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-mDsC-Zd6FMA/VUzC4L9dF_I/AAAAAAAAH9g/cAvRNw1VTkg/s1600/Screenshot_2015-05-08_16-05-36.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-mDsC-Zd6FMA/VUzC4L9dF_I/AAAAAAAAH9g/cAvRNw1VTkg/s1600/Screenshot_2015-05-08_16-05-36.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;On the backward Komolgorov equation, he applies a Lamperti transform like change of variable:&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-QiYromr1SDU/VUzE5q_OfjI/AAAAAAAAH94/nfbr14tEnj0/s1600/Screenshot_2015-05-08_16-10-33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-QiYromr1SDU/VUzE5q_OfjI/AAAAAAAAH94/nfbr14tEnj0/s1600/Screenshot_2015-05-08_16-10-33.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And then makes another change of variable so that the PDE has the same initial conditions for all moments: &#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-1C8j58UD1lA/VUzE5hZJ95I/AAAAAAAAH-I/541DaJBFbAU/s1600/Screenshot_2015-05-08_16-12-34.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-1C8j58UD1lA/VUzE5hZJ95I/AAAAAAAAH-I/541DaJBFbAU/s1600/Screenshot_2015-05-08_16-12-34.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&amp;nbsp;This leads to&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/--wj3nYsi9g8/VUzE5glQ0VI/AAAAAAAAH98/NlVSXIc2NDI/s1600/Screenshot_2015-05-08_16-13-32.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;107&#34; src=&#34;http://3.bp.blogspot.com/--wj3nYsi9g8/VUzE5glQ0VI/AAAAAAAAH98/NlVSXIc2NDI/s320/Screenshot_2015-05-08_16-13-32.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;It turns out that there is a magical symmetry for k=0 and k=2.&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-OStv8D0OSw0/VUzGJ6HKqDI/AAAAAAAAH-Y/jk85U57eHnY/s1600/Screenshot_2015-05-08_16-19-45.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;32&#34; src=&#34;http://1.bp.blogspot.com/-OStv8D0OSw0/VUzGJ6HKqDI/AAAAAAAAH-Y/jk85U57eHnY/s320/Screenshot_2015-05-08_16-19-45.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-67k-xTphi7Q/VUzGJwbwSWI/AAAAAAAAH-U/Faz235QNpKs/s1600/Screenshot_2015-05-08_16-20-09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;32&#34; src=&#34;http://3.bp.blogspot.com/-67k-xTphi7Q/VUzGJwbwSWI/AAAAAAAAH-U/Faz235QNpKs/s320/Screenshot_2015-05-08_16-20-09.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Note that in the second equation, the second derivative applies to the whole.&#xA;Because of this, he can express \\(Q^{(2)}\\) in terms of \\(Q^{(0)}\\):&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-k-3DQd9MuIQ/VUzHl5TSuAI/AAAAAAAAH-s/CjTFtlX4upw/s1600/Screenshot_2015-05-08_16-25-36.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;29&#34; src=&#34;http://1.bp.blogspot.com/-k-3DQd9MuIQ/VUzHl5TSuAI/AAAAAAAAH-s/CjTFtlX4upw/s320/Screenshot_2015-05-08_16-25-36.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And he plugs that back to the integrated Fokker-Planck equation to obtain the arbitrage free SABR PDE:&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-aEwTOxulsWI/VUzHl3qfZAI/AAAAAAAAH-o/qQZPb8Vvz7o/s1600/Screenshot_2015-05-08_16-25-48.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;29&#34; src=&#34;http://1.bp.blogspot.com/-aEwTOxulsWI/VUzHl3qfZAI/AAAAAAAAH-o/qQZPb8Vvz7o/s320/Screenshot_2015-05-08_16-25-48.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&#xA;There is a simple more common explanation in the world of local stochastic volatility for what&#39;s going on. For example, in the particle method paper from Guyon-Labordère, we have the following expression for the true local volatility.&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1U/8Iv1i23oXok/s1600/Screenshot%2Bfrom%2B2013-10-16%2B15%3A51%3A46.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;127&#34; src=&#34;http://3.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1U/8Iv1i23oXok/s320/Screenshot%2Bfrom%2B2013-10-16%2B15%3A51%3A46.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&#xA;&#xA;&#xA;&lt;p&gt;In the first equation, the numerator is simply \(Q^{(2)}\) and the denominator \(Q^{(0)}\). Of course, the integrated Fokker-Planck equation can be rewritten as:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Matching Hagan PDE SABR with the one-step Andreasen-Huge SABR</title>
      <link>https://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</link>
      <pubDate>Thu, 30 Apr 2015 17:16:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/matching-hagan-pde-sabr-with-the-one-step-andreasen-huge-sabr/</guid>
      <description>&lt;p&gt;I looked nearly two years ago already at &lt;a href=&#34;https://chasethedevil.github.io/post/sabr-with-new-hagan-pde-approach&#34;&gt;the arbitrage free SABR of Andreasen-Huge in comparison to the arbitrage free PDE of Hagan&lt;/a&gt; and showed how close the &lt;a href=&#34;https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach&#34;&gt;ideas were&lt;/a&gt;: Andreasen-Huge relies on the normal Dupire forward PDE using a slightly simpler local vol (no time dependent exponential term) while Hagan works directly on the Fokker-Planck PDE (you can think of it as the Dupire Forward PDE for the density) and uses an expansion of same order as the original SABR formula (which leads to an additional exponential term in the local volatility).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Modern Programming Language for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</link>
      <pubDate>Sat, 18 Apr 2015 22:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/modern-programming-language-for-monte-carlo/</guid>
      <description>&lt;p&gt;A few recent programming languages sparked my interest:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://julialang.org/&#34;&gt;Julia&lt;/a&gt; because of the wide coverage of mathematical functions, and great attention to quality of the implementations. It has also some interesting web interface.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;https://www.dartlang.org&#34;&gt;Dart&lt;/a&gt;: because it&amp;rsquo;s a language focused purely on building apps for the web, and has a supposedly good VM.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://www.rust-lang.org/&#34;&gt;Rust&lt;/a&gt;: it&amp;rsquo;s the latest fad. It has interesting concepts around concurrency and a focus on being low level all the while being simpler than C.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;I decided to see how well suited they would be on a simple Monte-Carlo simulation of a forward start option under the Black model. I am no expert at all in any of the languages, so this is a beginner&amp;rsquo;s test. I compared the runtime for executing 16K simulations times a multiplier.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Volatility Swap vs Variance Swap Replication - Truncation</title>
      <link>https://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication---truncation/</link>
      <pubDate>Mon, 16 Mar 2015 14:39:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/volatility-swap-vs-variance-swap-replication---truncation/</guid>
      <description>&lt;p&gt;I have looked at &lt;a href=&#34;https://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/&#34;&gt;jump effects on volatility vs. variance swaps&lt;/a&gt;. There is a similar behavior on tail events, that is, on truncating the replication. One main &lt;a href=&#34;https://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous&#34;&gt;problem with discrete replication of variance swaps&lt;/a&gt; is the implicit domain truncation, mainly because the variance swap equivalent log payoff is far from being linear in the wings. The equivalent payoff with Carr-Lee for a volatility swap is much more linear in the wings (&lt;a href=&#34;https://chasethedevil.github.io/post/a-volatility-swap-and-a-straddle&#34;&gt;not so far of a straddle&lt;/a&gt;). So we could expect the replication to be less sensitive to the wings truncation.&#xA;I have done a simple test on flat 40% volatility:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Arbitrage free SABR with negative rates - alternative to shifted SABR</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates---alternative-to-shifted-sabr/</link>
      <pubDate>Wed, 11 Mar 2015 18:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/arbitrage-free-sabr-with-negative-rates---alternative-to-shifted-sabr/</guid>
      <description>&lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2557046&#34;&gt;Antonov et al.&lt;/a&gt; present an interesting view on SABR with negative rates: instead of relying on a shifted SABR to allow negative rates up to a somewhat arbitrary shift, they modify slightly the SABR model to allow negative rates directly:&#xA;$$ dF_t = |F_t|^\beta v_t dW_F $$&#xA;with \\( v\_t \\) being the standard lognormal volatility process of SABR.&lt;br /&gt;&lt;br /&gt;Furthermore they derive a clever semi-analytical approximation for this model, based on low correlation, quite close to the Monte-Carlo prices in their tests. It&#39;s however not clear if it is arbitrage-free.&lt;br /&gt;&lt;br /&gt;It turns out that it is easy to tweak Hagan SABR PDE approach to this &#34;absolute SABR&#34; model: one just needs to push the boundary \\(F\_{min}\\) far away, and to use the absolute value in C(F).&lt;br /&gt;&lt;br /&gt;It then reproduces the same behavior as in Antonov et al. paper:&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-kKiSIo-QgAg/VQBp3sRtYFI/AAAAAAAAH34/_A9DKmA_n-E/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A13%3A45%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-kKiSIo-QgAg/VQBp3sRtYFI/AAAAAAAAH34/_A9DKmA_n-E/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A13%3A45%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;&#34;Absolute SABR&#34; arbitrage free PDE&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-cRLZged_Ees/VQBp7sKKTrI/AAAAAAAAH4A/PIZbUBBHP1I/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A14%3A02%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-cRLZged_Ees/VQBp7sKKTrI/AAAAAAAAH4A/PIZbUBBHP1I/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A14%3A02%2BPM.png&#34; height=&#34;250&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Antonov et al. graph&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&amp;nbsp;I obtain a higher spike, it would look much more like Antonov graph had I used a lower resolution to compute the density: the spike would be smoothed out.&lt;br /&gt;&lt;br /&gt;Interestingly, the arbitrage free PDE will also work for high beta (larger than 0.5):&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-YngRqD1ilNw/VQBroKrwKWI/AAAAAAAAH4M/3h9N7zHVjx0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A21%3A13%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-YngRqD1ilNw/VQBroKrwKWI/AAAAAAAAH4M/3h9N7zHVjx0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A21%3A13%2BPM.png&#34; height=&#34;190&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;beta = 0.75&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;It turns out to be then nearly the same as the absorbing SABR, even if prices can cross a little the 0. This is how the bpvols look like with beta = 0.75:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-J_cdl8wwUms/VQBsOst-xYI/AAAAAAAAH4U/xWRMyUpOwxA/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A24%3A18%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-J_cdl8wwUms/VQBsOst-xYI/AAAAAAAAH4U/xWRMyUpOwxA/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A24%3A18%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;red = absolute SABR, blue = absorbing SABR with beta=0.75&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;They overlap when the strike is positive.&lt;br /&gt;&lt;br /&gt;If we go back to Antonov et al. first example, the bpvols look a bit funny (very symmetric) with beta=0.1:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-DrQY0znkznc/VQBsxqF8GAI/AAAAAAAAH4g/MGMwg4sS2Zw/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A26%3A30%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-DrQY0znkznc/VQBsxqF8GAI/AAAAAAAAH4g/MGMwg4sS2Zw/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A26%3A30%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;For beta=0.25 we also reproduce Antonov bpvol graph, but with a lower slope for the left wing:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-QtPOjLCr4ts/VQBtT6hqvmI/AAAAAAAAH4o/jHLn9yC6Frk/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A28%3A55%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-QtPOjLCr4ts/VQBtT6hqvmI/AAAAAAAAH4o/jHLn9yC6Frk/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A28%3A55%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;bpvols with beta = 0.25&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;It&#39;s interesting to see that in this case, the positive strikes bp vols are closer to the normal Hagan analytic approximation (which is not arbitrage free) than to the absorbing PDE solution.&lt;br /&gt;&lt;br /&gt;For longer maturities, the results start to be a bit different from Antonov, as Hagan PDE relies on a order 2 approximation only:&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kPhdB8qyCKI/VQBuC3w2G4I/AAAAAAAAH40/lIIp0-zSokU/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A31%3A59%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-kPhdB8qyCKI/VQBuC3w2G4I/AAAAAAAAH40/lIIp0-zSokU/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A31%3A59%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;absolute SABR PDE with 10y maturity&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-h2QCjhFGF14/VQBuTcUmlOI/AAAAAAAAH48/TarV9Gu24M0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A33%3A08%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-h2QCjhFGF14/VQBuTcUmlOI/AAAAAAAAH48/TarV9Gu24M0/s1600/Screenshot%2B-%2B03112015%2B-%2B05%3A33%3A08%2BPM.png&#34; height=&#34;153&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The right wing is quite similar, except when it goes towards 0, it&#39;s not as flat, the left wing is much lower.&lt;br /&gt;&lt;br /&gt;Another important aspect is to reproduce Hagan&#39;s knee, the atm vols should produce a knee like curve, as different studies show (see for example &lt;a href=&#34;http://www-2.rotman.utoronto.ca/~hull/downloadablepublications/TreeBuilding.pdf&#34;&gt;this recent Hull &amp;amp; White study&lt;/a&gt; or this &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.740569&#34;&gt;other recent analysis by DeGuillaume&lt;/a&gt;). Using the same parameters as Hagan (beta=0, rho=0) leads to a nearly flat bpvol: no knee for the absolute SABR, curiously there is a bump at zero, possibly due to numerical difficulty with the spike in the density:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-H7PhpMWdy6U/VQB_m9TNZ0I/AAAAAAAAH5M/4yp7RMOwmo4/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A44%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-H7PhpMWdy6U/VQB_m9TNZ0I/AAAAAAAAH5M/4yp7RMOwmo4/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A44%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The problem is still there with beta = 0.1:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-ljZ11v0FIqw/VQB_m_hao1I/AAAAAAAAH5Q/Jpn7wgZ2Dwg/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A55%2BPM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-ljZ11v0FIqw/VQB_m_hao1I/AAAAAAAAH5Q/Jpn7wgZ2Dwg/s1600/Screenshot%2B-%2B03112015%2B-%2B06%3A46%3A55%2BPM.png&#34; height=&#34;191&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Overall, the idea of extending SABR to the full real line with the absolute value looks particularly simple, but it&#39;s not clear that it makes real financial sense.</description>
    </item>
    <item>
      <title>Variance swaps on a foreign asset</title>
      <link>https://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</link>
      <pubDate>Tue, 24 Feb 2015 13:50:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/variance-swaps-on-a-foreign-asset/</guid>
      <description>There is very little information on variance swaps on a foreign asset. There can be two kinds of contracts:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;one that pays the foreign variance in a domestic currency, this is a quanto contract as the exchange rate is implicitly fixed.&lt;/li&gt;&lt;li&gt;one that pays the foreign variance, multiplied by the fx rate at maturity. This is a flexo contract, and is just about buying a variance swap from a foreign bank. The price of such a contract today is very simple, just the standard variance swap price multiplied by the fx rate today (change of measure).&lt;/li&gt;&lt;/ul&gt;For quanto contracts, it&#39;s not so obvious a priori. If we consider a stochastic volatility model for the asset, the replication formula will not be applicable directly as the stochastic volatility will appear in the quanto drift correction. Furthermore, vanilla quanto option prices can not be computed simply as under Black-Scholes, a knowledge of the underlying model is necessary.&lt;br /&gt;&lt;br /&gt;Interestingly, under the Schobel-Zhu model, it is simple to fit an analytic formula for the quanto variance swap. The standard variance swap price is:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-gfq9trxBHhg/VOxxHi0khTI/AAAAAAAAH00/xD8NbXUSJls/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A33%3A17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-gfq9trxBHhg/VOxxHi0khTI/AAAAAAAAH00/xD8NbXUSJls/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A33%3A17.png&#34; height=&#34;50&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;The quanto variance swap can be priced with the same formula using a slightly different theta:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-OwYg8Hs9qRw/VOxxMe-iifI/AAAAAAAAH08/xBeV56UEGV8/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-OwYg8Hs9qRw/VOxxMe-iifI/AAAAAAAAH08/xBeV56UEGV8/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A06.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;We can use it to assess the accuracy of a naive quanto option replication where we use the ATM quanto forward instead of the forward in the variance swap replication formula.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-ate0oZ41eoY/VOxxhwqGI_I/AAAAAAAAH1E/6LRgNiAHCJg/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A52.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-ate0oZ41eoY/VOxxhwqGI_I/AAAAAAAAH1E/6LRgNiAHCJg/s1600/Screenshot%2B-%2B230215%2B-%2B19%3A34%3A52.png&#34; height=&#34;289&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Interestingly, the&amp;nbsp; quanto forward approximation turns out to be very accurate and the correction is important. The price without correction is the price with zero correlation, and we see it can be +/-5% off in this case.&lt;br /&gt;&lt;br /&gt;The local vol price seems a bit off, I am not sure exactly why. It could be due the discretization, the theoretical variance should be divided by (N-1) but here we divide by N where N is the number of observations. That would still lead to a skewed price but better centered around correlation 0.&lt;br /&gt;&lt;br /&gt;It&#39;s also a bit surprising that local vol is worse than the simpler ATM quanto forward approximation: it seems that it&#39;s extracting the wrong information to do a more precise quanto correction, likely related to the shift of stochastic volatility under the domestic measure.</description>
    </item>
    <item>
      <title>Jumps impact: Variance swap vs volatility swap</title>
      <link>https://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</link>
      <pubDate>Fri, 20 Feb 2015 13:24:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/jumps-impact-variance-swap-vs-volatility-swap/</guid>
      <description>&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20-%20200215%20-%2013%2013%2038.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;p&gt;Beside &lt;a href=&#34;https://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous&#34;&gt;the problem with the discreteness&lt;/a&gt; of the replication, variance swaps are sensitive to jumps. This is an often mentioned reason for the collapse of the single name variance swap market in 2008 as jumps are more likely on single name equities.&lt;/p&gt;&#xA;&lt;p&gt;Those graphs are the result of Monte-Carlo simulations with various jump sizes using the Bates model, and using Local Volatility implied from the Bates vanilla prices. The local volatility price will be the same price as per static replication for the variance swap, and we can see it they converge when there is no jump.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variance Swap Replication : Discrete or Continuous?</title>
      <link>https://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous/</link>
      <pubDate>Thu, 19 Feb 2015 18:45:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/variance-swap-replication--discrete-or-continuous/</guid>
      <description>People regularly believe that Variance swaps need to be priced by discrete replication, because the market trades only a discrete set of options.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-9dEW7QRFa7k/VOYguM8BHOI/AAAAAAAAH0Q/RPFxCeyq6nU/s1600/Screenshot%2B-%2B190215%2B-%2B18%3A36%3A26.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-9dEW7QRFa7k/VOYguM8BHOI/AAAAAAAAH0Q/RPFxCeyq6nU/s1600/Screenshot%2B-%2B190215%2B-%2B18%3A36%3A26.png&#34; height=&#34;340&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In reality, a discrete replication will misrepresent the tail, and can be quite arbitrary. It looks like the discrete replication as described in &lt;a href=&#34;http://bfi.cl/papers/Derman%201999%20-%20More%20about%20Variance%20Swaps.pdf&#34;&gt;Derman Goldman Sachs paper&lt;/a&gt; is in everybody&#39;s mind, probably because it&#39;s easy to grasp. Strangely, it looks like most forget the section &#34;Practical problems with replication&#34; on p27 of his paper, where you can understand that discrete replication is not all that practical.&lt;br /&gt;&lt;br /&gt;Reflecting on all of this, I noticed it was possible to create more accurate discrete replications easily, and that those can have vastly different hedging weights. It is a much better idea to just replicate the log payoff continuously with a decent model for interpolation and extrapolation and imply the hedge from the greeks.&lt;br /&gt;&lt;br /&gt;I wrote &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2567398&#34;&gt;a small paper around this here&lt;/a&gt;.</description>
    </item>
    <item>
      <title>Monte Carlo &amp; Inverse Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/monte-carlo--inverse-cumulative-normal-distribution/</link>
      <pubDate>Tue, 03 Feb 2015 14:53:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/monte-carlo--inverse-cumulative-normal-distribution/</guid>
      <description>In most financial Monte-Carlo simulations, there is the need of generating normally distributed random numbers. One technique is to use the inverse cumulative normal distribution function on uniform random numbers. There are several different popular numerical implementations:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Wichura AS241 (1988)&lt;/li&gt;&lt;li&gt;Moro &#34;The full Monte&#34; (1995)&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;http://home.online.no/~pjacklam/notes/invnorm/&#34;&gt;Acklam&lt;/a&gt; (2004)&lt;/li&gt;&lt;li&gt;&lt;a href=&#34;http://arxiv.org/abs/0901.0638&#34;&gt;Shaw breakless formula&lt;/a&gt; optimized for GPUs (2011) &lt;/li&gt;&lt;/ul&gt;W. Shaw has an excellent overview of the accuracy of the various methods in his paper &lt;i&gt;&lt;a href=&#34;http://www.mth.kcl.ac.uk/~shaww/web_page/papers/NormalQuantile1.pdf&#34;&gt;Refinement of the normal quantile&lt;/a&gt;&lt;/i&gt;.&lt;br /&gt;&lt;br /&gt;But what about performance? In Monte-Carlo, we could accept a slighly lower accuracy for an increase in performance.&lt;br /&gt;&lt;br /&gt;I tested the various methods on the Euler full truncation scheme for Heston using a small timestep (0.01). Here are the results with Sobol quasi-rng:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;AS241&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.9186256922511046 0.42s&lt;br /&gt;MORO &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0.9186256922459066 0.38s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ACKLAM &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0.9186256922549364 0.40s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ACKLAM REFINED 0.9186256922511045 2.57s&lt;br /&gt;SHAW-HYBRID &amp;nbsp;&amp;nbsp; 0.9186256922511048 0.68s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;In practice, the most accurate algorithm, AS241, is of comparable speed as the newer but less accurate algorithms of MORO and ACKLAM. Acklam refinement to go to double precision (which AS241 is) kills its performance.&lt;br /&gt;&lt;br /&gt;What about the Ziggurat on pseudo rng only? Here are the results with Mersenne-Twister-64, and using the Doornik implementation of the Ziggurat algorithm:&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-size: x-small;&#34;&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;AS241&amp;nbsp; 0.9231388565879476&amp;nbsp; 0.49s&lt;br /&gt;ZIGNOR 0.9321405648313437&amp;nbsp; 0.44s&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;There is a more optimized algorithm, VIZIGNOR, also from Doornik which should be a bit faster. As expected, the accuracy is quite lower than with Sobol, and the Ziggurat looks worse. This is easily visible if one plots the implied volatilities as a function of the spot for AS241 and for ZIGNOR.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-lITlDFhF-cE/VNDQfqtNbTI/AAAAAAAAHzU/zki5VJADyv4/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A43%3A10.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-lITlDFhF-cE/VNDQfqtNbTI/AAAAAAAAHzU/zki5VJADyv4/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A43%3A10.png&#34; height=&#34;321&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;AS241 implied volatility on Mersenne-Twister&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-QxKOGzNMSXE/VNDQp7dL0EI/AAAAAAAAHzc/wm1c-ymLYww/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A18%3A51.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-QxKOGzNMSXE/VNDQp7dL0EI/AAAAAAAAHzc/wm1c-ymLYww/s1600/Screenshot%2Bfrom%2B2015-02-03%2B14%3A18%3A51.png&#34; height=&#34;321&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;ZIGNOR implied volatility on Mersenne-Twister&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Zignor is much noisier.&lt;br /&gt;&lt;br /&gt;Note the slight bump in the scheme EULER-FT-BK that appears because the scheme, that approximates the Broadie-Kaya integrals with a trapeze (as in Andersen QE paper), does not respect martingality that well compared to the standard full truncated Euler scheme EULER-FT, and the slightly improved EULER-FT-MID where the variance integrals are approximated by a trapeze as in Van Haastrecht paper on Schobel-Zhu:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-pIO5C8vN1Es/VNDSPriO-OI/AAAAAAAAHzo/d0DUYBjiG8Q/s1600/Screenshot%2B-%2B030215%2B-%2B14%3A49%3A29.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-pIO5C8vN1Es/VNDSPriO-OI/AAAAAAAAHzo/d0DUYBjiG8Q/s1600/Screenshot%2B-%2B030215%2B-%2B14%3A49%3A29.png&#34; height=&#34;76&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;This allows to leak less correlation than the standard full truncated Euler.</description>
    </item>
    <item>
      <title>Local Stochastic Volatility - Particles and Bins</title>
      <link>https://chasethedevil.github.io/post/local-stochastic-volatility---particles-and-bins/</link>
      <pubDate>Fri, 30 Jan 2015 12:03:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/local-stochastic-volatility---particles-and-bins/</guid>
      <description>&lt;p&gt;In an &lt;a href=&#34;https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo&#34;&gt;earlier post&lt;/a&gt;, I mentioned the similarities between the Guyon-Labordere &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1885032&#34;&gt;particle method&lt;/a&gt; and the Vanderstoep-Grzelak-Oosterlee &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDIQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fabstract%3D2278122&amp;amp;ei=255eUqaEDMaxhAfdqoBI&amp;amp;usg=AFQjCNF2KqSTT2ouvAyiA2J77foOFTzMKw&amp;amp;sig2=fzb4vlDPp49Hp1oT5Wja4A&amp;amp;bvm=bv.54176721,d.ZG4&#34;&gt;&amp;ldquo;bin&amp;rdquo; method&lt;/a&gt; to calibrate and price under Local Stochastic volatility. I will be a bit more precise here. &lt;strong&gt;The same thing, really&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;p&gt;The particle method can be seen as a generalization of the &amp;ldquo;bin&amp;rdquo; method. In deed, the bin method consists in doing the particle method using a histogram estimation of the conditional variance. The histogram estimation can be more or less seen as a very basic rectangle kernel with the appropriate bandwidth. The &amp;ldquo;bin&amp;rdquo; method is then just the particle method with another kernel (wiki link) (in the particle method, the kernel is a quartic with bandwidth defined by some slightly elaborate formula). A very good paper on this is Silverman &lt;em&gt;&lt;a href=&#34;https://ned.ipac.caltech.edu/level5/March02/Silverman/paper.pdf&#34;&gt;Density estimation for statistics and data analysis&lt;/a&gt;&lt;/em&gt;, referenced by Guyon-Labordere.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Flat Volatility Surfaces &amp; Discrete Dividends</title>
      <link>https://chasethedevil.github.io/post/flat-volatility-surfaces--discrete-dividends/</link>
      <pubDate>Tue, 25 Nov 2014 13:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/flat-volatility-surfaces--discrete-dividends/</guid>
      <description>In papers around volatility and cash (discrete) dividends, we often encounter the example of the flat volatility surface. For example, the &lt;a href=&#34;http://www.opengamma.com/sites/default/files/equity-variance-swaps-dividends-opengamma.pdf&#34;&gt;OpenGamma paper&lt;/a&gt; presents this graph:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-AuaTFyvjgVA/VHRxUid4HzI/AAAAAAAAHjs/T4PAQTnUBN8/s1600/Screenshot%2Bfrom%2B2014-11-25%2B12%3A59%3A09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-AuaTFyvjgVA/VHRxUid4HzI/AAAAAAAAHjs/T4PAQTnUBN8/s1600/Screenshot%2Bfrom%2B2014-11-25%2B12%3A59%3A09.png&#34; height=&#34;167&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It shows that if the Black volatility surface is fully flat, there are jumps in the pure volatility surface (corresponding to a process that includes discrete dividends in a consistent manner) at the dividend dates or equivalently if the pure volatility surface is flat, the Black volatility jumps.&lt;br /&gt;&lt;br /&gt;This can be traced to the fact that the Black formula does not respect C(S,K,Td-) = C(S,K-d,Td) as the forward drops from F(Td-) to F(Td-)-d where d is dividend amount at td, the dividend ex date.&lt;br /&gt;&lt;br /&gt;Unfortunately, those examples are not very helpful. In practice, the market observables are just Black volatility points, which can be interpolated to volatility slices for each expiry without regards to dividends, not a full volatility surface. Discrete dividends will mostly happen between two slices: the Black volatility jump will happen on some time-interpolated data.&lt;br /&gt;&lt;br /&gt;While the jump size is known (it must obey to the call price continuity), the question of how one should interpolate that data until the jump is far from trivial even using two flat Black volatility slices.&lt;br /&gt;&lt;br /&gt;The most logical is to consider a model that includes discrete dividends consistently. For example, one can fully lookup the Black volatility corresponding the price of an option assuming a piecewise lognormal process with jumps at the dividend dates. It can be priced by applying a finite difference method on the PDE. Alternatively, &lt;a href=&#34;http://www.risk.net/risk-magazine/technical-paper/1530307/finessing-fixed-dividends&#34;&gt;Bos &amp;amp; Vandermark&lt;/a&gt; propose a simple spot and strike adjusted Black formula that obey the continuity requirement (the Lehman model), which, in practice, stays quite close to the piecewise lognormal model price. Another possibility is to rely on a forward modelling of the dividends, as in &lt;a href=&#34;http://www.quantitative-research.de/dl/Dividends_And_Volatility.pdf&#34;&gt;Buehler&lt;/a&gt; (if one is comfortable with the idea that the option price will then depend ultimately on dividends past the option expiry).&lt;br /&gt;&lt;br /&gt;Recently, a &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1002/wilm.10112/abstract&#34;&gt;Wilmott article&lt;/a&gt; suggested to only rely on the jump adjustment, but did not really mention how to find the volatility just before or just after the dividend. Here is an illustration of how those assumptions can change the volatility in between slices using two dividends at T=0.9 and T=1.1.&lt;br /&gt;&lt;br /&gt;In the first graph, we just interpolate linearly in forward moneyness the pure vol from the Bos &amp;amp; Vandermark formula, as it should be continuous with the forward (the PDE would give nearly the same result) and compute the equivalent Black volatility (and thus the jump at the dividend dates).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-Pye5KeoR16M/VHR1WACQD3I/AAAAAAAAHj4/h65Vpj4mMjI/s1600/bos_2_div_flat.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-Pye5KeoR16M/VHR1WACQD3I/AAAAAAAAHj4/h65Vpj4mMjI/s1600/bos_2_div_flat.png&#34; height=&#34;300&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the second graph, we interpolate linearly the two Black slices, until we find a dividend, at which point we impose the jump condition and repeat the process until the next slice. We process forward (while the Wilmott article processes backward) as it seemed a bit more natural to make the interpolation not depend on future dividends. Processing backward would just make the last part flat and first part down-slopping. On this example backward would be closer to the Bos Black volatility, but when the dividends are near the first slice, the opposite becomes true.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-ScSlBHCBoWc/VHR1eOigrXI/AAAAAAAAHkA/3HJ9zRQvguA/s1600/blackjump_2_div_flat.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-ScSlBHCBoWc/VHR1eOigrXI/AAAAAAAAHkA/3HJ9zRQvguA/s1600/blackjump_2_div_flat.png&#34; height=&#34;300&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;While the scale of those changes is not that large on the example considered, the choice can make quite a difference in the price of structures that depend on the volatility in between slices. A recent example I encountered is the variance swap when one includes adjustment for discrete dividends (then the prices just after the dividend date are used).&lt;br /&gt;&lt;br /&gt;To conclude, if one wants to use the classic Black formula everywhere, the volatility must jump at the dividend dates. Interpolation in time is then not straightforward and one will need to rely on a consistent model to interpolate. It is not exactly clear then why would anyone stay with the Black formula except familiarity.</description>
    </item>
    <item>
      <title>Machine Learning &amp; Quantitative Finance</title>
      <link>https://chasethedevil.github.io/post/machine-learning--quantitative-finance/</link>
      <pubDate>Tue, 18 Nov 2014 12:34:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/machine-learning--quantitative-finance/</guid>
      <description>&lt;p&gt;There is an interesting course on &lt;a href=&#34;https://class.coursera.org/ml-007/lecture&#34;&gt;Machine Learning on Coursera&lt;/a&gt;, it does not require much knowledge and yet manages to teach quite a lot.&lt;/p&gt;&#xA;&lt;p&gt;I was struck by the fact that most techniques and ideas apply also to problems in quantitative finance.&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Linear regression: used for example in the Longstaff-Schwartz approach to price Bermudan options with Monte-Carlo. Interestingly the teacher insists on feature normalization, something we can forget easily, especially with the polynomial features.&lt;/li&gt;&#xA;&lt;li&gt;Gradient descent: one of the most basic minimizer and we use minimizers all the time for model calibration.&lt;/li&gt;&#xA;&lt;li&gt;Regularization: in finance, this is sometimes used to smooth out the volatility surface, or can be useful to add stability in calibration. The lessons are very practical, they explain well how to find the right value of the regularization parameter.&lt;/li&gt;&#xA;&lt;li&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;Neural networks&lt;/a&gt;: calibrating a model is very much like training a neural network. The &lt;a href=&#34;http://en.wikipedia.org/wiki/Backpropagation&#34;&gt;backpropagation&lt;/a&gt; is the same thing as the adjoint differentiation. It&amp;rsquo;s very interesting to see that it is a key feature for Neural networks, otherwise training would be much too slow and Neural networks would not be practical. Once the network is trained, it is evaluated relatively quickly forward. It&amp;rsquo;s basically the same thing as calibration and then pricing.&lt;/li&gt;&#xA;&lt;li&gt;Support vector machines: A gaussian kernel is often used to represent the frontier. We find the same idea in the particle Monte-Carlo method.&lt;/li&gt;&#xA;&lt;li&gt;Principal component analysis: can be applied to the covariance matrix square root in Monte-Carlo simulations, or to &amp;ldquo;compress&amp;rdquo; large baskets, as well as for portfolio risk.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;It&amp;rsquo;s also interesting to hear the teacher repeating that people should not try possible improvements at random (often because they have only one idea) but analyze before what makes the most sense. And that can imply digging in the details, looking at what&amp;rsquo;s going on 100 samples.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pseudo-Random vs Quasi-Random Numbers</title>
      <link>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</link>
      <pubDate>Wed, 12 Nov 2014 17:05:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/pseudo-random-vs-quasi-random-numbers/</guid>
      <description>Quasi-Random numbers (like &lt;a href=&#34;http://en.wikipedia.org/wiki/Sobol_sequence&#34;&gt;Sobol&lt;/a&gt;) are a relatively popular way in finance to improve the Monte-Carlo convergence compared to more classic Pseudo-Random numbers (like &lt;a href=&#34;http://en.wikipedia.org/wiki/Mersenne_twister&#34;&gt;Mersenne-Twister&lt;/a&gt;). Behind the scenes one has to be a bit more careful about the dimension of the problem as the Quasi-Random numbers depends on the dimension (defined by how many random variables are independent from each other).&lt;br /&gt;&lt;br /&gt;For a long time, Sobol was limited to 40 dimensions using the so called Bratley-Fox direction numbers (his paper actually gives the numbers for 50 dimensions). Later Lemieux gave direction numbers for up to 360 dimensions. Then, P. Jäckel proposed some extension with a random initialization of the direction vectors in his book from 2006. And finally Joe &amp;amp; Kuo published direction numbers for up to 21200 dimensions.&lt;br /&gt;&lt;br /&gt;But there are very few studies about how good are real world simulations with so many quasi-random dimensions. A recent paper &#34;&lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2210420&#34;&gt;Fast Ninomiya-Victoir Calibration of the Double-Mean-Reverting Model&lt;/a&gt;&#34; by Bayer, Gatheral &amp;amp; Karlsmark tests this for once, and the results are not so pretty:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;https://3.bp.blogspot.com/-aYBusg02Kr0/VGOAlrsHGjI/AAAAAAAAHis/o4zfFf8-5hA/s1600/Screenshot%2Bfrom%2B2014-11-12%2B16%3A15%3A17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;342&#34; src=&#34;https://3.bp.blogspot.com/-aYBusg02Kr0/VGOAlrsHGjI/AAAAAAAAHis/o4zfFf8-5hA/s640/Screenshot%2Bfrom%2B2014-11-12%2B16%3A15%3A17.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;With their model, the convergence with Sobol numbers becomes worse when the number of time-steps increases, that is when the number of dimension increases. There seems to be even a threshold around 100 time steps (=300 dimensions for Euler) beyond which a much higher number of paths (2^13) is necessary to restore a proper convergence. And they use the latest and greatest Joe-Kuo direction numbers.&lt;br /&gt;&lt;br /&gt;Still the total number of paths is not that high compared to what I am usually using (2^13 = 8192). It&#39;s an interesting aspect of their paper: the calibration with a low number of paths.</description>
    </item>
    <item>
      <title>Integrating an oscillatory function</title>
      <link>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</link>
      <pubDate>Wed, 05 Nov 2014 16:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/integrating-an-oscillatory-function/</guid>
      <description>Recently, some instabilities were noticed in the Carr-Lee seasoned volatility swap price in some situations. &lt;br /&gt;&lt;br /&gt;The &lt;a href=&#34;https://math.nyu.edu/financial_mathematics/content/02_financial/2008-3.pdf&#34;&gt;Carr-Lee&lt;/a&gt; seasoned volatility swap price involve the computation of a double integral. The inner integral is really the problematic one as the integrand can be highly oscillating.&lt;br /&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-9Fh24CvDs_4/VFpCjz8_sMI/AAAAAAAAHig/Q0iTCTb3f9E/s1600/Screenshot%2Bfrom%2B2014-11-05%2B16%3A30%3A00.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-9Fh24CvDs_4/VFpCjz8_sMI/AAAAAAAAHig/Q0iTCTb3f9E/s1600/Screenshot%2Bfrom%2B2014-11-05%2B16%3A30%3A00.png&#34; height=&#34;151&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;br /&gt;I&amp;nbsp; first found a somewhat stable behavior using a specific adaptive Gauss-Lobatto implementation (&lt;a href=&#34;http://www.ii.uib.no/%7Eterje/Papers/bit2003.pdf&#34;&gt;the one from Espelid&lt;/a&gt;) and a change of variable. But it was not very satisfying to see that the outer integral was stable only with another specific adaptive Gauss-Lobatto (the one from Gander &amp;amp; Gauschi, present in Quantlib). I tried various choices of adaptive (coteda, modsim, adaptsim,...) or brute force trapezoidal integration, but either they were order of magnitudes slower or unstable in some cases. Just using the same Gauss-Lobatto implementation for both would fail...&lt;br /&gt;&lt;br /&gt;I then noticed you could write the integral as a Fourier transform as well, allowing the use of FFT. Unfortunately, while this worked, it turned out to require a very large number of points for a reasonable accuracy. This, plus the tricky part of defining the proper step size, makes the method not so practical.&lt;br /&gt;&lt;br /&gt;I had heard before of the &lt;a href=&#34;http://www.cs.berkeley.edu/~fateman/papers/oscillate.pdf&#34;&gt;Filon quadrature&lt;/a&gt;, which I thought was more of a curiosity. The main idea is to integrate exactly x^n * cos(k*x). One then relies on a piecewise parabolic approximation of the function f to integrate f(x) * cos(k*x). Interestingly, a very similar idea has been used in the &lt;a href=&#34;http://www.risk.net/risk-magazine/technical-paper/1500323/cutting-edges-domain-integration&#34;&gt;Sali quadrature method&lt;/a&gt; for option pricing, except one integrates exactly x^n * exp(-k*x^2).&lt;br /&gt;&lt;br /&gt;It turned out to be remarkable on that problem, combined with a &lt;a href=&#34;http://en.wikipedia.org/wiki/Adaptive_Simpson%27s_method&#34;&gt;simple adaptive Simpson&lt;/a&gt; like method to find the right discretization. Then as if by magic, any outer integration quadrature worked. &lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>The elusive reference: the Lamperti transform</title>
      <link>https://chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</link>
      <pubDate>Mon, 03 Nov 2014 11:23:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-elusive-reference-the-lamperti-transform/</guid>
      <description>&lt;p&gt;Without knowing that it was a well known general concept, I first noticed the use of the Lamperti transform in the Andersen-Piterbarg &amp;ldquo;Interest rate modeling&amp;rdquo; book p.292 &amp;ldquo;finite difference solutions for general phi&amp;rdquo;.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202014-11-03%2010%2055%2001.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;Pat Hagan used that transformation for a better discretization of the &lt;a href=&#34;https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde--spline-interpolation&#34;&gt;arbitrage free SABR PDE model&lt;/a&gt;.I then started to notice the use of this transformation in many more papers. The first one I saw naming it &amp;ldquo;Lamperti transform&amp;rdquo; was the paper from Ait-Sahalia  &lt;a href=&#34;http://www.princeton.edu/~yacine/mle.pdf&#34;&gt;Maximum likelyhood estimation of discretely sampled diffusions: a closed-form approximation approach&lt;/a&gt;. Recently those closed form formulae have been applied to the quadrature method (where one integrates the transition density by a quadrature rule) in &amp;ldquo;Advancing the universality of quadrature methods to any underlying process for option pricing&amp;rdquo;. There is also a recent interesting application to Monte-Carlo simulation in &amp;ldquo;&lt;a href=&#34;http://www-leland.stanford.edu/~glynn/papers/2013/RheeG13a.pdf&#34;&gt;Unbiased Estimation with Square Root Convergence for SDE Models&lt;/a&gt;&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Barrier options under negative rates: complex numbers to the rescue</title>
      <link>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</link>
      <pubDate>Thu, 02 Oct 2014 11:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/barrier-options-under-negative-rates-complex-numbers-to-the-rescue/</guid>
      <description>I stumbled upon an unexpected problem: the &lt;a href=&#34;http://books.google.com/books?id=FU7gam7ZqVsC&amp;amp;q=haug+binary+barrier&amp;amp;dq=haug+binary+barrier&amp;amp;hl=en&amp;amp;sa=X&amp;amp;ei=QyAtVITAGdjdatPxgMAO&amp;amp;ved=0CB0Q6AEwAA&#34;&gt;one touch barrier formula&lt;/a&gt; can break down under negative rates. While negative rates can sound fancy, they are actually quite real on some markets. Combined with relatively low volatilities, this makes the standard Black-Scholes one touch barrier formula blow up because somewhere the square root of a negative number is taken.&lt;br /&gt;&lt;br /&gt;At first, I had the idea to just floor the number to 0. But then I needed to see if this rough approximation would be acceptable or not. So I relied on a &lt;a href=&#34;http://www.risk.net/journal-of-computational-finance/technical-paper/2330321/tr-bdf2-for-fast-stable-american-option-pricing&#34;&gt;TR-BDF2&lt;/a&gt; discretization of the Black-Scholes PDE, where negative rates are not a problem.&lt;br /&gt;&lt;br /&gt;Later, I was convinced that we ought to be able to find a closed form formula for the case of negative rates. I went back to the derivation of the formula, &lt;a href=&#34;http://books.google.fr/books?id=2sGwSAfA8eAC&amp;amp;lpg=PA278&amp;amp;dq=kwok%20barrier&amp;amp;pg=PA193#v=onepage&amp;amp;q&amp;amp;f=false&#34;&gt;the book from Kwok&lt;/a&gt; is quite good on that. The closed form formula just stems from being the solution of an integral of the first passage time density (which is a simpler way to compute the one touch price than the PDE approach). It turns out that, then, the closed form solution to this integral with negative rates is just the same formula with complex numbers (there are actually some simplifications then).&lt;br /&gt;&lt;br /&gt;It is a bit uncommon to use the cumulative normal distribution on complex numbers, but the error function on complex numbers is more popular: it&#39;s actually even on &lt;a href=&#34;http://en.wikipedia.org/wiki/Error_function&#34;&gt;the wikipedia page of the error function&lt;/a&gt;. And it can be computed very quickly with machine precision thanks to the &lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package&#34;&gt;Faddeeva library&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;With this simple closed form formula, there is no need anymore for an approximation. I wrote &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2501907&#34;&gt;a small paper around this here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Later a collegue made the remark that it could be interesting to have the bivariate complex normal distribution for the case of partial start one touch options or partial barrier option rebates (not sure if those are common). Unfortunately I could not find any code or paper for this. And after asking Prof. Genz (who found a very elegant and fast algorithm for the bivariate normal distribution), it looks like an open problem.</description>
    </item>
    <item>
      <title>Initial Guesses for SVI - A Summary</title>
      <link>https://chasethedevil.github.io/post/initial-guesses-for-svi---a-summary/</link>
      <pubDate>Fri, 26 Sep 2014 10:46:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/initial-guesses-for-svi---a-summary/</guid>
      <description>&lt;p&gt;I have been looking at various ways of finding initial guesses for SVI calibration (&lt;a href=&#34;https://chasethedevil.github.io/post/another-svi-initial-guess&#34;&gt;Another SVI Initial Guess&lt;/a&gt;, &lt;a href=&#34;https://chasethedevil.github.io/post/more-svi-initial-guesses&#34;&gt;More SVI Initial Guesses&lt;/a&gt;, &lt;a href=&#34;https://chasethedevil.github.io/post/svi-and-long-maturities-issues&#34;&gt;SVI and long maturities issues&lt;/a&gt;). I decided to write &lt;a href=&#34;http://papers.ssrn.com/abstract=2501898&#34;&gt;a paper&lt;/a&gt; summarizing this. I find that the process of writing a paper makes me think more carefully about a problem.&lt;/p&gt;&#xA;&lt;p&gt;In this case, it turns out that the Vogt initial guess method (guess via asymptotes and minimum variance) is actually very good as long as one has a good way to lookup the asymptotes (the data is not always convex, while SVI is) and as long as rho is not close to -1, that is for long maturity affine like smiles, where SVI is actually more difficult to calibrate properly due to the over-parameterisation in those cases.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Asymptotic Behavior of SVI vs SABR</title>
      <link>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</link>
      <pubDate>Tue, 23 Sep 2014 12:06:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/asymptotic-behavior-of-svi-vs-sabr/</guid>
      <description>The variance under SVI becomes linear when the log-moneyness is very large in absolute terms. The lognormal SABR formula with beta=0 or beta=1 has a very different behavior. Of course, the theoretical SABR model has actually a different asymptotic behavior.&lt;br /&gt;&lt;br /&gt;As an illustration, we calibrate SABR (with two different values of beta) and SVI against the same implied volatility slice and look at the wings behavior. &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-1w0jjvR9-Mk/VCFFhSiOcdI/AAAAAAAAHg4/E3yP_m3vhKA/s1600/Screenshot%2Bfrom%2B2014-09-23%2B11%3A52%3A07.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-1w0jjvR9-Mk/VCFFhSiOcdI/AAAAAAAAHg4/E3yP_m3vhKA/s1600/Screenshot%2Bfrom%2B2014-09-23%2B11%3A52%3A07.png&#34; height=&#34;497&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;While the Lee moments formula implies that the variance should be at most linear, something that the SABR formula does not respect. It is in practice not the problem with SABR as the actual Lee boundary: V(x) &amp;lt; 2|x|/T (where V is the square of the implied volatility and x the log-moneyness) is attained for extremely low strikes only with SABR, except maybe for very long maturities.&lt;br /&gt;&lt;br /&gt;A related behavior is the fact that the lognormal SABR formula can actually match steeper curvatures at the money than SVI for given asymptotes.</description>
    </item>
    <item>
      <title>SVI and long maturities issues</title>
      <link>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</link>
      <pubDate>Fri, 01 Aug 2014 12:51:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/svi-and-long-maturities-issues/</guid>
      <description>&lt;p&gt;On long maturities equity options, the smile is usually very much like a skew: very little curvature. This usually means that the SVI rho will be very close to -1, in a similar fashion as what can happen for the the correlation parameter of a real stochastic volatility model (Heston, SABR).&lt;/p&gt;&#xA;&lt;p&gt;In terms of initial guess, &lt;a href=&#34;https://chasethedevil.github.io/post/another-svi-initial-guess&#34;&gt;I looked&lt;/a&gt; at the more usual use cases and showed that matching a parabola at the minimum variance point often leads to a decent initial guess if one has an ok estimate of the wings. We will see here that we can do also something a bit better than just a flat slice at-the-money in the case where rho is close to -1.&lt;/p&gt;</description>
    </item>
    <item>
      <title>More SVI Initial Guesses</title>
      <link>https://chasethedevil.github.io/post/more-svi-initial-guesses/</link>
      <pubDate>Thu, 31 Jul 2014 14:54:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/more-svi-initial-guesses/</guid>
      <description>In the previous post, I showed one could extract the SVI parameters from a best fit parabola at-the-money. It seemed to work reasonably well, but I found some real market data where it can be much less satisfying.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png&#34; height=&#34;588&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;Sometimes (actually not so rarely) the ATM slope and curvatures can&#39;t be matched given rho and b found through the asymptotes. As a result if I force to just match the curvature and set m=0 (when the slope can&#39;t be matched), the simple ATM parabolic guess looks shifted. It can be much worse than this specific example.&lt;br /&gt;&lt;br /&gt;It is then a bit clearer why Vogt looked to match the lowest variance instead of ATM. We can actually also fit a parabola at the lowest variance (MV suffix in the graph) instead of ATM. It seems to fit generally better.&lt;br /&gt;&lt;br /&gt;I also tried to estimate the asymptotic slopes more precisely (using the slope of the 5-points parabola at each end), but it seems to not always be an improvement.&lt;br /&gt;&lt;br /&gt;However this does not work when rho is close to -1 or 1 as there is then no minimum. Often, matching ATM also does not work when rho is -1 or 1. This specific case, but quite common as well for longer expiries in equities need more thoughts, usually a constant slice is ok, but this is clearly where Zeliade&#39;s quasi explicit method shines.&lt;br /&gt;&lt;br /&gt;So far it still all looks good, but then looking at medium maturities (1 year), sometimes all initial guesses don&#39;t look comforting (although Levenberg-Marquardt minimization still works on those - but one can easily imagine that it can break as well, for example by tweaking slightly the rho/b and look at what happens then).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png&#34; height=&#34;640&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;There is lots of data on this 1 year example. One can clearly see the problem when the slope can not be fitted ATM (SimpleParabolicATM-guess), and even if by chance when it can (TripleParabolicATM-guess), it&#39;s not so great.&lt;br /&gt;Similarly fitting the lowest variance leads only to a good fit of the right wing and a bad fit everywhere else.&lt;br /&gt;&lt;br /&gt;Still, as if by miracle, everything converges to the best fit on this example (again one can find cases where some guesses don&#39;t converge to the best fit). I have added some weights +-20% around the money, to ensure that we capture the ATM behavior accurately (otherwise the best fit is funny).&lt;br /&gt;&lt;br /&gt;It is interesting to see that if one minimizes the min square sum of variances (what I do in Vogt-LM, it&#39;s in theory slightly faster as there is no sqrt function cost) it results in an ugly looking steeper curvature, while if we just minimize the min square sum of volatilities (what I do in SimpleParabolicMV_LM), it looks better.</description>
    </item>
    <item>
      <title>Another SVI Initial Guess</title>
      <link>https://chasethedevil.github.io/post/another-svi-initial-guess/</link>
      <pubDate>Tue, 29 Jul 2014 14:39:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/another-svi-initial-guess/</guid>
      <description>The SVI formula is:&lt;br /&gt;$$w(k) = a + b ( \rho (k-m) + \sqrt{(k-m)^2+ \sigma^2}$$&lt;br /&gt;where k is the log-moneyness, w(k) the implied variance at a given moneyness and a,b,rho,m,sigma the 5 SVI parameters.&lt;br /&gt;&lt;br /&gt;A. Vogt described a particularly simple way to find an initial guess to fit SVI to an implied volatility slice &lt;a href=&#34;http://www.nuclearphynance.com/User%20Files/66/GatheralSmile_estimation_for_one_expiry_NP.pdf&#34; target=&#34;_blank&#34;&gt;a while ago&lt;/a&gt;. The idea to compute rho and sigma from the left and right asymptotic slopes. a,m are recovered from the crossing point of the asymptotes and sigma using the minimum variance.&lt;br /&gt;&lt;br /&gt;Later, &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CB4QFjAA&amp;amp;url=http%3A%2F%2Fwww.zeliade.com%2Fwhitepapers%2Fzwp-0005.pdf&amp;amp;ei=xI3XU9T3OYme7AbzwoHQDw&amp;amp;usg=AFQjCNGsvbseObGCDAZ1QbYtvOL9J2-aRw&amp;amp;sig2=rz4SilY2q1RSuD9XgWKFig&amp;amp;bvm=bv.71778758,d.ZGU&#34; target=&#34;_blank&#34;&gt;Zeliade has shown&lt;/a&gt; a very nice reduction of the problem to 2 variables, while the remaining 3 can be deduced explicitly. The practical side is that constraints are automatically included, the less practical side is the choice of minimizer for the two variables (Nelder-Mead) and of initial guess (a few random points).&lt;br /&gt;&lt;br /&gt;Instead, a simple alternative is the following: given b and rho from the asymptotic slopes, one could also just fit a parabola at-the-money, in a similar spirit as the &lt;a href=&#34;http://papers.ssrn.com/abstract=2467231&#34; target=&#34;_blank&#34;&gt;explicit SABR calibration&lt;/a&gt;, and recover explicitly a, m and sigma.&lt;br /&gt;&lt;br /&gt;To illustrate I take the data from Zeliade, where the input is already some SVI fit to market data.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png&#34; height=&#34;385&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;3M expiry - Zeliade data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png&#34; height=&#34;391&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;4Y expiry, Zeliade data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;We clearly see that ATM the fit is better for the parabolic initial guess than for Vogt, but as one goes further away from ATM, Vogt guess seems better.&lt;br /&gt;&lt;br /&gt;Compared to SABR, the parabola itself fits decently only very close to ATM. If one computes the higher order Taylor expansion of SVI around k=0, powers of (k/sigma) appear, while sigma is often relatively small especially for short expiries: the fourth derivative will quickly make a difference.&lt;br /&gt;&lt;br /&gt;On implied volatilities stemming from a SABR fit of the SP500, here is how the various methods behave:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png&#34; height=&#34;383&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;1M expiry on SABR data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png&#34; height=&#34;383&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;4Y expiry on SABR data&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;As expected, because SABR (and thus the input implied vol) is much closer to a parabola, the parabolic initial guess is much better than Vogt. The initial guess of Vogt is particularly bad on long expiries, although it will still converge quite quickly to the true minimum with Levenberg-Marquardt.&lt;br /&gt;&lt;br /&gt;In practice, I have found the method of Zeliade to be very robust, even if a bit slower than Vogt, while Vogt can sometimes (rarely) be too sensitive to the estimate of the asymptotes.&lt;br /&gt;&lt;br /&gt;The parabolic guess method could also be applied to always fit exactly ATM vol, slope and curvature, and calibrate rho, b to gives the best overall fit. It might be an idea for the next blog post.</description>
    </item>
    <item>
      <title>New SABR Formulae</title>
      <link>https://chasethedevil.github.io/post/new-sabr-formulae/</link>
      <pubDate>Wed, 16 Jul 2014 22:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/new-sabr-formulae/</guid>
      <description>In a talk at the Global Derivatives conference of Amsterdam (2014), Pat Hagan presented some new SABR formulas, supposedly close to the arbitrage free PDE behavior.&lt;br /&gt;&lt;br /&gt;I tried to code those from the slides, but somehow that did not work out well on his example, I just had something very close to the good old SABR formulas. I am not 100% sure (only 99%) that it is due to a mistake in my code. Here is what I was looking to reproduce:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-gSpOYAndMkk/U8bfHfczKEI/AAAAAAAAHZ8/-eBE_igGVgw/s1600/Screenshot+from+2014-07-16+22:21:08.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-gSpOYAndMkk/U8bfHfczKEI/AAAAAAAAHZ8/-eBE_igGVgw/s1600/Screenshot+from+2014-07-16+22:21:08.png&#34; height=&#34;328&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Pat Hagan Global Derivatives example&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Fortunately, I then found in some thesis the idea of using Andersen &amp;amp; Brotherton-Ratcliffe local volatility expansion. In deed, the arbitrage free PDE from Hagan is equivalent to some Dupire local volatility forward PDE (see &lt;a href=&#34;http://papers.ssrn.com/abstract=2402001&#34;&gt;http://papers.ssrn.com/abstract=2402001&lt;/a&gt;), so Hagan just gave us the local volatility expansion to expand on&amp;nbsp; (the thesis uses Doust, which is not so different in this case).&lt;br /&gt;&lt;br /&gt;And then it produces on this global derivatives example the following:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-xH6i6aZ0O0A/U8bhUO5EQlI/AAAAAAAAHaI/oobJM0YaWQs/s1600/Screenshot+-+07162014+-+10:32:04+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-xH6i6aZ0O0A/U8bhUO5EQlI/AAAAAAAAHaI/oobJM0YaWQs/s1600/Screenshot+-+07162014+-+10:32:04+PM.png&#34; height=&#34;267&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The AB suffix are the new SABR formula. Even though the formulas are different, that looks very much like Hagan&#39;s own illustration (with a better scale)!&lt;br /&gt;&lt;br /&gt;I have a draft paper around this and more practical ideas to calibrate SABR:&lt;br /&gt;&lt;a href=&#34;http://papers.ssrn.com/abstract=2467231&#34;&gt;http://papers.ssrn.com/abstract=2467231&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Heston or Schobel-Zhu issues with short expiries</title>
      <link>https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/</link>
      <pubDate>Thu, 03 Jul 2014 23:28:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/</guid>
      <description>It&#39;s relatively well known that Heston does not fit the market for short expiries. Given that there are just 5 parameters to fit a full surface, it&#39;s almost logical that one part of the surface of it is not going to fit well the market.&lt;br /&gt;I was more surprised to see how bad Heston or Schobel-Zhu were to fit a single short expiry volatility slice. As an example I looked at SP500 options with 1 week expiry. It does not really matter if one forces kappa and rho to constant values (even to 0) the behavior is the same and the error in fit does not change much.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png&#34; height=&#34;400&#34; width=&#34;390&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Schobel-Zhu fit for a slice of maturity 1 week&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;In this graph, the brown, green and red smiles corresponds to Schobel-Zhu fit using an explicit guess (matching skew &amp;amp; curvature ATM), using Levenberg-Marquardt on this guess, and using plain differential evolution. &lt;br /&gt;What happens is that the smiles flattens to quickly in the strike dimension. One consequence is that the implied volatility can not be computed for extreme strikes: the smile being too low, the price becomes extremely small, under machine epsilon and the numerical method (Cos) fails. There is also a bogus angle in the right wing, because of numerical error. I paid attention to ignore too small prices in the calibration by truncating the initial data.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png&#34; height=&#34;400&#34; width=&#34;391&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Heston fit, with Lord-Kahl (exact wings)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;SABR behaves much better (fixing beta=1 in this case) in comparison (As I use the same truncation as for Schobel-Zhu, the flat left wing part is ignored). &lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png&#34; height=&#34;400&#34; width=&#34;391&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;SABR fit for a slice of maturity 1 week&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;For longer expiries, Heston &amp;amp; Schobel-Zhu, even limited to 3 parameters, actually give a better fit in general than SABR.</description>
    </item>
    <item>
      <title>Moore-Penrose Inverse &amp; Gauss-Newton SABR Minimization</title>
      <link>https://chasethedevil.github.io/post/moore-penrose-inverse--gauss-newton-sabr-minimization/</link>
      <pubDate>Tue, 24 Jun 2014 15:29:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/moore-penrose-inverse--gauss-newton-sabr-minimization/</guid>
      <description>I have found a particularly nice initial guess to calibrate SABR. As it is quite close to the true best fit, it is tempting to use a very simple minimizer to go to the best fit. Levenberg-Marquardt works well on this problem, but can we shave off a few iterations?&lt;br /&gt;&lt;br /&gt;I firstly considered the basic &lt;a href=&#34;http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization&#34; target=&#34;_blank&#34;&gt;Newton&#39;s method&lt;/a&gt;, but for least squares minimization, the Hessian (second derivatives) is needed. It&#39;s possible to obtain it, even analytically with SABR, but it&#39;s quite annoying to derive it and code it without some automatic differentiation tool. It turns out that as I experimented with the numerical Hessian, I noticed that it actually did not help convergence in our problem. &lt;a href=&#34;http://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm&#34; target=&#34;_blank&#34;&gt;Gauss-Newton&lt;/a&gt; converges similarly (likely because the initial guess is good), and what&#39;s great about it is that you just need the Jacobian (first derivatives). &lt;a href=&#34;https://www.math.lsu.edu/system/files/MunozGroup1%20-%20Paper.pdf&#34; target=&#34;_blank&#34;&gt;Here&lt;/a&gt; is a good overview of Newton, Gauss-Newton and Levenberg-Marquardt methods.&lt;br /&gt;&lt;br /&gt;While Gauss-Newton worked on many input data, I noticed it failed also on some long maturities equity smiles. The full Newton&#39;s method did not fare&amp;nbsp; better. I had to take a close look at the matrices involved to understand what was going on. It turns out that sometimes, mostly when the SABR rho parameter is close to -1, the Jacobian would be nearly rank deficient (a row close to 0), but not exactly rank deficient. So everything would appear to work, but it actually misbehaves badly.&lt;br /&gt;&lt;br /&gt;My first idea was to solve the reduced problem if a row of the Jacobian is too small, by just removing that row, and keep the previous value for the guess corresponding to that row. And this simplistic approach made the process work on all my input data. Here is the difference in RMSE compared to a highly accurate Levenberg-Marquardt minimization for 10 iterations:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png&#34; height=&#34;260&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;Later, while reading some more material related to least square optimization, I noticed the use of the &lt;a href=&#34;http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse&#34; target=&#34;_blank&#34;&gt;Moore-Penrose inverse&lt;/a&gt; in cases where a matrix is rank deficient. The Moore-Penrose inverse is defined as:&lt;br /&gt;$$ M^\star = V S^\star U^T$$&lt;br /&gt;where \( S^\star \) is the diagonal matrix with inverted eigenvalues and 0 if those are deemed numerically close to 0, and \(U, V\) the eigenvectors of the SVD decomposition:&lt;br /&gt;$$M=U S V^T$$&lt;br /&gt;It turns out to work very well, beside being simpler to code, I expected it to be more or less equivalent to the previous approach (a tiny bit slower but we don&#39;t care as we deal with small matrices, and the real slow part is the computation of the objective function and the Hessian, which is why looking at iterations is more important).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png&#34; height=&#34;267&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It seems to converge a little bit less quickly, likely due to the threshold criteria that I picked (1E-15).&lt;br /&gt;Three iterations is actually most of the time (90%) more than enough to achieve a good accuracy (the absolute RMSE is between 1E-4 and 5E-2) as the following graph shows. The few spikes near 1E-3 represent too large errors, the rest is accurate enough compared to the absolute RMSE.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png&#34; height=&#34;242&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;To conclude, we have seen that using the Moore-Penrose inverse in a Gauss-Newton iteration allowed the Gauss-Newton method to work on rank-deficient systems.&lt;br /&gt;I am not sure how general that is, in my example, the true minimum either lies inside the region of interest, or on the border, where the system becomes deficient. Of course, this is related to a &#34;physical&#34; constraint, here namely rho &amp;gt; -1.</description>
    </item>
    <item>
      <title>On the importance of accuracy for bpvol solvers</title>
      <link>https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/</link>
      <pubDate>Thu, 12 Jun 2014 17:31:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/</guid>
      <description>&lt;p&gt;While I was playing around calibrating the arbitrage free SABR model from Hagan (using the PDE on probability density approach), I noticed a misbehavior for some short maturity smiles. I thought it was due to the PDE implementation. Actually some of it was, but the remaining large error was due to the bpvol solver.&lt;/p&gt;&#xA;&lt;p&gt;I initially took the same approach as Choi et al. in &lt;a href=&#34;https://chasethedevil.github.io/post/building-a-more-accurate-basis-point-volatility-formula&#34;&gt;my solver&lt;/a&gt;, that is to work with in-the-money prices (they work with straddles) because it&amp;rsquo;s nice and convenient. I thought it was no big deal if prices lower than 1E-16 were not solved. It turns out I was wrong. Choi et al. solver has the same issue.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20-%2006122014%20-%2005%2024%2027%20PM.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Two SABR for the same smile</title>
      <link>https://chasethedevil.github.io/post/two-sabr-for-the-same-smile/</link>
      <pubDate>Tue, 20 May 2014 12:08:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/two-sabr-for-the-same-smile/</guid>
      <description>&lt;p&gt;While playing around with &lt;a href=&#34;https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/&#34;&gt;differential evolution&lt;/a&gt; to calibrate SABR, I noticed that sometimes, several set of parameters can lead to a very similar smile, usually the good one is for relatively low vol of vol and the bad one is for relatively high vol of vol. I first looked for errors in my implementation, but it&amp;rsquo;s a real phenomenon.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202014-05-19%2017%2055%2050.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;I used the normal implied volatility formula with beta=1, then converted it to lognormal (Black) volatility. While it might not be a great idea to rely on the normal formula with beta=1, I noticed the same phenomenon with the &lt;a href=&#34;https://chasethedevil.github.io/post/sabr-with-new-hagan-pde-approach&#34;&gt;arbitrage free PDE density approach&lt;/a&gt;, especially for long maturities. Interestingly, I did not notice such behavior before with other stochastic volatility models like Heston or Schobel-Zhu: I suspect it has to do with the approximations rather than with the true behavior of SABR.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Heston vs SABR slice by slice fit</title>
      <link>https://chasethedevil.github.io/post/heston-vs-sabr-slice-by-slice-fit/</link>
      <pubDate>Thu, 15 May 2014 22:06:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/heston-vs-sabr-slice-by-slice-fit/</guid>
      <description>&lt;p&gt;Some people use &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CCoQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1496982&amp;amp;ei=nx11U6QOoozQBcrfgIAK&amp;amp;usg=AFQjCNHi149E0_JiOzZRT9kgDWWYvHWlFQ&amp;amp;sig2=ZnhySOuJd8V-jBcWo4Ky2w&amp;amp;bvm=bv.66699033,d.d2k&#34;&gt;Heston to fit one slice&lt;/a&gt; of a volatility surface. In this case, some parameters are clearly redundant. Still, I was wondering how it fared against SABR, which is always used to fit a slice. And what about Schobel-Zhu?&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202014-05-15%2021%2020%2028.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Aggregated error in fit per slice on 10 surfaces&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;With Heston, the calibration is actually slightly better with kappa=0, that is, without mean reversion, because the global optimization is easier and the mean reversion is fully redundant. It&amp;rsquo;s still quite remarkable that 3 parameters result in a fit as good as 5 parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quadratic Spline with Knots at Mid-Points</title>
      <link>https://chasethedevil.github.io/post/quadratic-spline-with-knots-at-mid-points/</link>
      <pubDate>Wed, 14 May 2014 14:12:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quadratic-spline-with-knots-at-mid-points/</guid>
      <description>&lt;p&gt;Two months ago, I looked at &lt;a href=&#34;https://chasethedevil.github.io/post/arbitrage-free-interpolation-of-option-prices-using-piecewise-constant-density&#34;&gt;arbitrage free interpolation using piecewise-constant density&lt;/a&gt;. This is equivalent to a piecewise quadratic polynomial in call prices where each piece is centered around each call strike.&lt;/p&gt;&#xA;&lt;p&gt;I wondered at the time what a quadratic spline would look like on this problem, as it should be very close in theory, except that we can ensure that it is C1, a condition for a good looking implied volatility.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Non-linear Option Pricing</title>
      <link>https://chasethedevil.github.io/post/non-linear-option-pricing/</link>
      <pubDate>Fri, 18 Apr 2014 22:18:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/non-linear-option-pricing/</guid>
      <description>&lt;p&gt;I am currently reading the book &amp;ldquo;&lt;a href=&#34;http://www.amazon.co.uk/Nonlinear-Pricing-Chapman-Financial-Mathematics/dp/1466570334/ref=sr_1_1?ie=UTF8&amp;amp;qid=1397852099&amp;amp;sr=8-1&amp;amp;keywords=nonlinear+option+pricing&#34;&gt;Nonlinear Option Pricing&lt;/a&gt;&amp;rdquo; by J. Guyon and P. Henry-Labordère. It&amp;rsquo;s quite interesting even if the first third is quite theoretical. For example they describe how to solve some not well defined non-linear parabolic PDE by relying on the parabolic envelope. They also explain why most problems lead to parabolic PDEs in finance.&lt;/p&gt;&#xA;&lt;p&gt;The rest is a bit more practical. I stumbled upon an good remark regarding Longstaff-Schwartz: the algorithm as Longstaff and Schwarz describe it does not necessary lead to a low-biased estimate as they use future information (the paths they regress on) in the Monte-Carlo estimate. It was actually &lt;a href=&#34;https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/&#34;&gt;a subject of discussion&lt;/a&gt; with colleagues, and I analyzed the numerical impact in a simple use case in &lt;a href=&#34;http://papers.ssrn.com/abstract=2262259&#34;&gt;http://papers.ssrn.com/abstract=2262259&lt;/a&gt; In short: it&amp;rsquo;s actually more precise to include the path, even if the estimate is not purely low biased anymore, but the bias is really small in practice.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Building a more accurate basis point volatility formula</title>
      <link>https://chasethedevil.github.io/post/building-a-more-accurate-basis-point-volatility-formula/</link>
      <pubDate>Sat, 05 Apr 2014 15:42:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/building-a-more-accurate-basis-point-volatility-formula/</guid>
      <description>P. Jaeckel has defied the limits of accuracy with his&lt;a href=&#34;http://www.pjaeckel.webspace.virginmedia.com/LetsBeRational.7z&#34; target=&#34;_blank&#34;&gt; latest Black-Scholes volatility solver&lt;/a&gt;, managing to also improve performance compared to his earlier solver &#34;&lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0CC8QFjAA&amp;amp;url=http%3A%2F%2Fwww.pjaeckel.webspace.virginmedia.com%2FByImplication.pdf&amp;amp;ei=dwNAU5zkO4aI7AaFy4H4BQ&amp;amp;usg=AFQjCNF1RnfynBrckd79E9RVbhVhuXQrQg&amp;amp;sig2=kOxFO9jmD3wL1E94GjYhRQ&amp;amp;bvm=bv.64125504,d.ZGU&#34; target=&#34;_blank&#34;&gt;By Implication&lt;/a&gt;&#34;. Out of a silly exercise, I decided to try my hand for a more accurate &lt;a href=&#34;http://www.clarusft.com/analytic-implied-basis-point-volatility/&#34; target=&#34;_blank&#34;&gt;Normal (or basis point) volatility solver&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;In reality, the problem is much simpler in the Bachelier/Normal model. A very basic analysis of Bachelier formula shows that the problem can be reduced to a single variable, as Choi et al explain in &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/13504860802583436&#34; target=&#34;_blank&#34;&gt;their paper&lt;/a&gt;. So the problem is not really one of solving, but one of approximating (the inverse of) a function.&lt;br /&gt;&lt;br /&gt;The first step to build that function is to actually have a highly accurate slow solver as reference. This is quite easy, I just started with Choi formula and used &lt;a href=&#34;http://en.wikipedia.org/wiki/Halley%27s_method&#34; target=&#34;_blank&#34;&gt;Halley&#39;s method&lt;/a&gt; to refine. In reality, &lt;a href=&#34;http://en.wikipedia.org/wiki/Halley%27s_method&#34; target=&#34;_blank&#34;&gt;Halley&#39;s method&lt;/a&gt; is already a bit overkill on this problem: it works impressively well, 1 iteration is enough to have an insane level of accuracy, only noticeable when one works in high precision arithmetic (for example 50 digits). For double precision, &lt;a href=&#34;http://en.wikipedia.org/wiki/Newton%27s_method&#34; target=&#34;_blank&#34;&gt;Newton&#39;s method&lt;/a&gt; would actually be enough - I initially thought that my Halley&#39;s implementation did not work as it produced the exact same output as Newton in double precision. &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/14697680902849361&#34; target=&#34;_blank&#34;&gt;Li proposes&lt;/a&gt; the use of the SOR method, which for this exercise, behaves very much like Newton&#39;s method.&lt;br /&gt;&lt;br /&gt;I then followed the logic from Choi et al, but working directly with in-the-money call options instead of &lt;a href=&#34;http://en.wikipedia.org/wiki/Straddle&#34; target=&#34;_blank&#34;&gt;straddles&lt;/a&gt;. Straddles sound neat at first (hides that we work in-the-money), but it&#39;s actually useless for the algorithm. Choi et al. ignore half of the straddle range when they use their eta transform in the paper. One other change is the mapping itself, I found a better mapping for the call options (but not that far of Choi initial idea). Finally, because I am lazy, I did not go to the pain of finding a good rational fraction approximation along with the square root problem they describe, I just tried a Chebyshev polynomial.&lt;br /&gt;&lt;br /&gt;Unfortunately, a single Chebyshev polynomial does not work well: even with a very large (1000) degree it&#39;s not very precise, so much that I thought that my transform was garbage. I had noticed by mistake, that on another part (negative) of the interval, the Chebyshev polynomial worked actually very well to approximate something related to the volatility of another option. Suddendly came to me the idea of, like Johnson does in &lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package&#34; target=&#34;_blank&#34;&gt;his Faddeeva package&lt;/a&gt;, using N Chebyshev polynomials on N small intervals. This is like the big heavy hammer for which everything looks like nails, but it&#39;s actually very fast to evaluate as the degree of each polynomial can then be low (7), plus a table lookup (could be coded as switch statements if one really cares about such details). The slowest part is actually the call to the log function.&lt;br /&gt;&lt;br /&gt;The final bit is the use of a Taylor approximation for my -u/log(1-u) transform as it is not all that accurate in double precision when u is near 0. And that produces the following graph&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-r3FCXrjN13k/U0AHWIg7MRI/AAAAAAAAHGU/yDE6BEPtmIs/s1600/Screenshot+from+2014-04-05+15%253A37%253A03.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-r3FCXrjN13k/U0AHWIg7MRI/AAAAAAAAHGU/yDE6BEPtmIs/s1600/Screenshot+from+2014-04-05+15%253A37%253A03.png&#34; height=&#34;342&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It is interesting to note that &#34;solving&#34; the b.p. vol is &lt;b&gt;10x faster&lt;/b&gt; than solving the Black vol.&lt;br /&gt;&lt;br /&gt;I wrote &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2420757&#34; target=&#34;_blank&#34;&gt;a small paper around all this&lt;/a&gt; where you&#39;ll find the details as well as Matlab code.</description>
    </item>
    <item>
      <title>Fast and Accurate Implied Volatility Solver</title>
      <link>https://chasethedevil.github.io/post/fast-and-accurate-implied-volatility-solver/</link>
      <pubDate>Wed, 19 Mar 2014 18:10:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/fast-and-accurate-implied-volatility-solver/</guid>
      <description>&lt;p&gt;The calibration of a stochastic volatility model or a volatility surface parameterization (like SVI) involves minimizing the model options volatilities against market options volatilities. Often, the model computes an option price, not an implied volatility. It is therefore useful to have a fast way to invert that option price to get back the implied volatility that corresponds to it. Furthermore during the calibration procedure, the model option price can vary widely: it is convenient to have a robust implied volatility solver.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Another more basic use of implied volatility solvers, is for the computation of Black-Scholes greeks for a given market option price.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;A few years ago, P. Jaeckel produced a much better algorithm than a simple Newton or Brent solver, in his paper &amp;ldquo;&lt;!-- raw HTML omitted --&gt;By Implication&lt;!-- raw HTML omitted --&gt;&amp;rdquo;. There is also a much &lt;!-- raw HTML omitted --&gt;simpler algorithm from Li&lt;!-- raw HTML omitted --&gt;, based on SOR and a good initial guess for SOR, which I found to be actually quite robust and fast. Now P. Jaeckel has &lt;!-- raw HTML omitted --&gt;a newer algorithm&lt;!-- raw HTML omitted --&gt;, faster and more accurate, close to double accuracy.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I have tested those on 1 million options of random volatility (between 0.001 and 3), random strikes (N deviations with a cap at 1M) for a few expiries.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;In terms of performance, the results are independent of expiries, but in terms of accuracy, the new Jaeckel algorithm is particularly more accurate for the long-term options (5y and 15y).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Algorithm   Expiry  Vol RMSE  Price RMSE  Time&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2014 5y      3.8E-16   2.1E-16     1.8s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2006 5y      1.3E-10   1.1E-10     3.0s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2014 15y     2.0E-12   2.4E-16     1.8s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2006 15y     1.5E-7    8.0E-11     2.7s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The maximum error from Jaeckel2006 is around 1e-8, while the one from Jaeckel2014 is 2e-15 (for very large unrealistic strike)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;As a comparison, the simpler Li SOR-TS algorithm follows the given price accuracy independently of the expiry; I have tested with 1E-12. The error in implied volatility will be slightly higher: different close vols can give the same price due to the maximum achievable accuracy of the Black-Scholes formula with double numbers, even with &lt;!-- raw HTML omitted --&gt;a good cumulative normal distribution implementation&lt;!-- raw HTML omitted --&gt;. Its performance is however dependent on the number of deviations considered: closer to ATM means faster for Li algorithm.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Algorithm   Expiry  Deviation  Vol RMSE  Price RMSE  Time&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2014 1y      5          4.2E-16   2.0E-17     1.8s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;LiSORTS     1y      5          8.5E-9    1.9E-13     2.1s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Jaeckel2014 1y      3          3.1E-16   5.9E-17     1.7s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;LiSORTS     1y      3          4.3E-12   1.6E-13     1.3s&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Actually, I have cheated in my Li SOR-TS implementation: I have reused the idea from P. Jaeckel to compute the Black-Scholes price with &lt;!-- raw HTML omitted --&gt;erfc_x&lt;!-- raw HTML omitted --&gt; (unscaled erfc) instead of &lt;!-- raw HTML omitted --&gt;erfc&lt;!-- raw HTML omitted --&gt;. This simple change divides the number of &lt;!-- raw HTML omitted --&gt;exp&lt;!-- raw HTML omitted --&gt; calls by 2. Without this trick, for 5 deviations, SOR-TS took 3.6s (almost twice the time).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I would not be surprised if this was the main performance improvement between the two Jaeckels.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Arbitrage Free Interpolation of Option Prices using Piecewise Constant Density</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-interpolation-of-option-prices-using-piecewise-constant-density/</link>
      <pubDate>Mon, 17 Mar 2014 15:25:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/arbitrage-free-interpolation-of-option-prices-using-piecewise-constant-density/</guid>
      <description>&lt;p&gt;Tension splines can produce in some cases &lt;!-- raw HTML omitted --&gt;arbitrage free C2 interpolation&lt;!-- raw HTML omitted --&gt; of options, but unfortunately this is not guaranteed. It turns out that, on some not so nice looking data, where the discrete probability density is not monotone but only positive, all previously considered interpolation fail (spline in volatility or variance, tension spline in log prices, harmonic spline on prices).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;K     vol   put   b-slope  b-convexity&lt;!-- raw HTML omitted --&gt;300.0 0.682 0.090 0.00e+00 0.00e+00&lt;!-- raw HTML omitted --&gt;310.0 0.654 0.136 4.60e-03 0.00e+00&lt;!-- raw HTML omitted --&gt;320.0 0.621 0.192 5.60e-03 1.00e-03&lt;!-- raw HTML omitted --&gt;330.0 0.594 0.288 9.60e-03 4.00e-03&lt;!-- raw HTML omitted --&gt;340.0 0.560 0.404 1.16e-02 2.00e-03&lt;!-- raw HTML omitted --&gt;350.0 0.520 0.530 1.26e-02 1.00e-03&lt;!-- raw HTML omitted --&gt;360.0 0.484 0.736 2.06e-02 8.00e-03&lt;!-- raw HTML omitted --&gt;370.0 0.467 1.232 4.96e-02 2.90e-02&lt;!-- raw HTML omitted --&gt;380.0 0.442 1.898 6.66e-02 1.70e-02&lt;!-- raw HTML omitted --&gt;390.0 0.427 3.104 1.21e-01 5.40e-02&lt;!-- raw HTML omitted --&gt;400.0 0.412 4.930 1.83e-01 6.20e-02&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Possibly the simplest arbitrage free interpolation is to postulate the density as piecewise constant, ideally centered around each strike (if not centered, there is no guarantee that it will be positive). If a spline is used for interpolation, this means a quadratic spline. Unfortunately, because it is not C2, it then still fails to be arbitrage free.&lt;!-- raw HTML omitted --&gt;It is also possible to price by integrating the payoff over the density. There is then one degree of freedom, the Fmin (minimum forward allowed before absorption) that can be adjusted so as to make the density always positive. This produces our only arbitrage free interpolation of the above input put option prices.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The implied volatility looks reasonable on this strange input: very much like a spline on volatilities. In contrast, the parabolic interpolator produces an oddly looking implied volatility shape, even though the density is in a way similar: piecewise constant. This is likely because I forced the second derivatives to match the discrete curvature, it is then not C1 in prices.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Unfortunately, the piecewise constant density interpolant can also produce some strange implied volatility shapes, for example on P. Jaeckel example data:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;We find back the wiggles for large strikes. The lower end is particularly funny (which could be due to the fact that I don&amp;rsquo;t have the data for low strikes). This is the corresponding density:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It appears then not so easy to produce a simple generally good looking interpolation.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>C2 Arbitrage Free Interpolation with Tension Splines</title>
      <link>https://chasethedevil.github.io/post/c2-arbitrage-free-interpolation-with-tension-splines/</link>
      <pubDate>Tue, 11 Mar 2014 17:05:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/c2-arbitrage-free-interpolation-with-tension-splines/</guid>
      <description>&lt;p&gt;In a previous post, I have explored the &lt;a href=&#34;https://chasethedevil.github.io/post/arbitrage-free-wiggles&#34;&gt;arbitrage free wiggles&lt;/a&gt; in the volatility surface that P. Jaeckel found in &lt;a href=&#34;http://www.pjaeckel.webspace.virginmedia.com/ClampingDownOnArbitrage.pdf&#34;&gt;his paper&lt;/a&gt;. I showed that interpolating in log prices instead of prices was enough to remove the wiggles, but then, it appears that the interpolation is not guaranteed to be arbitrage free, even though it often is. On another example from P. Jaeckel paper, that I reproduced inaccurately but well enough, it is not.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bachelier and Black-Scholes Fits of the Volatility Surface, what about SABR?</title>
      <link>https://chasethedevil.github.io/post/bachelier-and-black-scholes-fits-of-the-volatility-surface-what-about-sabr/</link>
      <pubDate>Fri, 07 Mar 2014 15:31:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/bachelier-and-black-scholes-fits-of-the-volatility-surface-what-about-sabr/</guid>
      <description>I always wondered if Bachelier was really worse than Black-Scholes in practice. As an experiment I fit various implied volatility surfaces with Bachelier and Black-Scholes and look at the average error in implied volatility by slice.&lt;br /&gt;&lt;br /&gt;In theory Bachelier is appealing because slightly simpler: log returns are a bit more challenging to think about than returns. And it also takes indirectly into account the fact that OTM calls are less likely than OTM puts because of default risk, if you assume absorbing probability at strike 0.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-FtWwKMYTItY/Uxn4UIURgnI/AAAAAAAAHDk/qJDWTdPl6gw/s1600/bachelier.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-FtWwKMYTItY/Uxn4UIURgnI/AAAAAAAAHDk/qJDWTdPl6gw/s1600/bachelier.png&#34; height=&#34;336&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Bachelier&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-XGjl9Pcs5aY/Uxn4aNFFB8I/AAAAAAAAHDs/-v8zOpNFu24/s1600/black.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-XGjl9Pcs5aY/Uxn4aNFFB8I/AAAAAAAAHDs/-v8zOpNFu24/s1600/black.png&#34; height=&#34;336&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Black-Scholes&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;The error is in general smaller for Black-Scholes for short expiries, and higher for long expiries when compared to Bachelier. Interestingly, in theory, the difference of the models is more pronounced for longer expiries. One would have imagined that for very short expiries Bachelier would be equivalent to Black, it is not the case in term of fitting the smile.&lt;br /&gt;&lt;br /&gt;Displaced diffusion (mixing both Bachelier and Black linearly) allows to gain x2 accuracy.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-zCvfSvMtsmc/Uxn5HHk0gBI/AAAAAAAAHD0/FrFzvnmnd08/s1600/dd.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-zCvfSvMtsmc/Uxn5HHk0gBI/AAAAAAAAHD0/FrFzvnmnd08/s1600/dd.png&#34; height=&#34;336&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Displaced Diffusion&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;What about SABR? Let&#39;s look at lognormal SABR, usually used for equities.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-lvB1kTAXh7U/Uxn5VHjEWvI/AAAAAAAAHD8/XMGwCisCiKg/s1600/sabr.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-lvB1kTAXh7U/Uxn5VHjEWvI/AAAAAAAAHD8/XMGwCisCiKg/s1600/sabr.png&#34; height=&#34;336&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Lognormal SABR (beta=1)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;The fit is much better for long expiries, but not so great for a few outliers for long maturities, it can be actually worse than the simple displaced diffusion. A normal SABR might fix that.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-QcGK_40mOHQ/Uxn5qLeI45I/AAAAAAAAHEE/tMWUPEtaxvs/s1600/sabr0.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-QcGK_40mOHQ/Uxn5qLeI45I/AAAAAAAAHEE/tMWUPEtaxvs/s1600/sabr0.png&#34; height=&#34;336&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Normal SABR (beta=0) using Hagan lognormal formula&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;If one relies on the standard lognormal formula, the beta=0 SABR behaves very badly.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kpi0HJKc25o/Uxn57qj8QpI/AAAAAAAAHEM/TLBa6dseWzI/s1600/Screenshot+from+2014-03-07+17:35:49.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-kpi0HJKc25o/Uxn57qj8QpI/AAAAAAAAHEM/TLBa6dseWzI/s1600/Screenshot+from+2014-03-07+17:35:49.png&#34; height=&#34;321&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Normal SABR using Hagan normal formula&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;This is fixed using the normal (bpvol) Hagan formula directly. The fit is then better overall for long maturities as expected from Black-Scholes vs Bachelier behavior.&lt;br /&gt;&lt;br /&gt;If one look in log scale, the conclusion is not so obvious: beta=1 produces a better fit for a majority even for long expiries, but worse for a few (30% in my case) outliers.&lt;br /&gt;&lt;br /&gt;What&#39;s clear however, is that one should never use the Black implied volatility Hagan formula with beta=0. This leaves a question on displaced SABR. Is displaced SABR is better suited than SABR with varying beta?</description>
    </item>
    <item>
      <title>Arbitrage Free Wiggles</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-wiggles/</link>
      <pubDate>Mon, 03 Mar 2014 17:13:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/arbitrage-free-wiggles/</guid>
      <description>Peter Jaeckel, in a &lt;a href=&#34;http://www.pjaeckel.webspace.virginmedia.com/ClampingDownOnArbitrage.pdf&#34;&gt;recent paper (pdf)&lt;/a&gt;, shows that something that sounds like a reasonable arbitrage free interpolation can produce wiggles in the implied volatility slice.&lt;br /&gt;&lt;br /&gt;The interpolation in question is using some convexity preserving spline on call and put option prices directly and in strike, assuming those input prices are arbitrage free. This is very similar to &lt;a href=&#34;http://nkahale.free.fr/papers/Interpolation.pdf&#34;&gt;Kahale interpolation (pdf)&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;It seemed too crazy for me so I had to try out his example. And using a &lt;a href=&#34;http://papers.ssrn.com/abstract=2175002&#34;&gt;harmonic spline&lt;/a&gt;, it does produce arbitrage free wiggles.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-HhHvYhvdlwU/UxSkNCTJiXI/AAAAAAAAHBQ/buAU6Q_wALM/s1600/Screenshot+from+2014-03-03+16:29:29.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-HhHvYhvdlwU/UxSkNCTJiXI/AAAAAAAAHBQ/buAU6Q_wALM/s1600/Screenshot+from+2014-03-03+16:29:29.png&#34; height=&#34;297&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Wiggles in the implied volatility&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;If we look at the probability density (the curvature), the Harmonic spline maintains a positive density, nearly piecewise flat in log scale, while Hyman, because it preserves only monotonicity, has some negative density which are cut out from the graph.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-UKuoXUcVRYY/UxSk1ylZ7jI/AAAAAAAAHBY/RDxMR2Q69GY/s1600/Screenshot+from+2014-03-03+16:47:46.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-UKuoXUcVRYY/UxSk1ylZ7jI/AAAAAAAAHBY/RDxMR2Q69GY/s1600/Screenshot+from+2014-03-03+16:47:46.png&#34; height=&#34;297&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Probability Density&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;In reality, if we look at the interpolation of prices in log scale, one can see that splines won&#39;t behave as expected at first on small numbers: they will give a much higher weight to the high values, producing something like a piecewise linear interpolation.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-5WOPPmW088c/UxSml6PGWmI/AAAAAAAAHBk/xOfiaaRcpLI/s1600/Screenshot+from+2014-03-03+16:57:33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-5WOPPmW088c/UxSml6PGWmI/AAAAAAAAHBk/xOfiaaRcpLI/s1600/Screenshot+from+2014-03-03+16:57:33.png&#34; height=&#34;297&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In reality, what one really wants for such data is to just interpolate the log prices with a spline, not the prices. This is the curve named &#34;Log&#34; in the graphs, where a simple cubic spline is used on the log prices, and fed to exp after interpolation.&lt;br /&gt;&lt;br /&gt;Now it sounds like a reasonable arbitrage free interpolation would be to interpolate the discrete density log linearly, in a similar spirit as &lt;a href=&#34;http://finmod.co.za/interpreview.pdf&#34;&gt;Hagan-West yield curve interpolation&lt;/a&gt; (pdf).&lt;br /&gt;&lt;br /&gt;In general, if you interpolate very small numbers with a spline, you probably are doing something wrong.</description>
    </item>
    <item>
      <title>Adjoint Delta for Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/adjoint-delta-for-monte-carlo/</link>
      <pubDate>Tue, 25 Feb 2014 18:37:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/adjoint-delta-for-monte-carlo/</guid>
      <description>&lt;p&gt;In an earlier post, I have been quickly exploring &lt;!-- raw HTML omitted --&gt;adjoint differentiation&lt;!-- raw HTML omitted --&gt; in the context of analytical Black-Scholes. Today, I tried to mix it in a simple Black-Scholes Monte-Carlo as described in &lt;!-- raw HTML omitted --&gt;L. Capriotti paper&lt;!-- raw HTML omitted --&gt;, and measured the performance to compute delta compared to a numerical single sided finite difference delta.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I was a bit surprised that even on a single underlying, without any real optimization, adjoint delta was faster by a factor of nearly 40%. I suspect this is mostly due to exp evaluations being /2.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;On a basket of 4 assets, the adjoint method was 3.25x faster.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It&amp;rsquo;s quick to have such results on basic payoffs: it took me a few hours and worked on the first run, even though my Monte-Carlo is slightly different from the Capriotti paper. It is much more challenging to have it working across a wide variety of payoffs, and to automatize some of it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SVI on top of SABR</title>
      <link>https://chasethedevil.github.io/post/svi-on-top-of-sabr/</link>
      <pubDate>Thu, 20 Feb 2014 18:36:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/svi-on-top-of-sabr/</guid>
      <description>Several papers show that the limit for large strikes of &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;ved=0CDUQFjAB&amp;amp;url=http%3A%2F%2Farxiv.org%2Fabs%2F1002.3633&amp;amp;ei=GjUGU5uVIeu00wXEwoCwBw&amp;amp;usg=AFQjCNHvnJ9xYAv60R4GSUbMORRI9_-7Zg&amp;amp;sig2=iqplpcXauJBLJ9DIw71vWA&amp;amp;bvm=bv.61725948,d.d2k&#34;&gt;Heston is SVI&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Interestingly, I stumbled onto a surface where the Hagan SABR fit was perfect as well as the SVI fit, while the Heston fit was not.&lt;br /&gt;&lt;br /&gt;Originally, I knew that, on this data, the SVI fit was perfect. Until today, I just never tried to fit a lognormal SABR on the same data. I did a small test with random values of the SABR parameters alpha, rho, nu, and found out that in deed, the SVI fit is always perfect on SABR.&lt;br /&gt;&lt;br /&gt;Is this just because the Taylor expansion of SVI will match the Taylor expansion of SABR up to the fourth order? It seems that the wings are also not too far off in general so there could be more to it.&lt;br /&gt;&lt;br /&gt;Gatheral actually devised an SVI formula that uses SABR like variables &lt;a href=&#34;http://mfe.baruch.cuny.edu/wp-content/uploads/2013/01/OsakaSVI2012.pdf&#34;&gt;in a talk here&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-QNLwJnVGPUE/UwY9EwCkZ6I/AAAAAAAAHA4/DP6MstmUqg4/s1600/Screenshot+from+2014-02-20+18:36:06.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-QNLwJnVGPUE/UwY9EwCkZ6I/AAAAAAAAHA4/DP6MstmUqg4/s1600/Screenshot+from+2014-02-20+18:36:06.png&#34; height=&#34;79&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;Of course, the reverse is not true. SVI has more parameters and provides in general a better fit than SABR.</description>
    </item>
    <item>
      <title>Smart Initial Guess for Schobel-Zhu</title>
      <link>https://chasethedevil.github.io/post/smart-initial-guess-for-schobel-zhu/</link>
      <pubDate>Wed, 19 Feb 2014 18:57:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/smart-initial-guess-for-schobel-zhu/</guid>
      <description>&lt;p&gt;With a &lt;a href=&#34;https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion&#34;&gt;small time expansion&lt;/a&gt;, it is easy to derive a reasonable initial guess, without resorting to some global minimizer.&lt;/p&gt;&#xA;&lt;p&gt;Like &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCgQFjAA&amp;amp;url=http%3A%2F%2Fpage.math.tu-berlin.de%2F~jacquier%2Findex_files%2FJacquier%2520-%2520SmallTimeHeston2.pdf&amp;amp;ei=72D7Ur6NHeqP0AX7soFo&amp;amp;usg=AFQjCNGkx9ifAh3UQQI4UE_pD8osCzH7Rg&amp;amp;sig2=lIcs-s2yrxWZsxidkQlxOA&amp;amp;bvm=bv.61190604,d.d2k&#34;&gt;Forde did for Heston&lt;/a&gt;, one can find the 5 Schobel-Zhu parameters through 5 points at coordinates (0,0), (x0,t1), (-x0,t1), (x0,t2), (-x0,t2), where x0 is a chosen the log-moneyness, for example, 0.1 and t1, t2 relatively short expiries (for example, 0.1, 0.25).&lt;/p&gt;&#xA;&lt;p&gt;We can truncate the small time expansion so that the polynomial in (x,t) is fully captured by those 5 points. In practice, I have noticed that using a more refined expansion with more terms resulted not only in more complex formulas to lookup the original stochastic volatility parameters, but also in an increased error, because of the redundancy of parameters in the polynomial expansion. My previous Schobel-Zhu expansion becomes just:&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Look at Small Time Expansions for Heston</title>
      <link>https://chasethedevil.github.io/post/a-look-at-small-time-expansions-for-heston/</link>
      <pubDate>Wed, 12 Feb 2014 13:13:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-look-at-small-time-expansions-for-heston/</guid>
      <description>&lt;p&gt;Small time expansions for Heston can be useful during the calibration of the implied volatility surface, in order to find an initial guess for a local minimizer (for example, Levenberg-Marquardt). Even if they are not so accurate, they capture the dynamic of the model parameters, and that is often enough.&lt;/p&gt;&#xA;&lt;p&gt;In 2011, Forde et al. proposed &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCgQFjAA&amp;amp;url=http%3A%2F%2Fpage.math.tu-berlin.de%2F~jacquier%2Findex_files%2FJacquier%2520-%2520SmallTimeHeston2.pdf&amp;amp;ei=72D7Ur6NHeqP0AX7soFo&amp;amp;usg=AFQjCNGkx9ifAh3UQQI4UE_pD8osCzH7Rg&amp;amp;sig2=lIcs-s2yrxWZsxidkQlxOA&amp;amp;bvm=bv.61190604,d.d2k&#34;&gt;a second order small time expansion&lt;/a&gt; around the money, which I found to work well for calibration. More recently, Lorig et al. proposed &lt;a href=&#34;http://arxiv.org/abs/1306.5447&#34;&gt;different expansions up to order-3&lt;/a&gt; around the money. I already looked at the later in my &lt;a href=&#34;https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion&#34;&gt;previous post&lt;/a&gt;, applying the idea to Schobel-Zhu.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Small-Time Schobel-Zhu Expansion</title>
      <link>https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion/</link>
      <pubDate>Mon, 10 Feb 2014 18:30:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-small-time-schobel-zhu-expansion/</guid>
      <description>&lt;p&gt;The paper &lt;a href=&#34;http://arxiv.org/abs/1306.5447&#34;&gt;implied vol for any local stochastic vol model&lt;/a&gt; from Lorig et al. presents a very generic and simple formula to compute implied volatility expansions up to order-2 (there is actually an order-3 formula available in their Mathematica CDF file).&lt;/p&gt;&#xA;&lt;p&gt;I tried it on the &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;ved=0CDIQFjAB&amp;amp;url=http%3A%2F%2Fssrn.com%2Fabstract%3D100831&amp;amp;ei=3gr5UtzkOYm90QWD7IDwBA&amp;amp;usg=AFQjCNFY9anH_AU0A5pWRX4Qdo6Kp2CjEg&amp;amp;sig2=teGVgsV4uneROlzZCuUFBw&amp;amp;bvm=bv.60983673,d.d2k&#34;&gt;Schobel-Zhu&lt;/a&gt; stochastic volatility model. This model is an interesting alternative to Heston. I found that, in practice, the implied volatility surface &lt;a href=&#34;https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit&#34;&gt;fit was as good&lt;/a&gt;, while the &lt;a href=&#34;https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston&#34;&gt;simulation under the QE scheme&lt;/a&gt; is quite faster (and simpler) than Heston. Here is the result of applying their technique on Schobel-Zhu:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brownian Bridge or Not with Heston Quadratic Exponential QMC</title>
      <link>https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston-quadratic-exponential-qmc/</link>
      <pubDate>Fri, 24 Jan 2014 19:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston-quadratic-exponential-qmc/</guid>
      <description>At first I did not make use of the Brownian Bridge technique in Heston QMC, because the variance process is not simulated like a Brownian Motion under the &lt;a href=&#34;http://www.google.com/url?q=http://www.javaquant.net/papers/LeifAndersenHeston.pdf&amp;amp;sa=U&amp;amp;ei=p9jjUtPJK8uUhQfb_YGgAw&amp;amp;ved=0CB4QFjAA&amp;amp;sig2=EjKPr39tR0ni1vq5gK9mkA&amp;amp;usg=AFQjCNGQyqSaDu2kl6_KpP-s-KwMvJ6hPg&#34;&gt;Quadratic Exponential algorithm from Andersen&lt;/a&gt;.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kWmb0WWAYzA/UuPX4UMySRI/AAAAAAAAG-4/cBcjRgfTc5s/s1600/Screenshot+from+2014-01-25+16:26:05.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-kWmb0WWAYzA/UuPX4UMySRI/AAAAAAAAG-4/cBcjRgfTc5s/s1600/Screenshot+from+2014-01-25+16:26:05.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It is, however, perfectly possible to use the Brownian Bridge on the asset process. Does it make a difference? In my small test, it does not seem to make a difference. An additional question would be, is it better to take first N for the asset and next N for the variance or vice versa or intertwined? Intertwined would seem the most natural (this is what I used without Brownian Bridge, but for simplicity I did Brownian bridge on first N).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-WN5frRWHix4/UuKyR69BtbI/AAAAAAAAG-g/BTXm5K2XOQo/s1600/heston_bnp_sobol2.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-WN5frRWHix4/UuKyR69BtbI/AAAAAAAAG-g/BTXm5K2XOQo/s1600/heston_bnp_sobol2.png&#34; height=&#34;222&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;By contrast, Schobel-Zhu QE scheme can make full use of the Brownian Bridge technique, in the asset process as well as in the variance process. Here is a summary of the volatility process under the QE scheme from &lt;a href=&#34;http://www.google.com/url?q=http://arno.uvt.nl/show.cgi%3Ffid%3D99534&amp;amp;sa=U&amp;amp;ei=itjjUo2jD6Wa0AXqzYHgCQ&amp;amp;ved=0CB4QFjAA&amp;amp;sig2=KdYdo3sAFjV6fwPS4Gak0A&amp;amp;usg=AFQjCNEZ8DRGCf4EXOxCXs3oPc_98628hA&#34;&gt;van Haastrecht&lt;/a&gt;:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kYpBO8PfEkw/UuPX_40DnbI/AAAAAAAAG_A/v7QAN_lX9oM/s1600/Screenshot+from+2014-01-25+16:26:23.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-kYpBO8PfEkw/UuPX_40DnbI/AAAAAAAAG_A/v7QAN_lX9oM/s1600/Screenshot+from+2014-01-25+16:26:23.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Another nice property of Schobel-Zhu is that the QE simulation is as fast as Euler, and therefore 2.5x faster than the Heston QE.&lt;br /&gt;&lt;br /&gt;I calibrated the model to the same surface, and the QMC price of a ATM call option seems to have a similar accuracy as Heston QMC. But we can see that the Brownian Bridge does increase accuracy in this case. I was surprised that accuracy was not much better than Heston, but maybe it is because I did yet not implement the Martingale correction, while I did so in the Heston case.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/--mK4u1K8nOk/UuKyR2y6FHI/AAAAAAAAG-k/QSOjDwosqLo/s1600/sz_bnp_sobol3.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/--mK4u1K8nOk/UuKyR2y6FHI/AAAAAAAAG-k/QSOjDwosqLo/s1600/sz_bnp_sobol3.png&#34; height=&#34;222&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Adjoint Algorithmic Differentiation for Black-Scholes</title>
      <link>https://chasethedevil.github.io/post/adjoint-algorithmic-differentiation-for-black-scholes/</link>
      <pubDate>Tue, 21 Jan 2014 13:03:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/adjoint-algorithmic-differentiation-for-black-scholes/</guid>
      <description>&lt;a href=&#34;http://en.wikipedia.org/wiki/Automatic_differentiation&#34;&gt;Adjoint algorithmic differentiation&lt;/a&gt; is particularly interesting in finance as we often encounter the case of a function that takes many input (the market data) and returns one output (the price) and we would like to also compute sensitivities (greeks) to each input.&lt;br /&gt;&lt;br /&gt;As I am just starting around it, to get a better grasp, I first tried to apply the idea to the analytic knock out barrier option formula, by hand, only to find out I was making way too many errors by hand to verify anything. So I tried the simpler vanilla Black-Scholes formula. I also made various errors, but managed to fix all of them relatively easily.&lt;br /&gt;&lt;br /&gt;I decided to compare how much time it took to compute price, delta, vega, theta, rho, rho2 between single sided finite difference and the adjoint approach. Here are the results for 1 million options:&lt;br /&gt;&lt;br /&gt;&lt;blockquote class=&#34;tr_bq&#34;&gt;FD time=2.13s&lt;br /&gt;Adjoint time=0.63s&lt;/blockquote&gt;&lt;br /&gt;&lt;b&gt;It works well&lt;/b&gt;, but doing it by hand is crazy and too error prone. It might be simpler for Monte-Carlo payoffs however.&lt;br /&gt;&lt;br /&gt;There are not many Java tools that can do reverse automatic differentiation, I found &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=5&amp;amp;cad=rja&amp;amp;ved=0CFAQFjAE&amp;amp;url=http%3A%2F%2Fcluster.grid.pub.ro%2Fwiki%2Findex.php%2FADiJaC_-_Automatic_Differentiation_of_Java_Classfiles&amp;amp;ei=SmDeUtDLCYrG0QWrlIHQDw&amp;amp;usg=AFQjCNHLMwobO6u5jSoUifCdHvE2yPZ6oQ&amp;amp;sig2=wFbGRYjgLenCNGRbrQ1gKQ&amp;amp;bvm=bv.59568121,d.d2k&#34;&gt;some thesis&lt;/a&gt; on it, with an interesting byte code oriented approach (one difficulty is that you need to reverse loops, while statements).</description>
    </item>
    <item>
      <title>Placing the Strike on the Grid and Payoff Smoothing in Finite Difference Methods for Vanilla Options</title>
      <link>https://chasethedevil.github.io/post/placing-the-strike-on-the-grid-and-payoff-smoothing-in-finite-difference-methods-for-vanilla-options/</link>
      <pubDate>Sun, 12 Jan 2014 16:27:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/placing-the-strike-on-the-grid-and-payoff-smoothing-in-finite-difference-methods-for-vanilla-options/</guid>
      <description>Pooley et al., in &lt;a href=&#34;https://cs.uwaterloo.ca/~paforsyt/report.pdf&#34;&gt;Convergence Remedies for non-smooth payoffs in option pricing&lt;/a&gt; suggest that placing the strike on the grid for a Vanilla option is good enough:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-CIS-yTdMn7k/UtKy6HtMIhI/AAAAAAAAG-I/Hef_w7gTcJ0/s1600/pooley_vanilla_smooth.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-CIS-yTdMn7k/UtKy6HtMIhI/AAAAAAAAG-I/Hef_w7gTcJ0/s1600/pooley_vanilla_smooth.png&#34; height=&#34;48&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;At the same time, Tavella and Randall show in their book that numerically, placing the strike in the middle of two nodes leads to a more accurate result. My own numerical experiments confirm Tavella and Randall suggestion.&lt;br /&gt;&lt;br /&gt;In reality, what Pooley et al. really mean, is that quadratic convergence is maintained if the strike is on the grid for vanilla payoffs, contrary to the case of discontinuous payoffs (like digital options) where the convergence decreases to order 1. So it&#39;s ok to place the strike on the grid for a vanilla payoff, but it&#39;s not optimal, it&#39;s still better to place it in the middle of two nodes. Here are absolute errors in a put option price:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;on grid, no smoothing&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.04473021824995271&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;on grid, Simpson smoothing&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.003942854282069419&lt;br /&gt;on grid, projection smoothing 0.044730218065351934&lt;br /&gt;middle, no smoothing&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.004040359609906119&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;As expected (and mentioned in Pooley et al.), the projection does not do anything here. When the grid size is doubled, the convergence ratio of all methods is the same (order 2), but placing the strike in the middle still increases accuracy significantly.&lt;br /&gt;&lt;br /&gt;Here is are the same results, but for a digital put option:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;on grid, no smoothing &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; 0.03781319921461046&lt;br /&gt;on grid, Simpson smoothing&amp;nbsp;&amp;nbsp;&amp;nbsp; 8.289052335705427E-4&lt;br /&gt;on grid, projection smoothing 1.9698293587372406E-4&lt;br /&gt;middle, no smoothing&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 3.5122153011418744E-4&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Here only the 3 last methods are of order-2 convergence, and projection is in deed the most accurate method, but placing the strike in the middle is really not that much worse, and much simpler.&lt;br /&gt;&lt;br /&gt;A disadvantage of Simpson smoothing (or smoothing by averaging), is that it breaks put-call parity (see the paper &#34;&lt;a href=&#34;http://papers.ssrn.com/abstract=2362969&#34;&gt;Exact Forward and Put Call Parity with TR-BDF2&lt;/a&gt;&#34;) &lt;br /&gt;&lt;br /&gt;I think the emphasis in their paper on &#34;no smoothing is required&#34; for vanilla payoffs can be misleading. I hope I have clarified it in this post.</description>
    </item>
    <item>
      <title>Coordinate Transform of the Andreasen Huge SABR PDE &amp; Spline Interpolation</title>
      <link>https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde--spline-interpolation/</link>
      <pubDate>Wed, 08 Jan 2014 18:51:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde--spline-interpolation/</guid>
      <description>&lt;p&gt;Recently, I noticed &lt;a href=&#34;https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach&#34;&gt;how close&lt;/a&gt; are the two PDE based approaches from Andreasen-Huge and Hagan for an arbitrage free SABR. Hagan gives a local volatility very close to the one Andreasen-Huge use in the forward PDE in call prices. A multistep Andreasen-Huge (instead of their one step PDE method) gives back prices and densities nearly equal to Hagan density based approach.&lt;/p&gt;&#xA;&lt;p&gt;Hagan proposed in some unpublished paper a coordinate transformation for two reasons: the ideal range of strikes for the PDE can be very large, and concentrating the points where it matters should improve stability and accuracy. The transform itself can be found in the &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CC0QFjAA&amp;amp;url=http%3A%2F%2Fwww.andersen-piterbarg-book.com%2F&amp;amp;ei=dYzNUrG6Eo7n7Aamp4GwAQ&amp;amp;usg=AFQjCNE3sdrH2B8EDg40Gocp8FB-QEtnew&amp;amp;sig2=aoDaRX5-zTolem9mUrEumw&amp;amp;bvm=bv.58187178,d.ZGU&#34;&gt;Andersen-Piterbarg book&lt;/a&gt; &amp;ldquo;Interest Rate Modeling&amp;rdquo;, and is similar to the famous log transform, but for a general local volatility function (phi in the book notation).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Levenberg Marquardt &amp; Constraints by Domain Transformation</title>
      <link>https://chasethedevil.github.io/post/levenberg-marquardt--constraints-by-domain-transformation/</link>
      <pubDate>Tue, 17 Dec 2013 15:27:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/levenberg-marquardt--constraints-by-domain-transformation/</guid>
      <description>The Fortran &lt;a href=&#34;http://www.netlib.org/minpack/&#34;&gt;minpack&lt;/a&gt; library has a good &lt;a href=&#34;http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm&#34;&gt;Levenberg-Marquardt&lt;/a&gt; minimizer, so good, that it has been ported to many programming languages. Unfortunately it does not support contraints, even simple bounds.&lt;br /&gt;&lt;br /&gt;One way to achieve this is to transform the domain via a bijective function. For example, \(a+\frac{b-a}{1+e^{-\alpha t}}\) will transform \(]-\infty, +\infty[\) to ]a,b[. Then how should one choose \(\alpha\)?&lt;br /&gt;&lt;br /&gt;A large \(\alpha\) will make tiny changes in \(t\) appear large. A simple rule is to ensure that \(t\) does not create large changes in the original range ]a,b[, for example we can make \(\alpha t \leq 1\), that is \( \alpha t= \frac{t-a}{b-a} \).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s1600/Screenshot+from+2013-12-17+13:02:22.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;293&#34; src=&#34;http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s400/Screenshot+from+2013-12-17+13:02:22.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;In practice, for example in the calibration of the&lt;a href=&#34;http://en.wikipedia.org/wiki/Heston_model&#34;&gt; Double-Heston&lt;/a&gt; model on real data, a naive \( \alpha=1 \) will converge to a RMSE of 0.79%, while our choice will converge to a RMSE of 0.50%. Both will however converge to the same solution if the initial guess is close enough to the real solution. Without any transform, the RMSE is also 0.50%. The difference in error might not seem large but this results in vastly different calibrated parameters. Introducing the transform can significantly change the calibration result, if not done carefully.&lt;br /&gt;&lt;br /&gt;Another simpler way would be to just impose a cap/floor on the inputs, thus ensuring that nothing is evaluated where it does not make sense. In practice, it however will not always converge as well as the unconstrained problem: the gradient is not defined at the boundary. On the same data, the Schobel-Zhu, unconstrained converges with RMSE 1.08% while the transform converges to 1.22% and the cap/floor to 1.26%. The Schobel-Zhu example is more surprising since the initial guess, as well as the results are not so far:&lt;br /&gt;             &lt;style&gt;  &lt;!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:&#34;Liberation Sans&#34;; font-size:x-small }    --&gt; &lt;/style&gt;     &lt;br /&gt;&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cols=&#34;2&#34;&gt; &lt;colgroup span=&#34;2&#34; width=&#34;85&#34;&gt;&lt;/colgroup&gt; &lt;tbody&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Initial volatility (v0)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;18.1961174789&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Long run volatility (theta)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;1&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Speed of mean reversion (kappa)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;101.2291161766&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Vol of vol (sigma)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;35.2221829015&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Correlation (rho)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;-73.7995231799&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;17&#34; style=&#34;border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;ERROR MEASURE&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;1.0614889526&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;             &lt;style&gt;  &lt;!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:&#34;Liberation Sans&#34;; font-size:x-small }    --&gt; &lt;/style&gt;     &lt;br /&gt;&lt;table border=&#34;0&#34; cellspacing=&#34;0&#34; cols=&#34;2&#34;&gt; &lt;colgroup span=&#34;2&#34; width=&#34;85&#34;&gt;&lt;/colgroup&gt; &lt;tbody&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Initial volatility (v0)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;17.1295934569&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Long run volatility (theta)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;1&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Speed of mean reversion (kappa)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;67.9818356373&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Vol of vol (sigma)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;30.8491256097&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;16&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;Correlation (rho)&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;-74.614636128&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;tr&gt;  &lt;td align=&#34;LEFT&#34; height=&#34;17&#34; style=&#34;border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;ERROR MEASURE&lt;/span&gt;&lt;/td&gt;  &lt;td align=&#34;RIGHT&#34; style=&#34;border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;&#34; valign=&#34;TOP&#34;&gt;&lt;span style=&#34;font-family: Liberation Serif;&#34;&gt;1.2256421987&lt;/span&gt;&lt;/td&gt; &lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;The initial guess is kappa=61% theta=11% sigma=26% v0=18% rho=-70%. Only the kappa is different in the two results, and the range on the kappa is (0,2000) (it is expressed in %), much larger than the result. In reality, theta is the issue (in (0,1000)). Forbidding a negative theta has an impact on how kappa is picked. The only way to be closer&lt;br /&gt;&lt;br /&gt;Finally, a third way is to rely on a simple penalty: returning an arbitrary large number away from the boundary. In our examples this was no better than the transform or the cap/floor.&lt;br /&gt;&lt;br /&gt;Trying out the various ways, it seemed that allowing meaningless parameters, as long as they work mathematically produced the best results with Levenberg-Marquardt, particularly, allowing for a negative theta in Schobel-Zhu made a difference.</description>
    </item>
    <item>
      <title>Arbitrage Free SABR - Another View on Hagan Approach</title>
      <link>https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach/</link>
      <pubDate>Sat, 14 Dec 2013 00:56:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach/</guid>
      <description>&lt;p&gt;Several months ago, I took a look at &lt;a href=&#34;https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach&#34;&gt;two interesting recent ways&lt;/a&gt; to price under SABR with no arbitrage:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;One way is due to &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CC0QFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;amp;ei=F4yrUoL7Kq2M7AasuIFg&amp;amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;amp;sig2=-fFTrJR1wY1elyXBC1EC0A&amp;amp;bvm=bv.57967247,d.ZGU&#34;&gt;Andreasen and Huge&lt;/a&gt;, where they find an equivalent local volatility expansion, and then use a one-step finite difference technique to price.&lt;/li&gt;&#xA;&lt;li&gt;The other way is due to Hagan himself, where he numerically solves an approximate PDE in the probability density, and then price with options by integrating on this density.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;It turns out that the two ways are much closer than I first thought. Hagan PDE in the probability density is actually just the &lt;a href=&#34;http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation&#34;&gt;Fokker-Planck&lt;/a&gt; (forward) equation.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-12-13%2023%2026%2043.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;The \(\alpha D(F)\) is just the equivalent local volatility. Andreasen and Huge use nearly the same local volatility formula but without the exponential part (that is often negligible except for long maturities), directly in Dupire forward PDE:&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-12-13%2023%2031%2018.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;A common derivation (for example in &lt;a href=&#34;http://www.amazon.com/The-Volatility-Surface-Practitioners-Finance/dp/0471792519/ref=sr_1_1?ie=UTF8&amp;amp;qid=1386974691&amp;amp;sr=8-1&amp;amp;keywords=Gatheral+the+volatility&#34;&gt;Gatheral book&lt;/a&gt; of the Dupire forward PDE is to actually use the Fokker-Planck equation in the probability density integral formula. Out of curiosity, I tried to price direcly with Dupire forward PDE and the Hagan local volatility formula, using just linear boundary conditions. Here are the results on Hagan own example:&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/hagan_dens1.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/hagan_iv.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;The Local Vol direct approach overlaps the Density approach nearly exactly, except at the high strike boundary, when it comes to probability density measure or to implied volatility smile. On Andreasen and Huge data, it gives the following:&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/ah_dens1.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/ah_iv1.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>American Option on Forward/Futures</title>
      <link>https://chasethedevil.github.io/post/american-option-on-forwardfutures/</link>
      <pubDate>Thu, 21 Nov 2013 11:17:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/american-option-on-forwardfutures/</guid>
      <description>&lt;p&gt;Prices of a Future contract and a Forward contract are the same under the Black-Scholes assumptions (deterministic rates) but the price of options on Futures or options on Forwards might still differ. I did not find this obvious at first.&lt;/p&gt;&#xA;&lt;p&gt;For example, when the underlying contract expiration date (Futures, Forward) is different from the option expiration date. For a Future Option, the Black-76 formula can be used, the discounting is done from the option expiry date, because one receives the cash on expiration due to the margin account. For a Forward Option, the discounting need to be done from the Forward contract expiry date.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Spikes in Heston/Schobel-Zhu Local Volatility</title>
      <link>https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/</link>
      <pubDate>Wed, 20 Nov 2013 13:33:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/</guid>
      <description>&lt;p&gt;Using precise vanilla option pricing engine for Heston or Schobel-Zhu, like the Cos method with enough points and a large enough truncation can still lead to spikes in the Dupire local volatility (using the variance based formula).&lt;/p&gt;&#xA;&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-11-20%2012%2053%2032.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Local volatility&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA; &lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-11-20%2012%2054%2039.png&#34;&gt;&lt;figcaption&gt;&#xA;       &lt;h4&gt;Implied volatility&lt;/h4&gt;&#xA;     &lt;/figcaption&gt;&#xA; &lt;/figure&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;The large spikes in the local volatility 3d surface are due to constant extrapolation, but there are spikes even way before the extrapolation takes place at longer maturities. Even if the Cos method is precise, it seems to be not enough, especially for large strikes so that the second derivative over the strike combined with the first derivative over time can strongly oscillate.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local Stochastic Volatility with Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/</link>
      <pubDate>Wed, 16 Oct 2013 16:14:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/</guid>
      <description>I always imagined local stochastic volatility to be complicated, and thought it would be very slow to calibrate.&lt;br /&gt;&lt;br /&gt;After reading a bit about it, I noticed that the calibration phase could just consist in calibrating independently a Dupire local volatility model and a stochastic volatility model the usual way.&lt;br /&gt;&lt;br /&gt;One can then choose to compute on the fly the local volatility component (not equal the Dupire one, but including the stochastic adjustment) in the Monte-Carlo simulation to price a product. &lt;br /&gt;&lt;br /&gt;There are two relatively simple algorithms that are remarkably similar, one by Guyon and Henry-Labordère in &#34;&lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1885032&#34;&gt;The Smile Calibration Problem Solved&lt;/a&gt;&#34;:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s1600/Screenshot+from+2013-10-16+15:51:46.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;157&#34; src=&#34;http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s400/Screenshot+from+2013-10-16+15:51:46.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;And one from Van der Stoep, Grzelak &amp;amp; Oosterlee &#34;&lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDIQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fabstract%3D2278122&amp;amp;ei=255eUqaEDMaxhAfdqoBI&amp;amp;usg=AFQjCNF2KqSTT2ouvAyiA2J77foOFTzMKw&amp;amp;sig2=fzb4vlDPp49Hp1oT5Wja4A&amp;amp;bvm=bv.54176721,d.ZG4&#34;&gt;The Heston Stochastic-Local Volatility Model: Efficient Monte Carlo Simulation&lt;/a&gt;&#34;:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s1600/Screenshot+from+2013-10-16+16:01:01.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;63&#34; src=&#34;http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s400/Screenshot+from+2013-10-16+16:01:01.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s1600/Screenshot+from+2013-10-16+15:59:08.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;256&#34; src=&#34;http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s400/Screenshot+from+2013-10-16+15:59:08.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;In the particle method, the delta function is a regularizing kernel approximating the Dirac function. If we use the notation of the second paper, we have a = psi.&lt;br /&gt;&lt;br /&gt;The methods are extremely similar, the evaluation of the expectation is slightly different, but even that is not very different. The disadvantage is that all paths are needed at each time step. As a payoff is evaluated over one full path, this is quite memory intensive.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Heston, Schobel-Zhu, Bates, Double-Heston Fit</title>
      <link>https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit/</link>
      <pubDate>Mon, 07 Oct 2013 19:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit/</guid>
      <description>I did some experiments fitting Heston, Schobel-Zhu, Bates and Double-Heston to a real world equity index implied volatility surface. I used a global optimizer (differential evolution).&lt;br /&gt;&lt;br /&gt;To my surprise, the Heston fit is quite good: the implied volatility error is less than 0.42% on average. Schobel-Zhu fit is also good (0.47% RMSE), but a bit worse than Heston. Bates improves quite a bit on Heston although it has 3 more parameters, we can see the fit is better for short maturities (0.33% RMSE). Double-Heston has the best fit overall but it is not that much better than simple Heston while it has twice the number of parameters, that is 10 (0.24 RMSE). Going beyond, for example Triple-Heston, does not improve anything, and the optimization becomes very challenging. With Double-Heston, I already noticed that kappa is very low (and theta high) for one of the processes, and kappa is very high (and theta low) for the other process: so much that I had to add a penalty to enforce constraints in my local optimizer. The best fit is at the boundary for kappa and theta. So double Heston already feels over-parameterized.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-C_IidKxPcx4/UlKmCDJFLmI/AAAAAAAAG0o/PxOjIyGqdQQ/s1600/heston_cos3d.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;480&#34; src=&#34;http://3.bp.blogspot.com/-C_IidKxPcx4/UlKmCDJFLmI/AAAAAAAAG0o/PxOjIyGqdQQ/s640/heston_cos3d.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Heston volatility error&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-rvjEFAbCS_o/UlKmXZc--7I/AAAAAAAAG0s/yomyHTMJU18/s1600/schobelzhu_cos3d.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;480&#34; src=&#34;http://3.bp.blogspot.com/-rvjEFAbCS_o/UlKmXZc--7I/AAAAAAAAG0s/yomyHTMJU18/s640/schobelzhu_cos3d.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Schobel-Zhu  volatility error&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-_628A9dt_nU/UlKmf4oWujI/AAAAAAAAG00/b8fU_hw_oOY/s1600/bates_cos3d.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;480&#34; src=&#34;http://4.bp.blogspot.com/-_628A9dt_nU/UlKmf4oWujI/AAAAAAAAG00/b8fU_hw_oOY/s640/bates_cos3d.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Bates  volatility error&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-ioz8CuCWNmU/UlKmonoZozI/AAAAAAAAG08/eOTk8KiqxD0/s1600/double_heston_cos3d.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;480&#34; src=&#34;http://4.bp.blogspot.com/-ioz8CuCWNmU/UlKmonoZozI/AAAAAAAAG08/eOTk8KiqxD0/s640/double_heston_cos3d.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Double Heston  volatility error&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;Another advantage of Heston is that one can find tricks to find a good initial guess for a local optimizer.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update October 7: &lt;/b&gt;My initial fit relied only on differential evolution and was not the most stable as a result. Adding Levenberg-Marquardt at the end stabilizes the fit, and improves the fit a lot, especially for Bates and Double Heston. I updated the graphs and conclusions accordingly. Bates fit is not so bad at all.</description>
    </item>
    <item>
      <title>Second Cumulant of Heston</title>
      <link>https://chasethedevil.github.io/post/second-cumulant-of-heston/</link>
      <pubDate>Thu, 03 Oct 2013 17:27:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/second-cumulant-of-heston/</guid>
      <description>&lt;p&gt;I recently stumbled upon an error in the various papers related to the Heston &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=3&amp;amp;cad=rja&amp;amp;ved=0CEEQFjAC&amp;amp;url=http%3A%2F%2Fta.twi.tudelft.nl%2Fmf%2Fusers%2Foosterle%2Foosterlee%2FCOS.pdf&amp;amp;ei=qYxNUqryFqXJ0QW5u4HYDA&amp;amp;usg=AFQjCNGaMK8Lotud1DP5qReeLWgpoCA0aA&amp;amp;sig2=fljLmTq0WhG8SCgzaOxXxA&amp;amp;bvm=bv.53537100,d.d2k&#34;&gt;Cos method&lt;/a&gt; regarding the second cumulant. It is used to define the boundaries of the Cos method. Letting phi be Heston characteristic function, the cumulant generating function is:&#xA;$$g(u) = \log(\phi(-iu))$$&lt;/p&gt;&#xA;&lt;p&gt;And the second cumulant is defined a:&#xA;$$c_2 = g&amp;rsquo;&amp;rsquo;(0)$$&lt;/p&gt;&#xA;&lt;p&gt;Compared to a numerical implementation, the c_2 from the paper is really off in many use cases.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maxima for Symbolic Calculus</title>
      <link>https://chasethedevil.github.io/post/maxima-for-symbolic-calculus/</link>
      <pubDate>Wed, 02 Oct 2013 15:06:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/maxima-for-symbolic-calculus/</guid>
      <description>&lt;p&gt;A few years ago, I found an interesting open source symbolic calculus software called &lt;a href=&#34;http://www-fourier.ujf-grenoble.fr/~parisse/giac.html%E2%80%8E&#34;&gt;Xcas&lt;/a&gt;. It can however be quickly limited, for example, it does not seem to work well to compute Taylor expansions with several embedded functions. Google pointed me to another popular open source package, &lt;a href=&#34;http://maxima.sourceforge.net/&#34;&gt;Maxima&lt;/a&gt;. It looks a bit rudimentary (command like interface), but formulas can actually be very easily exported to latex with the tex command. Here is a simple example:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Making Classic Heston Integration Faster than the Cos Method</title>
      <link>https://chasethedevil.github.io/post/making-classic-heston-integration-faster-than-the-cos-method/</link>
      <pubDate>Thu, 05 Sep 2013 17:35:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/making-classic-heston-integration-faster-than-the-cos-method/</guid>
      <description>&lt;p&gt;A coworker pointed to me that Andersen and Piterbarg book &amp;ldquo;Interest Rate Modeling&amp;rdquo; had a chapter on Fourier integration applied to Heston. The authors rely on the Lewis formula to price vanilla call options under Heston.&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/lewis_formula.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Lewis formula&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;&#xA;&lt;p&gt;More importantly, they strongly advise the use of a Black-Scholes control variate. I had read about that idea before, and actually tried it in the Cos method, but it did not improve anything for the Cos method. So I was a bit sceptical. I decided to add the control variate to &lt;a href=&#34;https://chasethedevil.github.io/post/attari-lord-kahl--cos-methods-comparison-on-heston/&#34;&gt;my Attari code&lt;/a&gt;. The results were very encouraging. So I pursued on implementing the Lewis formula and their basic integration scheme (no change of variable).&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/attari_formula.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Attari formula&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/carrmadan_formula.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Carr-Madan formula (used by Lord-Kahl)&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/heston_formula.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Heston formula&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/cos_formula.png&#34;&gt;&lt;figcaption&gt;&#xA;      &lt;h4&gt;Cos formula&lt;/h4&gt;&#xA;    &lt;/figcaption&gt;&#xA;&lt;/figure&gt;&#xA;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Attari, Lord-Kahl &amp; Cos Methods Comparison on Heston</title>
      <link>https://chasethedevil.github.io/post/attari-lord-kahl--cos-methods-comparison-on-heston/</link>
      <pubDate>Wed, 28 Aug 2013 17:54:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/attari-lord-kahl--cos-methods-comparison-on-heston/</guid>
      <description>&lt;p&gt;I recently wrote about the &lt;a href=&#34;https://chasethedevil.github.io/post/the-cos-method-for-heston/&#34;&gt;Cos method&lt;/a&gt;. While rereading the various papers on Heston semi-analytical pricing, especially the &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDAQFjAA&amp;amp;url=http%3A%2F%2Fpfadintegral.com%2Fdocs%2FSchmelzle2010%2520Fourier%2520Pricing.pdf&amp;amp;ei=qREeUpO9HuPE0QXfn4DADw&amp;amp;usg=AFQjCNHANjSlqO6-o5ZfWR8xLpoVT7d5XA&amp;amp;sig2=xeon4_iLfpw8HpKz-0PQQA&amp;amp;bvm=bv.51156542,d.d2k&#34;&gt;nice summary by Schmelzle&lt;/a&gt;, it struck me how close were the Attari/Bates methods and the Cos method derivations. I then started wondering if Attari was really much worse than the Cos method or not.&lt;/p&gt;&#xA;&lt;p&gt;I noticed that Attari method accuracy is directly linked to the underlying Gaussian quadrature method accuracy. I found that the doubly adaptive Newton-Cotes quadrature by Espelid (coteda) was the most accurate/fastest on this problem (compared to Gauss-Laguerre/Legendre/Extrapolated Simpson/Lobatto). If the accuracy of the integration is 1e-6, Attari maximum accuracy will also be 1E-6, this means that very out of the money options will be completely mispriced (might even be negative). In a sense it is similar to what I observed on the Cos method.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Julia and the Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</link>
      <pubDate>Tue, 13 Aug 2013 15:52:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;I just stumbled upon &lt;!-- raw HTML omitted --&gt;Julia&lt;!-- raw HTML omitted --&gt;, a new programming language aimed at numerical computation. It&amp;rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library. I had not heard of this algorithm or this library before, but the author, &lt;!-- raw HTML omitted --&gt;Steven G. Johnson&lt;!-- raw HTML omitted --&gt;, claims it is faster and as precise as Cody &amp;amp; SLATEC implementations. As &lt;!-- raw HTML omitted --&gt;I previously had a look at those algorithms&lt;!-- raw HTML omitted --&gt; and was quite impressed by Cody&amp;rsquo;s implementation.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The &lt;!-- raw HTML omitted --&gt;source of Faddeeva&lt;!-- raw HTML omitted --&gt; shows a big list (100) of Chebychev expansions for various ranges of a normalized error function. I slightly modified the Faddeva code to compute directly the cumulative normal distribution, avoiding some exp(-x&lt;em&gt;x)&lt;em&gt;exp(x&lt;/em&gt;x) calls on the way.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Is it as accurate? I compared against a high precision implementation as in my previous test of cumulative normal distribution algorithms. And after replacing the exp(-x&lt;/em&gt;x) with &lt;!-- raw HTML omitted --&gt;Cody&amp;rsquo;s trick&lt;!-- raw HTML omitted --&gt; to compute it with higher accuracy, here is how it looks (referenced as &amp;ldquo;Johnson&amp;rdquo;).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I also measured performance on various ranges, and found out that this Johnson algorithm is around 2x faster than Cody (in Scala) and 30% faster than my optimization of Cody (using a table of exponentials for Cody&amp;rsquo;s trick).&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>The COS method for Heston</title>
      <link>https://chasethedevil.github.io/post/the-cos-method-for-heston/</link>
      <pubDate>Fri, 02 Aug 2013 14:19:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-cos-method-for-heston/</guid>
      <description>&lt;p&gt;Fang, in &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CC4QFjAA&amp;amp;url=http%3A%2F%2Frepository.tudelft.nl%2Fassets%2Fuuid%3A9aa17357-af21-4c09-86a2-3904ced4b873%2Fthesis.pdf&amp;amp;ei=Epf7Uam8CYWChQeCroCgCw&amp;amp;usg=AFQjCNGyjjlwi-ylN6cl2xoUp5A32wwePA&amp;amp;sig2=m-qvIkWMgVH-qw4hq_Y5Ow&amp;amp;bvm=bv.50165853,d.ZG4%22&#34;&gt;her thesis&lt;/a&gt;, has the idea of the COS method and applies it to Heston. There are several published papers around it to price options under various models that have a known characteristic function, as well as to price more exotic options like barriers or bermudans.&lt;/p&gt;&#xA;&lt;p&gt;The COS method is very close to the more standard Heston quasi analytic formula (use transform of characteristic function for the density and integrates the payoff with the density, exchanging summation), except that the more simple &lt;a href=&#34;http://en.wikipedia.org/wiki/Fourier_series&#34;&gt;Fourier series&lt;/a&gt; are used instead of the standard Fourier transform. As a consequence there are a few more approximations that are done related to the truncation of the domain of integration and the result is already discrete, so no need for a Gaussian quadrature.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Octave vs Scilab for PDEs in Finance</title>
      <link>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</link>
      <pubDate>Tue, 30 Jul 2013 12:10:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/</guid>
      <description>&lt;p&gt;I was used to &lt;a href=&#34;https://www.scilab.org&#34;&gt;Scilab&lt;/a&gt; for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.&lt;/p&gt;&#xA;&lt;p&gt;Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the &lt;a href=&#34;https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/&#34;&gt;Arbitrage Free SABR problem&lt;/a&gt;. It worked nicely and is a bit nicer to read than my equivalent Scala program. But I was a bit surprised by the low performance.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The CUDA Performance Myth II</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</link>
      <pubDate>Fri, 12 Jul 2013 15:23:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth-ii/</guid>
      <description>&lt;p&gt;This is a kind of following to the &lt;a href=&#34;http://chasethedevil.github.io/post/the-cuda-performance-myth/&#34;&gt;CUDA performance myth&lt;/a&gt;. There is a recent news on the java concurrent mailing list about &lt;a href=&#34;http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/SplittableRandom.java?revision=1.7&amp;amp;view=markup&#34;&gt;SplittableRandom class&lt;/a&gt; proposed for JDK8. It is a new parallel random number generator a priori usable for Monte-Carlo simulations.&lt;/p&gt;&#xA;&lt;p&gt;It seems to rely on some very recent algorithm. There are some a bit older ones: the ancestor, L&amp;rsquo;Ecuyer &lt;a href=&#34;http://www.iro.umontreal.ca/~simardr/rng/MRG32k3a.c&#34;&gt;MRG32k3a&lt;/a&gt; that can be parallelized through relatively costless skipTo methods, a Mersenne Twister variant &lt;a href=&#34;http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MTGP/&#34;&gt;MTGP&lt;/a&gt;, and even the less rigourous XorWow popularized by NVidia CUDA.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bessel and Harmonic Kinks in the Forward</title>
      <link>https://chasethedevil.github.io/post/bessel-and-harmonic-kinks-in-the-forward/</link>
      <pubDate>Tue, 02 Jul 2013 15:44:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/bessel-and-harmonic-kinks-in-the-forward/</guid>
      <description>As Bessel (sometimes called Hermite) spline interpolation is only &lt;a href=&#34;http://en.wikipedia.org/wiki/Smooth_function#The_space_of_Ck_functions&#34;&gt;C1&lt;/a&gt;, like the Harmonic spline from Fritsch-Butland, the forward presents small kinks compared to a standard cubic spline. Hyman filtering also creates a kink where it fixes the monotonicity. Those are especially visible with a log scale in time. Here is how it looks on the Hagan-West difficult curve.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-xsT1uMr-S6c/UdLZE3SjlEI/AAAAAAAAGg0/lESF6PVyLkk/s942/snapshot30.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;369&#34; src=&#34;http://4.bp.blogspot.com/-xsT1uMr-S6c/UdLZE3SjlEI/AAAAAAAAGg0/lESF6PVyLkk/s640/snapshot30.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>The Finite Difference Theta Scheme Optimal Theta</title>
      <link>https://chasethedevil.github.io/post/the-finite-difference-theta-scheme-optimal-theta/</link>
      <pubDate>Tue, 18 Jun 2013 15:02:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-finite-difference-theta-scheme-optimal-theta/</guid>
      <description>&lt;p&gt;The theta finite difference scheme is a common generalization of Crank-Nicolson. In finance, the &lt;a href=&#34;http://www.amazon.com/Wilmott-Quantitative-Finance-Volume-Edition/dp/0470018704/ref=sr_1_1?ie=UTF8&amp;amp;qid=1371557569&amp;amp;sr=8-1&amp;amp;keywords=paul+wilmott+on+quantitative+finance&#34;&gt;book from Wilmott&lt;/a&gt;, a &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDEQFjAA&amp;amp;url=http%3A%2F%2Fwww.javaquant.net%2Fpapers%2Ffdpaper.pdf&amp;amp;ei=DE7AUf-XHo24hAfkvICwBg&amp;amp;usg=AFQjCNHD0qmjyMZtzbLfao3YHCFwySYixg&amp;amp;sig2=9DdLJ9FoVCXoeaus5JykQg&amp;amp;bvm=bv.47883778,d.ZG4&#34;&gt;paper from A. Sepp&lt;/a&gt;, &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;sqi=2&amp;amp;ved=0CDcQFjAB&amp;amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F4451%2Fv1n2a1b.pdf&amp;amp;ei=6k7AUZXXFYXLhAeF3YDYDA&amp;amp;usg=AFQjCNFq4dBzQ54M34qd0DMQ4pgSCcjfQg&amp;amp;sig2=AR3zjeDQ3WIvWOvt-1tNeg&amp;amp;bvm=bv.47883778,d.ZG4&#34;&gt;one from Andersen-Ratcliffe&lt;/a&gt; present it. Most of the time, it&amp;rsquo;s just a convenient way to handle implicit \(\theta=1\), explicit \(\theta=0\) and Crank-Nicolson \(\theta=0.5\) with the same algorithm.&lt;/p&gt;&#xA;&lt;p&gt;Wilmott makes an interesting remark: one can choose a theta that will cancel out higher order terms in the local truncation error and therefore should lead to increased accuracy. $$\theta = \frac{1}{2}- \frac{(\Delta x)^2}{12 b \Delta t} $$&#xA;where \(b\) is the diffusion coefficient.&#xA;This leads to \(\theta &amp;lt; \frac{1}{2}\), which means the scheme is not unconditionally stable anymore but needs to obey (see Morton &amp;amp; Mayers p 30):&#xA;$$b \frac{\Delta t}{(\Delta x)^2} \leq \frac{5}{6}$$&#xA;and to ensure that \(\theta \geq 0 \):&#xA;$$b \frac{\Delta t}{(\Delta x)^2} \geq \frac{1}{6}$$&lt;/p&gt;</description>
    </item>
    <item>
      <title>Akima for Yield Curve Interpolation ?</title>
      <link>https://chasethedevil.github.io/post/akima-for-yield-curve-interpolation-/</link>
      <pubDate>Mon, 03 Jun 2013 00:07:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/akima-for-yield-curve-interpolation-/</guid>
      <description>On my test of &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;ved=0CDgQFjAB&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2175002&amp;amp;ei=d8GrUb70CJCChQeA7IHoCg&amp;amp;usg=AFQjCNHHizgzORef228lnYX3HygLb9okAg&amp;amp;sig2=NYbhD30aD7sD8TS7CYodzw&amp;amp;bvm=bv.47244034,d.ZG4&#34;&gt;yield curve interpolations&lt;/a&gt;, focusing on parallel delta versus sequential delta, &lt;a href=&#34;http://200.17.213.49/lib/exe/fetch.php/wiki:internas:biblioteca:akima.pdf&#34;&gt;Akima&lt;/a&gt; is the worst of the lot. I am not sure why this interpolation is still popular when most alternatives seem much better. Hyman presented some of the issues with Akima in &lt;a href=&#34;http://epubs.siam.org/doi/abs/10.1137/0904045&#34;&gt;his paper&lt;/a&gt; in 1983. &lt;br /&gt;&lt;br /&gt;In the following graph, a higher value is a higher parallel-vs-sequential difference. &lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-y1jkd6Pu4Y8/UavBtMgvLQI/AAAAAAAAGc4/CDRwTqv-suc/s1600/snapshot18.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;272&#34; src=&#34;http://4.bp.blogspot.com/-y1jkd6Pu4Y8/UavBtMgvLQI/AAAAAAAAGc4/CDRwTqv-suc/s400/snapshot18.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;That plus the Hagan-West example of a tricky curve looks a bit convoluted with it (although it does not have any negative forward).&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-kKk7NEYtUaw/UavB245C8OI/AAAAAAAAGdA/elBq-es3jWY/s1600/snapshot19.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;225&#34; src=&#34;http://4.bp.blogspot.com/-kKk7NEYtUaw/UavB245C8OI/AAAAAAAAGdA/elBq-es3jWY/s400/snapshot19.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;I have used Quantlib implementation, those results make me wonder if there is not something wrong with the boundaries.</description>
    </item>
    <item>
      <title>2 Ways for an Accurate Barrier with Finite Difference </title>
      <link>https://chasethedevil.github.io/post/2-ways-for-an-accurate-barrier-with-finite-difference/</link>
      <pubDate>Sun, 02 Jun 2013 00:46:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/2-ways-for-an-accurate-barrier-with-finite-difference/</guid>
      <description>I had explored the issue of pricing a barrier using finite difference discretization of the Black-Scholes PDE a few years ago. Briefly, for explicit schemes, one just need to place the barrier on the grid and not worry about much else, but for implicit schemes, either the barrier should be placed on the grid and the grid&amp;nbsp; &lt;b&gt;truncated &lt;/b&gt;at the barrier, or a &lt;b&gt;fictitious point&lt;/b&gt; should be introduced to force the correct price at the barrier level (0, typically).&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-BDNo4x-nxVw/Uap5CCA8BJI/AAAAAAAAGco/SxT1WlDrIbA/s1600/barrier_grid.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;135&#34; src=&#34;http://1.bp.blogspot.com/-BDNo4x-nxVw/Uap5CCA8BJI/AAAAAAAAGco/SxT1WlDrIbA/s400/barrier_grid.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The fictitious point approach is interesting for the case of varying rebates, or when the barrier moves around. I first saw this idea in the book &#34;Paul Wilmott on Quantitative Finance&#34;.&lt;br /&gt;&lt;br /&gt;Recently, I noticed that Hagan made use of the ficitious point approach in its &#34;Arbitrage free SABR&#34; paper, specifically he places the barrier in the middle of 2 grid points. There is very little difference between truncating the grid and the fictitious point for a constant barrier.&lt;br /&gt;&lt;br /&gt;In this specific case there is a difference because there are 2 additional ODE solved on the same grid, at the boundaries. I was especially curious if one could place the barrier exactly at 0 with the fictitious point, because then one would potentially need to evaluate coefficients for negative values. It turns out you can, as values at the fictitious point are actually not used: the mirror point inside is used because of the mirror boundary conditions.&lt;br /&gt;&lt;br /&gt;So the only difference is the evaluation of the first derivative at the barrier (used only for the ODE): the fictitious point uses the value at barrier+h/2 where h is the space between two points at the same timestep, while the truncated barrier uses a value at barrier+h (which can be seen as standard forward/backward first order finite difference discretization at the boundaries). For this specific case, the fictitious point will be a little bit more precise for the ODE.</description>
    </item>
    <item>
      <title>SABR with the new Hagan PDE Approach</title>
      <link>https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</link>
      <pubDate>Tue, 28 May 2013 15:56:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/</guid>
      <description>At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it&#39;s just like a regular PDE with proper boundary conditions.&lt;br /&gt;&lt;br /&gt;I was wondering how it compared to Andreasen Huge results.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s1600/snapshot14.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s640/snapshot14.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;My first implementation was quite slow. I postulated it was likely the Math.pow function calls. It turns out they could be reduced a great deal. As a result, it&#39;s now quite fast. But it would still be much slower than Andreasen Huge. Typically, one might use 40 time steps, while Andreasen Huge is 1, so it could be around a 40 to 1 ratio. In practice it&#39;s likely to be less than 10x slower, but still.&lt;br /&gt;&lt;br /&gt;While looking at the implied volatilities I found something intriguing with Andreasen Huge: the implied volatilities from the refined solution using the corrected forward volatility look further away from the Hagan implied volatilitilies than without adjustment, and it&#39;s quite pronounced at the money.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s1600/snapshot15.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s640/snapshot15.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;Interestingly, the authors don&#39;t plot that graph in their paper. They  plot a similar graph of their own closed form analytic formula, that is  in reality used to compute the forward volatility. I suppose that  because they calibrate and price through their method, they don&#39;t really  care so much that the ATM prices don&#39;t match Hagan original formula.&lt;br /&gt;&lt;br /&gt;We can see something else on that graph: Hagan PDE boundary is not as nice as Andreasen Huge boundary for high strikes (they use a Hagan like approx at the boundaries, this is why it crosses the Hagan implied volatilities there). &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s1600/snapshot16.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s640/snapshot16.png&#34; height=&#34;304&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;If we use a simple option gamma = 0 boundary in Andreasen Huge, this results in a very similar shape as the Hagan PDE. This is because the option price is effectively 0 at the boundary.&lt;br /&gt;Hagan chose a specifically taylored Crank-Nicolson scheme. I was wondering how it fared when I reduced the number of time-steps. &lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s1600/snapshot17.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s400/snapshot17.png&#34; height=&#34;190&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The answer is: not good. This is the typical Crank-Nicolson issue. It could be interesting to adapt the method to use Lawson-Morris-Goubet or TR-BDF2, or a simple Euler Richardson extrapolation. This would allow to use less time steps, as in practice, the accuracy is not so bad with 10 time steps only.&lt;br /&gt;&lt;br /&gt;What I like about the Hagan PDE approach is that the implied vols and the probability density converge well to the standard Hagan formula, when there is no negative density problem, for example for shorter maturities. This is better than Andreasen Huge, where there seems to be always 1 vol point difference. However their method is quite slow compared to the original simple analytic formula.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update March 2014&lt;/b&gt; - I have now a paper around this &#34;&lt;a href=&#34;http://ssrn.com/abstract=2402001&#34;&gt;Finite Difference Techniques for Arbitrage Free SABR&lt;/a&gt;&#34;</description>
    </item>
    <item>
      <title>SABR with Andreasen-Huge</title>
      <link>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</link>
      <pubDate>Fri, 24 May 2013 14:17:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</guid>
      <description>I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.&lt;br /&gt;&lt;br /&gt;Around 2 years ago, I saw a presentation from &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDAQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;amp;ei=-VOfUYncL8HB7AbK0YHgBg&amp;amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;amp;sig2=SChIkU-TBR7ECaLdDm1orA&amp;amp;bvm=bv.47008514,d.ZGU&#34;&gt;Andreasen and Huge about how they were able to price/calibrate SABR&lt;/a&gt; by a one-step finite difference technique. At that time, I did not understand much their idea. My mind was too focused on more classical finite differences techniques and not enough on the big picture in their idea. Their idea is quite general and can be applied to much more than SABR. &lt;br /&gt;&lt;br /&gt;Recently there has been some talk and development going on where I work about SABR (a popular way to interpolate the option implied volatility surface for interest rate derivatives), especially regarding the implied volatility wings at low strike, and sometimes on how to price in a negative rates environment. There are actually quite a bit of research papers around this. I am not really working on that part so I just mostly listened. Then a former coworker suggested that the Andreasen Huge method was actually what banks seemed to choose in practice. A few weeks later, the Thalesians (a group for people interested in quantitative finance) announced a presentation by Hagan (one of the inventor of SABR) about a technique that sounded very much like Andreasen-Huge&amp;nbsp; to deal with the initial SABR issues in low rates.&lt;br /&gt;&lt;br /&gt;As the people working on this did not investigate Andreasen-Huge technique, I somehow felt that I had to and that maybe, this time, I would be able to grasp their idea.&lt;br /&gt;&lt;br /&gt;It took me just a few hours to have meaningful results. Here is the price of out of the money vanilla options using alpha = 0.0758194, nu = 0.1, beta = 0.5, rho = -0.1, forward = 0.02, and a maturity of 2 years.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s1600/Screenshot+from+2013-05-24+13:29:24.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;212&#34; src=&#34;http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s400/Screenshot+from+2013-05-24+13:29:24.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s1600/Screenshot+from+2013-05-24+13:30:09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;215&#34; src=&#34;http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s400/Screenshot+from+2013-05-24+13:30:09.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;I did not have in my home library a way to find the implied volatility for a given price. I knew of 2 existing methods, &lt;a href=&#34;http://www.pjaeckel.webspace.virginmedia.com/ByImplication.pdf&#34;&gt;Jaeckel &#34;By Implication&#34;&lt;/a&gt;, and &lt;a href=&#34;http://scholar.google.fr/citations?view_op=view_citation&amp;amp;hl=fr&amp;amp;user=3GRhH_IAAAAJ&amp;amp;citation_for_view=3GRhH_IAAAAJ:d1gkVwhDpl0C&#34;&gt;Li rational functions&lt;/a&gt; approach. I discovered that Li wrote &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/14697680902849361&#34;&gt;a new paper&lt;/a&gt; on the subject where he uses a SOR method to find the implied volatility and claims it&#39;s very accurate, very fast and very robust. Furthermore, the same idea can be applied to normal implied volatility. What attracted me to it is the simplicity of the underlying algorithm. Jaeckel&#39;s way is a nice way to do Newton-Raphson, but there seems to be so many things to &#34;prepare&#34; to make it work in most cases, that I felt it would be too much work for my experiment. It took me a few more hours to code Li SOR solvers, but it worked amazingly well for my experiment.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s1600/Screenshot+from+2013-05-24+13%253A31%253A33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;215&#34; src=&#34;http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s400/Screenshot+from+2013-05-24+13%253A31%253A33.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s1600/Screenshot+from+2013-05-24+13%253A37%253A51.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;211&#34; src=&#34;http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s400/Screenshot+from+2013-05-24+13%253A37%253A51.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;At first I had an error in my boundary condition and had no so good results especially with a long maturity. The traps with Andreasen-Huge technique are very much the same as with classical finite differences: be careful to place the strike on the grid (eventually smooth it), and have good boundaries.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Large Steps in Schobel-Zhu/Heston the Lazy Way</title>
      <link>https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/</link>
      <pubDate>Fri, 17 May 2013 12:46:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/</guid>
      <description>&lt;a href=&#34;http://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCoQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1485403&amp;amp;ei=ODqVUY-8GY2v7AaSsIHgDw&amp;amp;usg=AFQjCNGxk1TqaYu0mxni-OQib6V6lU-M0g&amp;amp;sig2=xzLPCiO5kdF97KN4Tz474A&amp;amp;bvm=bv.46471029,d.ZGU&#34;&gt;Van Haastrecht, Lord and Pelsser&lt;/a&gt; present an effective way to price derivatives by Monte-Carlo under the Schobel-Zhu model (as well as under the Schobel-Zhu-Hull-White model). It&#39;s quite similar to Andersen QE scheme for Heston in spirit.&lt;br /&gt;&lt;br /&gt;In their paper they evolve the (log) asset process together with the volatility process, using the same discretization times. A while ago, when looking at&amp;nbsp; &lt;a href=&#34;http://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCoQFjAA&amp;amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;amp;bvm=bv.46471029,d.ZGU&#34;&gt;Joshi and Chan&lt;/a&gt; large steps for Heston, I noticed that, inspired by Broadie-Kaya exact Heston scheme, they present the idea to evolve the variance process using small steps and the asset process using large steps (depending on the payoff) using the integrated variance value computed by small steps. The asset steps correspond to payoff evaluation dates&amp;nbsp; At that time I had applied this idea to Andersen QE scheme and it worked reasonably well.&lt;br /&gt;&lt;br /&gt;So I tried to apply the same logic to Schobel Zhu, and my first tests show that it works too. Interestingly, the speed gain is about 2x. Here are the results for a vanilla call option of different strikes.&lt;br /&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s1600/Screenshot+from+2013-05-17+12:33:07.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;276&#34; src=&#34;http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s400/Screenshot+from+2013-05-17+12:33:07.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Similar Error between long and short asset steps&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s1600/Screenshot+from+2013-05-17+12:32:50.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;276&#34; src=&#34;http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s400/Screenshot+from+2013-05-17+12:32:50.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Long steps take around 1/2 the time to compute&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;I would have expected the difference in performance to increase when the step size is decreasing, but it&#39;s not the case on my computer.&lt;br /&gt;&lt;br /&gt;It&#39;s not truly large steps like &lt;a href=&#34;http://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CCoQFjAA&amp;amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;amp;bvm=bv.46471029,d.ZGU&#34;&gt;Joshi and Chan&lt;/a&gt; do in their integrated double gamma scheme as the variance is still discretized in relatively small steps in my case, but it seems like a good, relatively simple optimization. A while ago, I did also implement the full Joshi and Chan scheme, but it&#39;s really interesting if one is always looking for long steps: it is horribly slow when the step size is small, which might occur for many exotic payoffs, while Andersen QE scheme perform almost as well as log-Euler in terms of computational cost.</description>
    </item>
    <item>
      <title>Exact Forward in Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</link>
      <pubDate>Mon, 13 May 2013 17:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</guid>
      <description>&lt;p&gt;Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.&lt;/p&gt;&#xA;&lt;p&gt;In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &amp;ldquo;The Equity Option Volatility Smile: a Finite Difference Approach&amp;rdquo; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quasi Monte Carlo in Finance</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/</link>
      <pubDate>Mon, 13 May 2013 13:16:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/</guid>
      <description>I have been wondering if there was any better alternative than the standard Sobol (+ Brownian Bridge) quasi random sequence generator for the Monte Carlo simulations of finance derivatives.&lt;br /&gt;&lt;br /&gt;Here is what I found:&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Scrambled Sobol. The idea is to rerandomize the quasi random numbers slightly. It can provide better uniformity properties and allows for a real estimate of the standard error. There are many ways to do that. The simple Cranley Patterson rotation consisting in adding a pseudo random number modulo 1, Owen scrambling (permutations of the digits) and simplifications of it to achieve a reasonable speed. This is all very well described in &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;cad=rja&amp;amp;ved=0CFwQFjAF&amp;amp;url=http%3A%2F%2Fwww-stat.stanford.edu%2F~owen%2Fcourses%2F362%2Freadings%2Fsiggraph03.pdf&amp;amp;ei=08CQUea-F4jMhAfx5YDgCA&amp;amp;usg=AFQjCNGLnKapkdJ4_caiSE3Ro_kf21NvkQ&amp;amp;sig2=j2b_JqQuO9JNU0ko7yTeOw&amp;amp;bvm=bv.46340616,d.ZG4&#34;&gt;Owen Quasi Monte Carlo document&lt;/a&gt; &lt;/li&gt;&lt;li&gt;Lattice rules. It is another form of quasi random sequences, which so far was not very well adapted to finance problems. A &lt;a href=&#34;https://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=2&amp;amp;cad=rja&amp;amp;ved=0CDYQFjAB&amp;amp;url=https%3A%2F%2Fwww.maths.unsw.edu.au%2Fsites%2Fdefault%2Ffiles%2Famr08_9_0.pdf&amp;amp;ei=ysKQUebXO4axO7ungPAN&amp;amp;usg=AFQjCNErqQvM1IyLlUJH2EX5_mVG3f-ZCw&amp;amp;sig2=gYbfQebTwUP4mtj6bteCcQ&amp;amp;bvm=bv.46340616,d.ZWU&#34;&gt;presentation from Giles &amp;amp; Kuo&lt;/a&gt; look like it&#39;s changing.&lt;/li&gt;&lt;li&gt;Fast PCA. An alternative to Brownian Bridge is the standard PCA. The problem with PCA is the performance in O(n^2). A possible speedup is possible in the case of a equidistant time steps. &lt;a href=&#34;http://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.142.5057%26rep%3Drep1%26type%3Dpdf&amp;amp;sa=U&amp;amp;ei=5MWQUaioA8KXhQfDnYCYDQ&amp;amp;ved=0CB0QFjAC&amp;amp;usg=AFQjCNHUhpr6_Ofiqqw2XeU8SY_amnx0pw&#34;&gt;This paper&lt;/a&gt; shows it can be generalized. But the data in it shows it is only advantageous for more than 1024 steps - not so interesting in Finance.&lt;/li&gt;&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Upper Bounds in American Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</link>
      <pubDate>Tue, 30 Apr 2013 17:05:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/</guid>
      <description>&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.2367&amp;amp;rep=rep1&amp;amp;type=pdf&#34;&gt;Glasserman and Yu&lt;/a&gt; (GY) give a relatively simple algorithm to compute lower and upper bounds of a the price of a Bermudan Option through Monte-Carlo.&lt;br /&gt;&lt;br /&gt;I always thought it was very computer intensive to produce an upper bound, and that the standard &lt;a href=&#34;http://escholarship.org/uc/item/43n1k4jb.pdf&#34;&gt;Longstaff Schwartz algorithm&lt;/a&gt; was quite precise already. GY algorithm is not much slower than the Longstaff-Schwartz algorithm, but what&#39;s a bit tricky is the choice of basis functions: they have to be Martingales. This is the detail I overlooked at first and I, then, could not understand why my results were so bad. I looked for a coding mistake for several hours before I figured out that my basis functions were not Martingales. Still it is possible to find good Martingales for the simple Bermudan Put option case and GY actually propose some &lt;a href=&#34;http://arxiv.org/pdf/math.PR/0503556&#34;&gt;in another paper&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Here are some preliminary results where I compare the random number generator influence and the different methods. I include results for GY using In-the-money paths only for the regression (-In suffix) or all (no suffix). &lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-c7Jhn_s05Ak/UYACCpfoRjI/AAAAAAAAGXQ/LoQscl4b4yM/s1600/gy_train16k_sim_value.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;238&#34; src=&#34;http://2.bp.blogspot.com/-c7Jhn_s05Ak/UYACCpfoRjI/AAAAAAAAGXQ/LoQscl4b4yM/s400/gy_train16k_sim_value.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;value using 16k training path and Sobol - GY-Low-In is very close to LS.&lt;/td&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-Jx0NPC1QvAs/UYACisXbFiI/AAAAAAAAGXY/mvsHmNhENcE/s1600/gy_tr16k_sim.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;300&#34; src=&#34;http://4.bp.blogspot.com/-Jx0NPC1QvAs/UYACisXbFiI/AAAAAAAAGXY/mvsHmNhENcE/s640/gy_tr16k_sim.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;error using 16k training path - a high number of simulations not that useful&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-w6Mp3BDNp-w/UYADEhUQx4I/AAAAAAAAGXg/ESRJyLUybNk/s1600/gy_sim1m_training.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;282&#34; src=&#34;http://2.bp.blogspot.com/-w6Mp3BDNp-w/UYADEhUQx4I/AAAAAAAAGXg/ESRJyLUybNk/s640/gy_sim1m_training.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;error using 1m simulation paths - GY basis functions require less training than LS&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;One can see the the upper bound is not that precise compared to the lower bound estimate, and that using only in the money paths makes a big difference. GY regression is good with only 1k paths, LS requires 10x more.&lt;br /&gt;&lt;br /&gt;Surprisingly, I noticed that the Brownian bridge variance reduction applied to Sobol was&amp;nbsp; increasing the GY low estimate, so as to make it sometimes slightly higher than Longstaff-Schwartz price.</description>
    </item>
    <item>
      <title>Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</link>
      <pubDate>Mon, 22 Apr 2013 18:00:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</guid>
      <description>In the book &lt;i&gt;&lt;a href=&#34;http://books.google.fr/books?id=e9GWUsQkPNMC&amp;amp;lpg=PA461&amp;amp;vq=longstaff&amp;amp;hl=fr&amp;amp;pg=PA459#v=snippet&amp;amp;q=longstaff&amp;amp;f=false&#34;&gt;Monte Carlo Methods in Financial Engineering&lt;/a&gt;&lt;/i&gt;, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.&lt;br /&gt;&lt;br /&gt;However Longstaff and Schwartz seem to just reuse the paths in &lt;a href=&#34;http://rfs.oxfordjournals.org/content/14/1/113.short&#34;&gt;their paper&lt;/a&gt;, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.&lt;br /&gt;&lt;br /&gt;How large is this bias? What is the correct methodology?&lt;br /&gt;&lt;br /&gt;I have tried with Sobol quasi random numbers to evaluate that bias on a simple Bermudan put option of maturity 180 days, exercisable at 30 days, 60 days, 120 days and 180 days using a Black Scholes volatility of 20% and a dividend yield of 6%. As a reference I use &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDcQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1648878&amp;amp;ei=9151UZ3pM4LZPZ-ugeAF&amp;amp;usg=AFQjCNFS9fdRJt9RoerSnb87YDIZmLcCtw&amp;amp;sig2=k8lHjhUe14ep4giVM5Mr5Q&amp;amp;bvm=bv.45512109,d.ZWU&#34;&gt;a finite difference solver based on TR-BDF2&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;I found it particularly difficult to evaluate it: should we use the same number of paths for the 2 methods or should we use the same number of paths for the monte carlo mean computation only? Should we use the same number of paths for regression and for monte carlo mean computation or should the monte carlo mean computation use much more paths?&lt;br /&gt;&lt;br /&gt;I have tried those combinations and was able to clearly see the bias only in one case: a large number of paths for the Monte-Carlo mean computation compared to the number of paths used for the regression using a fixed total number of paths of 256*1024+1, and 32*1024+1 paths for the regression.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;FDM price=2.83858387194312&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Longstaff discarded paths price=2.8385854695510426&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Longstaff reused paths price=2.8386108892756847&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Those numbers are too good to be a real. If one reduces too much the total number of paths or the number of paths for the regression, the result is not precise enough to see the bias. For example, using 4K paths for the regression leads to 2.83770 vs 2.83767. Using 4K paths for regression and only 16K paths in total leads to 2.8383 vs 2.8387. Using 32K paths for regressions and increasing to 1M paths in total leads to 2.838539 vs 2.838546.&lt;br /&gt;&lt;br /&gt;For this example the Longstaff-Schwartz price is biased low, the slight increase due to path reuse is not very visible and most of the time does not deteriorate the overall accuracy. But as a result of reusing the paths, the Longstaff-Schwartz price might be higher than the real value.</description>
    </item>
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.&lt;br /&gt;&lt;br /&gt;Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an &lt;a href=&#34;http://bad-concurrency.blogspot.co.uk/2012/08/arithmetic-overflow-and-intrinsics.html&#34;&gt;intrinsic function call&lt;/a&gt; and that should be difficult to beat. However what if one is ok for a bit lower accuracy? Could a simple &lt;a href=&#34;http://www.siam.org/books/ot99/OT99SampleChapter.pdf&#34;&gt;Chebyshev polynomial expansion&lt;/a&gt; be faster?&lt;br /&gt;&lt;br /&gt;Out of curiosity, I tried a Chebyshev polynomial expansion with 10 coefficients stored in a final double array. I computed the coefficient using a precise quadrature (Newton-Cotes) and end up with 1E-9, 1E-10 absolute and relative accuracy on [-1,1].&lt;br /&gt;&lt;br /&gt;Here are the results of a simple sum of 10M random numbers:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.75s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.48s for ChebyshevExp sum=1.718281669341388E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;So while this simple implementation is actually faster than Math.exp (but only works within [-1,1]), FastMath from Apache commons maths, that relies on a table lookup algorithm is just faster (in addition to being more precise and not limited to [-1,1]).&lt;br /&gt;&lt;br /&gt;Of course if I use only 5 coefficients, the speed is better, but the relative error becomes around 1e-4 which is unlikely to be satisfying for a finance application.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.78s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.27s for ChebyshevExp sum=1.718193001875838E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;</description>
    </item>
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part III)</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii/</link>
      <pubDate>Fri, 12 Apr 2013 13:41:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii/</guid>
      <description>&lt;p&gt;I forgot two important points in my &lt;a href=&#34;https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/&#34;&gt;previous post&lt;/a&gt; about &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord-Kahl method&lt;/a&gt; to compute the Heston call price:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;Scaling: scaling the call price appropriately allows to increase the maximum precision significantly, because the &lt;a href=&#34;http://portal.tugraz.at/portal/page/portal/Files/i5060/files/staff/mueller/FinanzSeminar2012/CarrMadan_OptionValuationUsingtheFastFourierTransform_1999.pdf&#34;&gt;Carr-Madan&lt;/a&gt; formula operates on log(Forward) and log(Strike) directly, but not the ratio, and alpha is multiplied by the log(Forward). I simply scale by the spot, the call price is (S_0*max(S/S_0-K/S0)). Here are the results for &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord-Kahl&lt;/a&gt;, &lt;a href=&#34;http://pjaeckel.webspace.virginmedia.com/NotSoComplexLogarithmsInTheHestonModel.pdf&#34;&gt;Kahl-Jaeckel&lt;/a&gt; (the more usual way limited to machine epsilon accuracy), &lt;a href=&#34;http://epubs.siam.org/doi/abs/10.1137/110830241&#34;&gt;Forde-Jacquier-Lee&lt;/a&gt; ATM implied volatility without scaling for a maturity of 1 day:&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;table&gt;&#xA;  &lt;thead&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;th&gt;Strike&lt;/th&gt;&#xA;          &lt;th&gt;Lord-Kahl&lt;/th&gt;&#xA;          &lt;th&gt;Kahl-Jaeckel&lt;/th&gt;&#xA;          &lt;th&gt;Forde-Jacquier-Lee&lt;/th&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/thead&gt;&#xA;  &lt;tbody&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;62.5&lt;/td&gt;&#xA;          &lt;td&gt;2.919316809400033E-34&lt;/td&gt;&#xA;          &lt;td&gt;8.405720564041985E-12&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;68.75&lt;/td&gt;&#xA;          &lt;td&gt;-8.923683388191852E-28&lt;/td&gt;&#xA;          &lt;td&gt;1.000266536266281E-11&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;75.0&lt;/td&gt;&#xA;          &lt;td&gt;-3.2319611910032E-22&lt;/td&gt;&#xA;          &lt;td&gt;2.454925152051146E-12&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;81.25&lt;/td&gt;&#xA;          &lt;td&gt;1.9401743410877718E-16&lt;/td&gt;&#xA;          &lt;td&gt;2.104982854689297E-12&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;87.5&lt;/td&gt;&#xA;          &lt;td&gt;-Infinity&lt;/td&gt;&#xA;          &lt;td&gt;-1.6480150577535824E-11&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;93.75&lt;/td&gt;&#xA;          &lt;td&gt;Infinity&lt;/td&gt;&#xA;          &lt;td&gt;1.8277663826893331E-9&lt;/td&gt;&#xA;          &lt;td&gt;1.948392142070432E-9&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;100.0&lt;/td&gt;&#xA;          &lt;td&gt;0.4174318393886519&lt;/td&gt;&#xA;          &lt;td&gt;0.41743183938679845&lt;/td&gt;&#xA;          &lt;td&gt;0.4174314959743768&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;106.25&lt;/td&gt;&#xA;          &lt;td&gt;1.326968012594355E-11&lt;/td&gt;&#xA;          &lt;td&gt;7.575717830832218E-11&lt;/td&gt;&#xA;          &lt;td&gt;1.1186618909114702E-11&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;112.5&lt;/td&gt;&#xA;          &lt;td&gt;-5.205783145942609E-21&lt;/td&gt;&#xA;          &lt;td&gt;2.5307755890935368E-11&lt;/td&gt;&#xA;          &lt;td&gt;6.719872683111381E-45&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;118.75&lt;/td&gt;&#xA;          &lt;td&gt;4.537094156599318E-25&lt;/td&gt;&#xA;          &lt;td&gt;1.8911094912255066E-11&lt;/td&gt;&#xA;          &lt;td&gt;3.615356241778357E-114&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;125.0&lt;/td&gt;&#xA;          &lt;td&gt;1.006555799739525E-27&lt;/td&gt;&#xA;          &lt;td&gt;3.2365221613872563E-12&lt;/td&gt;&#xA;          &lt;td&gt;2.3126009701775733E-240&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;      &lt;tr&gt;&#xA;          &lt;td&gt;131.25&lt;/td&gt;&#xA;          &lt;td&gt;4.4339539263484925E-31&lt;/td&gt;&#xA;          &lt;td&gt;2.4794388764348696E-11&lt;/td&gt;&#xA;          &lt;td&gt;0.0&lt;/td&gt;&#xA;      &lt;/tr&gt;&#xA;  &lt;/tbody&gt;&#xA;&lt;/table&gt;&#xA;&lt;p&gt;One can see negative prices and meaningless prices outside ATM. With scaling it changes to:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/&#34;&gt;previous post&lt;/a&gt;, I explored the &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord-Kahl method&lt;/a&gt; to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</link>
      <pubDate>Tue, 09 Apr 2013 19:49:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</guid>
      <description>&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-04-09%2019%2042%2009.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;I just tried to implement &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord Kahl algorithm to compute the Heston call price&lt;/a&gt;. The big difficulty of their method is to find the optimal alpha.  That&amp;rsquo;s what make it work or break. The tricky part is that the function  of alpha we want to minimize has multiple discontinuities (it&amp;rsquo;s  periodic in some ways). This is why the authors rely on the computation  of an alpha_max: bracketing is very important, otherwise your optimizer  will jump the discontinuity without even noticing it, while you really  want to stay in the region before the first discontinuity.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;Marsaglia in &lt;!-- raw HTML omitted --&gt;his paper on Normal Distribution&lt;!-- raw HTML omitted --&gt; made the same mistake I initially did while trying to verify &lt;!-- raw HTML omitted --&gt;the accuracy of the normal density&lt;!-- raw HTML omitted --&gt;.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.6 because of the truncation at machine epsilon precision. Using Python mpmath, one can see that:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; mpf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;rsquo;-16.6000000000000014210854715202004&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;This is the more accurate representation if one goes beyond double precision (here 30 digits). And the value of the cumulative normal distribution is:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.4845465199503256054808152068743e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It is different from:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(mpf(&amp;quot;-16.6&amp;quot;))&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.48454651995040810217553910503186e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;where in this case it is really evaluated around -16.6 (up to 30 digits precision). Marsaglia gives this second number as reference. But all the other algorithms will actually take as input the first input. It is more meaningful to compare results using the exact same input. Using human readable but computer truncated numbers is not the best.  The cumulative normal distribution will often be computed using some output of some calculation where one does not have an exact human readable input.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The standard code for Ooura and Schonfelder (as well as Marsaglia) algorithms for the cumulative normal distribution don&amp;rsquo;t use Cody&amp;rsquo;s trick to evaluate the exp(-x&lt;em&gt;x). This function appears in all those implementations because it is part of the dominant term in the usual expansions. Out of curiosity, I replaced this part with Cody trick. For Ooura I also made minor changes to make it work directly on the CND instead of going through the error function erfc indirection. Here are the results without the Cody trick (except for Cody):&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;and with it:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;All 3 algorithms are now of similiar accuracy (note the difference of scale compared to the previous graph), with Schonfelder being a bit worse, especially for x &amp;gt;= -20. If one uses only easily representable numbers (for example -37, -36,75, -36,5, &amp;hellip;) in double precision then, of course, Cody trick importance won&amp;rsquo;t be visible and here is how the 3 algorithms would fare with or without Cody trick:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Schonfelder looks now worse than it actually is compared to Cody and Ooura.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;To conclude, if someone claims that a cumulative normal distribution is up to double precision accuracy and it does not use any tricks to compute exp(-x&lt;/em&gt;x), then beware, it probably is quite a bit less than double precision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/&#34;&gt;previous post&lt;/a&gt;, I stated that some library (SPECFUN by W.D. Cody) computes \(e^{-\frac{x^2}{2}}\) the following way:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;del &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt; xsq) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; xsq);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;del &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;);&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;p&gt;where &lt;code class=&#34;code-inline language-C&#34;&gt;&lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(z)&lt;/code&gt; computes the floor of z.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Why 1.6?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.6 is equivalent to multiplying by 10 and dividing by 16 which is an exact operation). It also allows to have something very close to a rounding function: x=2.6 will make xsq=2.5, x=2.4 will make xsq=1.875, x=2.5 will make xsq=2.5. The maximum difference between x and xsq will be 0.625.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Double Precision Puzzle with the Gaussian</title>
      <link>https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/</link>
      <pubDate>Wed, 20 Mar 2013 17:50:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/</guid>
      <description>&lt;p&gt;Some library computes the Gaussian density function $$e^{-\frac{x^2}{2}}$$ the following way:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;del &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt; xsq) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; xsq);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;del &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;);&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;p&gt;where &lt;code class=&#34;code-inline language-C&#34;&gt;&lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(z)&lt;/code&gt; computes the floor of z.&lt;/p&gt;&#xA;&lt;p&gt;Basically, &lt;code class=&#34;code-inline language-C&#34;&gt;x&lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;x&lt;/code&gt; is rewritten as &lt;code class=&#34;code-inline language-C&#34;&gt;xsq&lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;xsq&lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt;del&lt;/code&gt;. I have seen that trick once before, but I just can&amp;rsquo;t figure out where and why (except that it is probably related to high accuracy issues).&lt;/p&gt;&#xA;&lt;p&gt;The answer is in the &lt;a href=&#34;https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/&#34;&gt;next post&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Seasoned Volatility Swap</title>
      <link>https://chasethedevil.github.io/post/a-seasoned-volatility-swap/</link>
      <pubDate>Thu, 14 Mar 2013 19:55:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-seasoned-volatility-swap/</guid>
      <description>This is very much what&#39;s in the Carr-Lee paper &#34;Robust Replication of Volatility Derivatives&#34;, but it wasn&#39;t so easy to obtain in practice:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The formulas as written in the paper are not usable as is: they can be simplified (not too difficult, but intimidating at first)&lt;/li&gt;&lt;li&gt;The numerical integration is not trivial: a simple Gauss-Laguerre is not precise enough (maybe if I had an implementation with more points), a Gauss-Kronrod is not either (maybe if we split it in different regions). Funnily a simple adaptive Simpson works ok (but my boundaries are very basic: 1e-5 to 1e5).&lt;/li&gt;&lt;/ul&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-UbYc6dfh8Yw/UUIc6V2Mg8I/AAAAAAAAGSI/25Rdvjzk-xk/s1600/Screenshot+from+2013-03-14+19:33:04.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;254&#34; src=&#34;http://4.bp.blogspot.com/-UbYc6dfh8Yw/UUIc6V2Mg8I/AAAAAAAAGSI/25Rdvjzk-xk/s320/Screenshot+from+2013-03-14+19:33:04.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>A Volatility Swap and a Straddle</title>
      <link>https://chasethedevil.github.io/post/a-volatility-swap-and-a-straddle/</link>
      <pubDate>Tue, 12 Mar 2013 21:36:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-volatility-swap-and-a-straddle/</guid>
      <description>&lt;p&gt;A &lt;a href=&#34;https://en.wikipedia.org/wiki/Volatility_swap&#34;&gt;Volatility swap&lt;/a&gt; is a forward contract on future realized volatility. The pricing of such a contract used to be particularly challenging, often either using an unprecise popular expansion in the variance, or a model specific way (like Heston or local volatility with Jumps). Carr and Lee have recently proposed a way to price those contracts in a model independent way in their paper &amp;ldquo;&lt;em&gt;robust replication of volatility derivatives&lt;/em&gt;&amp;rdquo;. Here is the difference between the value of a synthetic volatility swap payoff at maturity (a newly issued one, with no accumulated variance) and a straddle.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Local Volatility Delta &amp; Dynamic</title>
      <link>https://chasethedevil.github.io/post/local-volatility-delta--dynamic/</link>
      <pubDate>Thu, 29 Nov 2012 12:30:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/local-volatility-delta--dynamic/</guid>
      <description>This will be a very technical post, I am not sure that it will be very understandable by people not familiar with the implied volatility surface.&lt;br /&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Something one notices when computing an option price under local volatility using a PDE solver, is how different is the Delta from the standard Black-Scholes Delta, even though the price will be very close for a Vanilla option. In deed, the Finite difference grid will have a different local volatility at each point and the Delta will take into account a change in local volatility as well.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;But this finite-difference grid Delta is also different from a standard numerical Delta where one just move the initial spot up and down, and takes the difference of computed prices. The numerical Delta will eventually include a change in implied volatility, depending if the surface is sticky-strike (vol will stay constant) or sticky-delta (vol will change). So the numerical Delta produced with a sticky-strike surface will be the same as the standard Black-Scholes Delta. In reality, what happens is that the local volatility is different when the spot moves up, if we recompute it: it is not static. The finite difference solver computes Delta with a static local volatility. If we call twice the finite difference solver with a different initial spot, we will reproduce the correct Delta, that takes into account the dynamic of the implied volatility surface.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Here how different it can be if the delta is computed from the grid (static local volatility) or numerically (dynamic local volatility) on an exotic trade:&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-WW8ZE7urDGE/ULdHKNe_VXI/AAAAAAAAGKQ/JQ5Rd7wQYkk/s1600/static_localvol.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;235&#34; src=&#34;http://3.bp.blogspot.com/-WW8ZE7urDGE/ULdHKNe_VXI/AAAAAAAAGKQ/JQ5Rd7wQYkk/s320/static_localvol.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;br /&gt;This is often why people assume the local volatility model is wrong, not consistent. It is wrong if we consider the local volatility surface as static to compute hedges.&lt;/div&gt;</description>
    </item>
    <item>
      <title>GPU computing in Finance</title>
      <link>https://chasethedevil.github.io/post/gpu-computing-in-finance/</link>
      <pubDate>Mon, 15 Oct 2012 16:14:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/gpu-computing-in-finance/</guid>
      <description>&lt;p&gt;Very interesting presentation from Murex about their GPU computing. Some points were:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GPU demand for mostly exotics pricing and greeks&lt;/li&gt;&#xA;&lt;li&gt;Local vol main model for EQD exotics. Local vol calibrated via PDE approach.&lt;/li&gt;&#xA;&lt;li&gt;Markov functional model becoming main model for IRD.&lt;/li&gt;&#xA;&lt;li&gt;Use of local regression instead of Longstaff Schwartz (or worse CVA like sim of sim).&lt;/li&gt;&#xA;&lt;li&gt;philox RNG from DE Shaw. But the presenter does not seem to know RNGs  very well (recommended Brownian Bridge for Mersenne Twister!).&lt;/li&gt;&#xA;&lt;li&gt;An important advantage of GPU is latency. Grid computing only improves throughput but not latency. GPU improves both.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://nvidia.fullviewmedia.com/gtc2010/0923-a7-2032.html&#34;&gt;http://nvidia.fullviewmedia.com/gtc2010/0923-a7-2032.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Binary Voting</title>
      <link>https://chasethedevil.github.io/post/binary-voting/</link>
      <pubDate>Fri, 07 Sep 2012 17:21:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/binary-voting/</guid>
      <description>How many reports have you had to fill up with a number of stars to choose? How much useless time is spent on figuring the this number just because it is always very ambiguous?&lt;br /&gt;&lt;br /&gt;Some blogger wrote an interesting entry on &lt;a href=&#34;http://davidcelis.com/blog/2012/02/01/why-i-hate-five-star-ratings/&#34;&gt;Why I Hate Five Stars Reviews&lt;/a&gt;. Basically he advocates binary voting instead via like/dislike. Maybe a ternary system via like/dislike/don&#39;t care would be ok too.&lt;br /&gt;&lt;br /&gt;One coworker used to advocate the same for a similar reason: people reading those reports only pay attention to the extremes: the 5 stars or the 0 stars. So if you want to have a voice, you need to express it via 5 or 0, nothing in between.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Adaptive Quadrature for Pricing European Option with Heston</title>
      <link>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</link>
      <pubDate>Mon, 25 Jun 2012 12:50:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</guid>
      <description>The Quantlib code to evaluate the Heston integral for European options is quite nice. It proposes &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=4&amp;amp;ved=0CGoQFjAD&amp;amp;url=http%3A%2F%2Fwww.math.uni-wuppertal.de%2F%7Ekahl%2Fpublications%2FNotSoComplexLogarithmsInTheHestonModel.pdf&amp;amp;ei=MkDoT8HHOaO_0QXS_6SeCQ&amp;amp;usg=AFQjCNFbAMQBLoKRd0BR_-HC0CkP4zrMtg&#34;&gt;Kahl &amp;amp; Jaeckel&lt;/a&gt; method as well as Gatheral method for the complex logarithm. It also contains expansions where it matters so that the resulting code is very robust. One minor issue is that it does not integrate both parts at the same time, and also does not propose Attari method for the Heston integral that is supposed to be more stable.&lt;br /&gt;&lt;br /&gt;I was surprised to find out that out of the money, short expiry options seemed badly mispriced. In the end I discovered it was just that it required sometimes more than 3500 function evaluations to have an accuracy of 1e-6.&lt;br /&gt;&lt;br /&gt;As this sounds a bit crazy, I thought that Jaeckel log transform was the culprit. In reality, it turned out that it was &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFMQFjAA&amp;amp;url=http%3A%2F%2Fusers.wpi.edu%2F%7Ewalker%2FMA510%2FHANDOUTS%2Fw.gander%2Cw.gautschi%2CAdaptive_Quadrature%2CBIT_40%2C2000%2C84-101.pdf&amp;amp;ei=U0HoT_HWBeXP0QW23O3xBw&amp;amp;usg=AFQjCNH4KRWMprUvL8yBPKRxO_sVNyc2Pg&#34;&gt;Gauss Lobatto Gander &amp;amp; Gautschi implementation&lt;/a&gt;. I tried the simplest algorithm in &lt;a href=&#34;http://www.ii.uib.no/%7Eterje/Papers/bit2003.pdf&#34;&gt;Espelid improved algorithms&lt;/a&gt;: modsim, an adaptive extrapolated Simpson method, and it was 4x faster for the same accuracy. That plus the fact that it worked out of the box (translated to Java) on my problem was impressive.&lt;br /&gt;&lt;br /&gt;Jaeckel log transform (to change the interval from 0,+inf to 0,1) works well, and seems to offer a slight speedup (10% to 15%) for around ATM options, mid to long term for the same accuracy. Unfortunately, it can also slow down by up to 50% the convergence for more OTM options or shorter expiries. So I am not so sure about its interest vs just cutting off the integration at phi=200.</description>
    </item>
    <item>
      <title>Gnome Shell more stable than Unity on Ubuntu 12.04</title>
      <link>https://chasethedevil.github.io/post/gnome-shell-more-stable-than-unity-on-ubuntu-12.04/</link>
      <pubDate>Thu, 14 Jun 2012 12:01:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/gnome-shell-more-stable-than-unity-on-ubuntu-12.04/</guid>
      <description>Regularly, the unity dock made some applications inaccessible: clicking on the app icon did not show or start the app anymore, a very annoying bug. This is quite incredible given that this version of Ubuntu is supposed to be long term support.  So I decided to give one more chance to Gnome Shell. Installing it on Ubuntu 12.04 is simple with &lt;a href=&#34;http://www.filiwiese.com/installing-gnome-on-ubuntu-12-04-precise-pangolin/&#34;&gt;this guide&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;To my surprise it is very stable so far. Earlier Gnome Shell versions were not as stable. After installing various extensions (dock especially) it is as usable as Unity for my needs. It seems more responsive as well. I am not really into the Unity new features like HUD. It sounds to me like Ubuntu is making a mistake with Unity compared to Gnome Shell.&lt;br /&gt;&lt;br /&gt;To make an old extension support latest Gnome Shell version, it is sometimes necessary&amp;nbsp; to update&amp;nbsp; the extension metadata with what&#39;s given by &lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;gnome-shell --version&lt;/span&gt;. For the weather extension you can just edit using gedit:&lt;br /&gt;&lt;br /&gt;&lt;blockquote class=&#34;tr_bq&#34;&gt;sudo gedit /usr/share/gnome-shell/extensions/weather@gnome-shell-extensions.gnome.org/metadata.json &lt;/blockquote&gt;</description>
    </item>
    <item>
      <title>Why primitive arrays matter in Java</title>
      <link>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</link>
      <pubDate>Wed, 29 Feb 2012 10:01:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</guid>
      <description>&lt;p&gt;In the past, I have seen that one could greatly improve performance of some Monte-Carlo simulation by using as much as possible &lt;code&gt;double[][]&lt;/code&gt; instead of arrays of objects.&lt;/p&gt;&#xA;&lt;p&gt;It was interesting to read &lt;a href=&#34;http://flavor8.com/index.php/2012/02/25/java-performance-autoboxing-and-data-structure-choice-obviously-matter-but-by-how-much/&#34;&gt;this blog post explaining why that happens&lt;/a&gt;: it is all about memory access.&lt;/p&gt;</description>
    </item>
    <item>
      <title>KDE 4.8 finally has a dock</title>
      <link>https://chasethedevil.github.io/post/kde-4.8-finally-has-a-dock/</link>
      <pubDate>Fri, 27 Jan 2012 13:38:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/kde-4.8-finally-has-a-dock/</guid>
      <description>KDE 4.8 finally has a dock: you just have to add the plasma icon tasks. Also the flexibility around ALT+TAB is welcome. With Krusader as file manager, Thunderbird and Firefox for email and web, it is becoming a real nice desktop, but it took a while since the very bad KDE 4.0 release.&lt;br /&gt;&lt;br /&gt;It is easy to install under ubuntu 11.10 through the backports and seems very stable so far.&lt;br /&gt;&lt;br /&gt;Something quite important is to tweak the fonts: use Déjà Vu Sans instead of Ubuntu fonts, use RGB subpixel rendering, use Crisp desktop effects. With those settings, KDE looks very nice. It&#39;s sad that they are not default in Kubuntu.&lt;br /&gt;&lt;br /&gt;&lt;b&gt;Update March 2013&lt;/b&gt;:&lt;i&gt; It&#39;s been a while now that it is in the standard Ubuntu repositories and I believe installed by default, one has just to remove the task manager widget add the icon task widget:&lt;/i&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-_LWYVO4CCKQ/UUry_yhXmQI/AAAAAAAAGSw/JPGByZuWsdw/s1600/snapshot3.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;60&#34; src=&#34;http://1.bp.blogspot.com/-_LWYVO4CCKQ/UUry_yhXmQI/AAAAAAAAGSw/JPGByZuWsdw/s400/snapshot3.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;i&gt;One can also change the settings using a right click (I find useful not to highlight the windows) and it can look like:&lt;/i&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-VqpLb8rULAQ/UUrz8cPYleI/AAAAAAAAGTA/fs3b8ytQID8/s1600/snapshot4.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;60&#34; src=&#34;http://3.bp.blogspot.com/-VqpLb8rULAQ/UUrz8cPYleI/AAAAAAAAGTA/fs3b8ytQID8/s400/snapshot4.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Generating random numbers following a given discrete probability distribution</title>
      <link>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</link>
      <pubDate>Mon, 09 Jan 2012 00:14:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</guid>
      <description>&lt;p&gt;I have never really thought very much about generating random numbers according to a precise discrete distribution, for example to simulate an unfair dice.&lt;/p&gt;&#xA;&lt;p&gt;In finance, we are generally interested in continuous distributions, where there is typically 2 ways:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the &lt;a href=&#34;http://en.wikipedia.org/wiki/Inverse_transform_sampling&#34;&gt;inverse transform&lt;/a&gt; (usually computed in a numerical way),&lt;/li&gt;&#xA;&lt;li&gt;and the &lt;a href=&#34;http://en.wikipedia.org/wiki/Rejection_sampling&#34;&gt;acceptance-rejection&lt;/a&gt; method, typically the &lt;a href=&#34;http://en.wikipedia.org/wiki/Ziggurat_algorithm&#34;&gt;ziggurat&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The inverse transform is often preferred, because it&amp;rsquo;s usable method for Quasi Monte-Carlo simulations while the acceptance rejection is not.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I would have thought about the simple way to generate random numbers according to a discrete distribution as first described &lt;a href=&#34;http://www.delicious.com/redirect?url=http%3A//blog.sigfpe.com/2012/01/lossless-decompression-and-generation.html&#34;&gt;here&lt;/a&gt;. But establishing a link with &lt;a href=&#34;http://en.wikipedia.org/wiki/Huffman_coding&#34;&gt;Huffman encoding&lt;/a&gt; is brilliant. Some better performing alternative (unrelated to Huffman) is offered &lt;a href=&#34;http://www.keithschwarz.com/darts-dice-coins/&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quant Interview &amp; Education</title>
      <link>https://chasethedevil.github.io/post/quant-interview--education/</link>
      <pubDate>Wed, 21 Dec 2011 17:37:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quant-interview--education/</guid>
      <description>Recently, I interviewed someone for a quant position. I was very surprised to find out that someone who did one of the best master in probabilities and finance in France could not solve a very basic probability problem:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-64ejCSg9Y8o/TvIHEF4PgwI/AAAAAAAAFio/uxVwdwVnQ54/s1600/Screenshot+at+2011-12-21+17%253A18%253A28.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;43&#34; src=&#34;http://2.bp.blogspot.com/-64ejCSg9Y8o/TvIHEF4PgwI/AAAAAAAAFio/uxVwdwVnQ54/s400/Screenshot+at+2011-12-21+17%253A18%253A28.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;This is accessible to someone with very little knowledge of probabilities &lt;br /&gt;&lt;br /&gt;When I asked this problem around to co-workers (who have all at least a master in a scientific subject), very few could actually answer it properly. Most of the time, I suspect it is because they did not dedicate enough time to do it properly, and wanted to answer it too quickly.&lt;br /&gt;&lt;br /&gt;It was more shocking that someone just out of school, with a major in probabilities could not answer that properly. It raises the question: what is all this education worth?&lt;br /&gt;&lt;br /&gt;The results were not better as soon as the question was not exactly like what students in those masters are used to, like for example, this simple stochastic calculus question:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-K9mO8qrp1Ow/TvIKO_MTOSI/AAAAAAAAFiw/igPGRdnFNEo/s1600/Screenshot+at+2011-12-21+17%253A19%253A11.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-K9mO8qrp1Ow/TvIKO_MTOSI/AAAAAAAAFiw/igPGRdnFNEo/s1600/Screenshot+at+2011-12-21+17%253A19%253A11.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;My opinion is that, today in our society, people study for too long. The ideal system for me would be one where people learn a lot in math/physics the first 2 years of university, and then have more freedom in their education, much like a doctorate.&lt;br /&gt;&lt;br /&gt;We still offered the job to this person, because live problem solving is not the most important criteria. Other qualities like seriousness and motivation are much more valuable.</description>
    </item>
    <item>
      <title>Gnome 3 not so crap after all</title>
      <link>https://chasethedevil.github.io/post/gnome-3-not-so-crap-after-all/</link>
      <pubDate>Wed, 30 Nov 2011 18:11:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/gnome-3-not-so-crap-after-all/</guid>
      <description>&lt;p&gt;In a previous post, I was complaining how &lt;a href=&#34;https://chasethedevil.github.io/posts/gnome-3-unity-crap&#34;&gt;bad Gnome 3&lt;/a&gt; was. Since I have installed a real dock: docky, it is now much more usable. I can easily switch / launch applications without an annoying full screen change.&lt;/p&gt;&#xA;&lt;p&gt;In addition I found out that it had a good desktop search (tracker). The ALT+F2 also does some sort of completion, too bad it can not use tracker here as well.&lt;/p&gt;&#xA;&lt;p&gt;So it looks like Gnome 3 + gnome-tweak-tool + docky makes a reasonably good desktop. XFCE does not really fit the bill for me: bad handling of sound, bad default applications, not so good integration with gnome application notifications.&#xA; &#xA;&#xA;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-2DF1jycI5ZU/TtZmvdt2k-I/AAAAAAAAFic/ej8a7g1Gswg/s1600/Screenshot+at+2011-11-30+18%253A20%253A29.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://4.bp.blogspot.com/-2DF1jycI5ZU/TtZmvdt2k-I/AAAAAAAAFic/ej8a7g1Gswg/s320/Screenshot+at+2011-11-30+18%253A20%253A29.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&#xA;&#xA;&#xA;Now if only I found a way to change this ugly big white scrollbar&amp;hellip;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Good &amp; Popular Algorithms are Simple</title>
      <link>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</link>
      <pubDate>Thu, 17 Nov 2011 12:28:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</guid>
      <description>I recently tried to minimise a function according to some constraints. One popular method to minimise a function in several dimensions is &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CB0QFjAA&amp;amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNelder%25E2%2580%2593Mead_method&amp;amp;ei=PfHETsKYDdG9sAb_q935Cw&amp;amp;usg=AFQjCNEVD7lMV4buMbVCJ3fuiyupPA6B1w&#34;&gt;Nelder-Mead Simplex&lt;/a&gt;. It is quite simple, so simple that I programmed it in Java in 1h30, including a small design and a test. It helped that the original paper from Nelder-Mead is very clear:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-glZ52IB0SkA/TsTxF4VmyFI/AAAAAAAAFhw/zSDQ0Het9MU/s1600/neldermeadalgo.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;184&#34; src=&#34;http://3.bp.blogspot.com/-glZ52IB0SkA/TsTxF4VmyFI/AAAAAAAAFhw/zSDQ0Het9MU/s320/neldermeadalgo.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;However the main issue is that it works only for unconstrained problems. Nelder and Mead suggested to add a penalty, but in practice this does not work so well. For constrained problems, there is an adaptation of the original idea by Box (incredible name for a constrained method) that he called &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CB4QFjAA&amp;amp;url=http%3A%2F%2Fcomjnl.oxfordjournals.org%2Fcontent%2F8%2F1%2F42.full.pdf&amp;amp;ei=EvLETqz6LcTvsgbN4I2ADA&amp;amp;usg=AFQjCNHq6JD1ik4rTLO4a1v4G9adbGMRzQ&#34;&gt;the Complex method&lt;/a&gt;, a deliberate pun to the Nelder-Mead Simplex method. The basic idea is to reset the trial point near the fixed boundary if it goes outside. Now this took me much longer to program, as the paper is not as clear, even if the method is still relatively simple. But worst, after a day of deciphering the paper and programming the complex method, I find out that it does not works so well: it does not manage to minimise a simple multidimensional quadratic with a simple bound constraint: f(X)=sum(X)^2 with X &amp;gt;= 0 where X is an N-dimensional vector. In the end, I don&#39;t want to work with such a simple function, but it is a good simple test to see if the method is really working or not. Here is how the function looks with N=2:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-ncMKzKcYdtA/TsT_TEyNHDI/AAAAAAAAFiA/er1pQWNcHXc/s1600/quadfull.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://2.bp.blogspot.com/-ncMKzKcYdtA/TsT_TEyNHDI/AAAAAAAAFiA/er1pQWNcHXc/s320/quadfull.png&#34; width=&#34;302&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And if we restrict to x,y &amp;gt;= 0 it becomes:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-msyZ9eQ7GnM/TsT_d36SD7I/AAAAAAAAFiI/Kt4QBwjsAE8/s1600/quadpart.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;304&#34; src=&#34;http://1.bp.blogspot.com/-msyZ9eQ7GnM/TsT_d36SD7I/AAAAAAAAFiI/Kt4QBwjsAE8/s320/quadpart.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;I suspected an error in my program, so I decided to try with scilab, that has also the Box method as part of their &lt;a href=&#34;http://help.scilab.org/docs/5.3.3/en_US/neldermead.html&#34;&gt;neldermead_search&lt;/a&gt; functionality. Scilab also failed to compute the minimum in 8 dimensions of my simple quadratic. &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-vO6JhSwv3MU/TsT1ge1H96I/AAAAAAAAFh4/DTnsYjnMeHE/s1600/neldermeadalgo1.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/-vO6JhSwv3MU/TsT1ge1H96I/AAAAAAAAFh4/DTnsYjnMeHE/s320/neldermeadalgo1.png&#34; width=&#34;265&#34; /&gt;&lt;/a&gt;&lt;/div&gt;I tried various settings, without ever obtaining a decent result (I expect to find a value near 0).&lt;br /&gt;&lt;br /&gt;There is another algorithm that can find a global minimum, also very popular: the &lt;a href=&#34;http://www.icsi.berkeley.edu/%7Estorn/TR-95-012.pdf&#34;&gt;Differential Evolution&lt;/a&gt;. At first, being a genetic algorithm, you would think it would be complicated to write. But no, the main loop is around 20 lines.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-cNeeYo8EQeQ/TsUB7Ek6rQI/AAAAAAAAFiQ/PTc4qwwAa7E/s1600/dealgo.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/-cNeeYo8EQeQ/TsUB7Ek6rQI/AAAAAAAAFiQ/PTc4qwwAa7E/s320/dealgo.png&#34; width=&#34;297&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Those 20 lines are a bit more difficult than Nelder-Mead, but still, when I saw that, I understood that &#34;this is a classic algorithm&#34;. And it does work with constraints easily. How to do this is explained well in K. Price book &#34;Differential Evolution&#34;, and takes only a few lines of code. Here is the result I got in dimension 29:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace; font-size: x-small;&#34;&gt;dim=29 min=1.2601854176573729E-12 genMax=412 x=[3.340096901536317E-8, 8.889252404343621E-8, 7.163904251807348E-10, 9.71847877381699E-9, 2.7423324674150668E-8, 2.4022269439114537E-9, 1.7336434478718816E-11, 7.244238163901778E-9, 1.0013136274729337E-8, 7.412154679865083E-9, 5.4694460144807974E-9, 2.3682413086417524E-9, 4.241739250073559E-7, 4.821920889534676E-10, 2.115396281722523E-9, 8.750883007882899E-8, 2.512011485133975E-9, 4.811507109129279E-9, 1.0752997894113096E-7, 5.120475258343548E-9, 8.404448964497456E-9, 4.1062290228305595E-9, 1.7030766521603753E-8, 5.589430643552073E-9, 8.237098544820173E-10, 3.5796523161196554E-9, 5.186299547466997E-9, 2.430326342762937E-7, 5.493850433494286E-9]&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;It works well, although there is quite a bit of parameters. I noticed that the strategy was especially important.</description>
    </item>
    <item>
      <title>exp(y*log(x)) Much Faster than Math.pow(x,y)</title>
      <link>https://chasethedevil.github.io/post/expylogx-much-faster-than-math.powxy/</link>
      <pubDate>Fri, 08 Apr 2011 23:03:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/expylogx-much-faster-than-math.powxy/</guid>
      <description>&lt;p&gt;Today I found out that replacing &lt;em&gt;Math.pow(x,y)&lt;/em&gt; by &lt;em&gt;Math.exp(y&lt;/em&gt;Math.log(x))* made me gain 50% performance in my program. Of course, both x and y are double in my case. I find this quite surprising, I expected better from Math.pow.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SIMD and Mersenne-Twister</title>
      <link>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</link>
      <pubDate>Sat, 05 Feb 2011 13:18:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</guid>
      <description>&lt;p&gt;Since 2007, there is a new kind of Mersenne-Twister (MT) that exploits SIMD architecture, the &lt;a href=&#34;http://www.math.sci.hiroshima-u.ac.jp/%7Em-mat/MT/SFMT/&#34;&gt;SFMT&lt;/a&gt;. The Mersenne-Twister has set quite a standard in random number generation for Monte-Carlo simulations, even though it has flaws.&lt;/p&gt;&#xA;&lt;p&gt;I was wondering if SFMT improved the performance over MT for a Java implementation. There is actually on the same page a decent Java port of the original algorithm. When I ran it, it ended up slower by more than 20% than the classical Mersenne-Twister (32-bit) on a 64-bit JDK 1.6.0.23 for Windows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>XORWOW L&#39;ecuyer TestU01 Results</title>
      <link>https://chasethedevil.github.io/post/xorwow-lecuyer-testu01-results/</link>
      <pubDate>Wed, 12 Jan 2011 20:26:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/xorwow-lecuyer-testu01-results/</guid>
      <description>&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;Nvidia uses XorWow random number generator in its CURAND library. It is a simple and fast random number generator with a reasonably long period. It can also be parallelized relatively easily. Nvidia suggests it passes L&#39;Ecuyer TestU01, but is not very explicit about it. So I&#39;ve decided to see how it performed on TestU01.&lt;/div&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;I found very simple to test a new random number generator on TestU01, the documentation is great and the examples helpful. Basically there is just a simple C file to create &amp;amp; compile &amp;amp; run.&lt;/div&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;XORWOW passes the SmallCrush battery, and according to Marsaglia, it also passes DieHard. But it fails on 1 test in Crush and 3 in BigCrush. By comparison, Mersenne-Twister fails on 2 tests in Crush and 2 in BigCrush. Here are the results of the failures:&lt;/div&gt;&lt;div style=&#34;font-family: Georgia,&amp;quot;Times New Roman&amp;quot;,serif;&#34;&gt;&lt;br /&gt;&lt;/div&gt;&lt;span style=&#34;font-family: inherit;&#34;&gt;&lt;br /&gt;&lt;/span&gt;&lt;/div&gt;&lt;div style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;========= Summary results of Crush =========&lt;br /&gt;&lt;br /&gt;&amp;nbsp;Version:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; TestU01 1.2.3&lt;br /&gt;&amp;nbsp;Generator:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Xorwow&lt;br /&gt;&amp;nbsp;Number of statistics:&amp;nbsp; 144&lt;br /&gt;&amp;nbsp;Total CPU time:&amp;nbsp;&amp;nbsp; 00:50:15.92&lt;br /&gt;&amp;nbsp;The following tests gave p-values outside [0.001, 0.9990]:&lt;br /&gt;&amp;nbsp;(eps&amp;nbsp; means a value &amp;lt; 1.0e-300):&lt;br /&gt;&amp;nbsp;(eps1 means a value &amp;lt; 1.0e-15):&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Test&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; p-value&lt;br /&gt;&amp;nbsp;----------------------------------------------&lt;br /&gt;&amp;nbsp;72&amp;nbsp; LinearComp, r = 29&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1 - eps1&lt;br /&gt;&amp;nbsp;----------------------------------------------&lt;br /&gt;&amp;nbsp;All other tests were passed&lt;/div&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;========= Summary results of BigCrush =========&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;Version:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; TestU01 1.2.3&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;Generator:&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Xorwow&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;Number of statistics:&amp;nbsp; 160&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;Total CPU time:&amp;nbsp;&amp;nbsp; 06:12:38.79&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;The following tests gave p-values outside [0.001, 0.9990]:&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;(eps&amp;nbsp; means a value &amp;lt; 1.0e-300):&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;(eps1 means a value &amp;lt; 1.0e-15):&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; Test&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; p-value&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;----------------------------------------------&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp; 7&amp;nbsp; CollisionOver, t = 7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1.6e-6&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;27&amp;nbsp; SimpPoker, r = 27&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; eps&amp;nbsp; &lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;81&amp;nbsp; LinearComp, r = 29&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 1 - eps1&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;----------------------------------------------&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;&amp;nbsp;All other tests were passed&lt;/span&gt;</description>
    </item>
    <item>
      <title>The CUDA Performance Myth</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth/</link>
      <pubDate>Mon, 03 Jan 2011 16:07:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth/</guid>
      <description>&lt;p&gt;There is an &lt;a href=&#34;http://arxiv.org/PS_cache/arxiv/pdf/0901/0901.0638v4.pdf&#34;&gt;interesting&lt;/a&gt; article on how to generate efficiently the inverse of the normal cumulative distribution on the GPU. This is useful for Monte-Carlo simulations based on normally distributed variables.&lt;/p&gt;&#xA;&lt;p&gt;Another result of the paper is a method (breakless algorithm) to compute it apparently faster than the very good &lt;a href=&#34;http://www.mth.kcl.ac.uk/~shaww/web_page/papers/Wichura.pdf&#34;&gt;Wichura&amp;rsquo;s AS241&lt;/a&gt; algorithm on the CPU as well keeping a similar precision. The key is to avoid branches (if-then) at the cost of not avoiding log() calls. As the algorithm is very simple, I decided to give it a try in Java.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Another Look at Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</link>
      <pubDate>Mon, 29 Nov 2010 12:45:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</guid>
      <description>&lt;p&gt;A while ago, &lt;a href=&#34;https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries&#34;&gt;I was already looking&lt;/a&gt; for a good Java Matrix library, complaining that there does not seem any real good one where development is still active: the 2 best ones are in my opinion &lt;a href=&#34;http://math.nist.gov/javanumerics/jama/&#34;&gt;Jama&lt;/a&gt; and &lt;a href=&#34;http://dsd.lbl.gov/~hoschek/colt/&#34;&gt;Colt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Recently I tried to &lt;a href=&#34;http://www.wilmott.com/detail.cfm?articleID=345&#34;&gt;price options via RBF&lt;/a&gt; (radial basis functions) based on &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1648878&#34;&gt;TR-BDF2&lt;/a&gt; time stepping.&#xA;This is a problem where one needs to do a few matrix multiplications and inverses (or better, LU solve) in a loop. The size of the matrix is typically 50x50 to 100x100, and one can loop between 10 and 1000 times.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Out of curiosity I decided to give &lt;a href=&#34;http://ojalgo.org/&#34;&gt;ojalgo&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/p/matrix-toolkits-java&#34;&gt;MTJ&lt;/a&gt; a chance. I had read benchmarks (&lt;a href=&#34;http://blog.mikiobraun.de/2009/04/some-benchmark-numbers-for-jblas.html&#34;&gt;here about jblas&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/p/java-matrix-benchmark/wiki/Runtime_2xXeon_2010_08&#34;&gt;here the java matrix benchmark&lt;/a&gt;) where those libraries performed really well.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;On my core i5 laptop under the latest 64bit JVM (Windows 7), I found out that for the 100x100 case, &lt;em&gt;Jama was actually 30% faster than MTJ&lt;/em&gt;, and ojalgo was more than 50% slower. I also found out that I did not like ojalgo API at all. I was quite disappointed by those results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Street Fighting Mathematics Book</title>
      <link>https://chasethedevil.github.io/post/street-fighting-mathematics-book/</link>
      <pubDate>Wed, 28 Jul 2010 14:25:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/street-fighting-mathematics-book/</guid>
      <description>&lt;p&gt;The MIT has a downloadable book on basic mathematics: &lt;a href=&#34;http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;amp;tid=12156&#34;&gt;Street Fighting Mathematics&lt;/a&gt;. I liked the part focused on the geometrical approach. It reminded me of the early greek mathematics.&lt;/p&gt;&#xA;&lt;p&gt;Overall it does look like a very American approach to Maths: answering a multiple choices questions test by elimination. But it is still an interesting book.&lt;/p&gt;</description>
    </item>
    <item>
      <title>double[][] Is Fine</title>
      <link>https://chasethedevil.github.io/post/double-is-fine/</link>
      <pubDate>Thu, 26 Nov 2009 14:51:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/double-is-fine/</guid>
      <description>In my previous post, I suggest that keeping a double[] performs better than keeping a double[][] if you do matrix multiplications and other operations.&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This is actually not true. I benchmarked 3 libraries, Colt (uses double[]), Apache Commons Math (uses double[][]) and Jama (uses double[][] cleverly). At first it looks like Jama has a similar performance as Colt (they avoid [][] slow access by a clever algorithm). But once hotspot hits, the difference is crazy and Jama becomes the fastest (Far ahead).&lt;/div&gt;&lt;br /&gt;&lt;table border=&#34;1&#34;&gt;&lt;tr&gt;&lt;td colspan=&#34;4&#34;  valign=&#34;bottom&#34;  align=&#34;center&#34;  style=&#34; font-size:10pt;&#34;&gt;&lt;b&gt;JDK 1.6.0 Linux 1000x1000 matrix multiplication on Intel Q6600&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;loop index&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;Colt&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;Commons Math&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;Jama&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;1&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;11.880748&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;24.455125&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.828977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;2&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;11.874975&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;24.265102&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.848916&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;3&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.772616&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.374153&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.826572&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;4&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.759679&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.368105&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.655915&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;5&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.799622&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.238928&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.649129&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;6&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.780556&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.741863&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.668104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;7&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.72831&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.509909&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646811&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;8&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.79838&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.724348&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646069&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.726143&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.988762&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646052&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;10&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.784505&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.121782&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.644572&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  style=&#34;&#34;&gt;&lt;/td&gt;&lt;td colspan=&#34;10&#34;  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt;&#34;&gt;We don&#39;t include matrix construction time, and fetching the result. Only the multiplication is taken into account.&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br /&gt;The difference is less pronounced on smaller matrices, but still there. Jama looks very good in this simple test case. In more real scenarios, the difference is not so obvious. For example Commons Math SVD is faster than Jama one.</description>
    </item>
    <item>
      <title>The Pain of Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries/</link>
      <pubDate>Thu, 26 Nov 2009 09:17:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries/</guid>
      <description>&lt;p&gt;Looking for a good Java Matrix (and actually also math) library, I was a bit surprised to find out there does not seem to be any really serious one still maintained.&lt;/p&gt;&#xA;&lt;p&gt;Sure, there is &lt;a href=&#34;%22http://commons.apache.org/math/&#34;&gt;Apache Commons Math&lt;/a&gt;, but it is still changing a lot, and it is not very performance optimized yet, while it has been active for several years already. There is also Java3D, it does Matrix through GMatrix, but not much linear algebra and if you look at their implementation, it is very basic, not performance oriented.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
