<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math on Chase the Devil</title>
    <link>http://chasethedevil.github.io/tags/math/</link>
    <description>Recent content in Math on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Fri, 24 May 2013 14:17:00 +0000</lastBuildDate>
    <atom:link href="/tags/math/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>SABR with Andreasen-Huge</title>
      <link>http://chasethedevil.github.io/post/sabr-with-andreasen-huge/</link>
      <pubDate>Fri, 24 May 2013 14:17:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/sabr-with-andreasen-huge/</guid>
      <description>I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.
Around 2 years ago, I saw a presentation from Andreasen and Huge about how they were able to price/calibrate SABR by a one-step finite difference technique. <a href="/post/sabr-with-andreasen-huge/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Exact Forward in Monte-Carlo</title>
      <link>http://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</link>
      <pubDate>Mon, 13 May 2013 17:58:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</guid>
      <description>Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.
In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &amp;ldquo;The Equity Option Volatility Smile: a Finite Difference Approach&amp;rdquo; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices. <a href="/post/exact-forward-in-monte-carlo/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</title>
      <link>http://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</link>
      <pubDate>Mon, 22 Apr 2013 18:00:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</guid>
      <description>In the book Monte Carlo Methods in Financial Engineering, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.
However Longstaff and Schwartz seem to just reuse the paths in their paper, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value. <a href="/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.
Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an intrinsic function call and that should be difficult to beat. <a href="/post/a-fast-exponential-function-in-java/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>In my previous post, I explored the Lord-Kahl method to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise. <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price</title>
      <link>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</link>
      <pubDate>Tue, 09 Apr 2013 19:49:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</guid>
      <description>I just tried to implement Lord Kahl algorithm to compute the Heston call price. The big difficulty of their method is to find the optimal alpha. That&amp;rsquo;s what make it work or break. The tricky part is that the function of alpha we want to minimize has multiple discontinuities (it&amp;rsquo;s periodic in some ways). This is why the authors rely on the computation of an alpha_max: bracketing is very important, otherwise your optimizer will jump the discontinuity without even noticing it, while you really want to stay in the region before the first discontinuity. <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>http://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>Marsaglia in his paper on Normal Distribution made the same mistake I initially did while trying to verify the accuracy of the normal density.
In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16. <a href="/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>http://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>In my previous post, I stated that some library (SPECFUN by W.D. Cody) computes $$e^{-\frac{x^2}{2}}$$ the following way:
xsq = fint(x * 1.6) / 1.6;
del = (x - xsq) * (x + xsq);
result = exp(-xsq * xsq * 0.5) * exp(-del &amp;nbsp;0.5);
where fint(z) computes the floor of z.
1. Why 1.6?
An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1. <a href="/post/cracking-the-double-precision-gaussian-puzzle/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>GPU computing in Finance</title>
      <link>http://chasethedevil.github.io/post/gpu-computing-in-finance/</link>
      <pubDate>Mon, 15 Oct 2012 16:14:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/gpu-computing-in-finance/</guid>
      <description>Very interesting presentation from Murex about their GPU computing. Some points were:
- GPU demand for mostly exotics pricing &amp;amp; greeks
- Local vol main model for EQD exotics. Local vol calibrated via PDE approach.
- Markov functional model becoming main model for IRD.
- Use of local regression instead of Longstaff Schwartz (or worse CVA like sim of sim).
- philox RNG from DE Shaw. But the presenter does not seem to know RNGs very well (recommended Brownian Bridge for Mersenne Twister! <a href="/post/gpu-computing-in-finance/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
    <item>
      <title>Binary Voting</title>
      <link>http://chasethedevil.github.io/post/binary-voting/</link>
      <pubDate>Fri, 07 Sep 2012 17:21:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/binary-voting/</guid>
      <description>How many reports have you had to fill up with a number of stars to choose? How much useless time is spent on figuring the this number just because it is always very ambiguous?
Some blogger wrote an interesting entry on Why I Hate Five Stars Reviews. Basically he advocates binary voting instead via like/dislike. Maybe a ternary system via like/dislike/don&amp;rsquo;t care would be ok too.
One coworker used to advocate the same for a similar reason: people reading those reports only pay attention to the extremes: the 5 stars or the 0 stars. <a href="/post/binary-voting/">Read More…</a> <p>Copyright 2006-2016 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</p></description>
    </item>
    
  </channel>
</rss>
