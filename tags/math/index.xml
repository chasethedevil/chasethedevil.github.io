<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Math on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/math/</link>
    <description>Recent content in Math on Chase the Devil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Thu, 25 Jul 2024 12:56:42 +0100</lastBuildDate>
    <atom:link href="https://chasethedevil.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Copilot vs ChatGPT on the Optimal Finite Difference Step-Size</title>
      <link>https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/</link>
      <pubDate>Thu, 25 Jul 2024 12:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/</guid>
      <description>&lt;p&gt;When computing the derivative of a function by finite difference, which step size is optimal? The answer depends on the kind of difference (forward, backward or central), and the degree of the derivative (first or second typically for finance).&lt;/p&gt;&#xA;&lt;p&gt;For the first derivative, the result is very quick to find (it&amp;rsquo;s on &lt;a href=&#34;https://en.wikipedia.org/wiki/Numerical_differentiation&#34;&gt;wikipedia&lt;/a&gt;). For the second derivative, it&amp;rsquo;s more challenging. The &lt;a href=&#34;https://paulklein.ca/newsite/teaching/Notes_NumericalDifferentiation.pdf&#34;&gt;Lecture Notes&lt;/a&gt; of Karen Kopecky provide an answer. I wonder if Copilot or ChatGPT would find a good solution to the question:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Volatility: Rough or Not? A Short Review</title>
      <link>https://chasethedevil.github.io/post/rough-volatility-or-not-a-review/</link>
      <pubDate>Tue, 10 May 2022 17:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/rough-volatility-or-not-a-review/</guid>
      <description>&lt;p&gt;It is well-known that the assumption of constant volatility in the Black-Scholes model for pricing financial contracts is wrong and may lead&#xA;to serious mispricing, especially for any exotic derivative contracts.&#xA;A classic approach is to use a deterministic local volatility model to take into account the variation both in the time dimension and in the underlying asset price dimension. But&#xA;the model is still not realistic in terms of forward smile (the implied volatilities of forward starting options). A stochastic volatility component must be added to correct for it.&#xA;More recently, the concept of rough volatility emerged in many academic papers. Instead of using a classic Brownian motion for the stochastic volatility process,&#xA;a fractional Brownian motion is used. The idea of using a fractional Brownian motion for financial time-series can be traced back to Mandelbrot, but it is only relatively recently that it has been proposed for the volatility process (and not the stock process).&lt;/p&gt;</description>
    </item>
    <item>
      <title>More Automatic Differentiation Awkwardness</title>
      <link>https://chasethedevil.github.io/post/more-automatic-differentiation-awkwardness/</link>
      <pubDate>Tue, 04 Jan 2022 21:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/more-automatic-differentiation-awkwardness/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://quantsrus.github.io/post/exp_b_spline_collocation_autodiff/&#34;&gt;This blog post&lt;/a&gt; from Jherek Healy presents some not so obvious behavior of automatic differentiation, when a function is decomposed&#xA;into the product of two parts where one part goes to infinity and the other to zero, and we know the overall result must go to zero (or to some other specific number).&#xA;This decomposition may be relatively simple to handle for the value of the function, but becomes far less trivial to think of in advance, at the derivative level.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quadprog in Julia</title>
      <link>https://chasethedevil.github.io/post/quadprog-in-julia/</link>
      <pubDate>Sun, 21 Nov 2021 13:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/quadprog-in-julia/</guid>
      <description>&lt;p&gt;As described &lt;a href=&#34;https://en.wikipedia.org/wiki/Quadratic_programming&#34;&gt;on wikipedia&lt;/a&gt;, a quadratic programming problem with &lt;em&gt;n&lt;/em&gt; variables and &lt;em&gt;m&lt;/em&gt; constraints is of the  form&#xA;$$ \min(-d^T x + 1/2 x^T D x) $$ with the&#xA;constraints \( A^T x \geq b_0 \), were \(D\) is a \(n \times n\)-dimensional real symmetric matrix, \(A\) is a \(n \times m\)-dimensional real matrix, \( b_0 \) is a \(m\)-dimensional vector of constraints, \( d \) is a \(n\)-dimensional vector, and the variable \(x\) is a \(n\)-dimensional vector.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Bad papers and the roots of high degree polynomials</title>
      <link>https://chasethedevil.github.io/post/bad-papers-polynomial-roots/</link>
      <pubDate>Sat, 07 Nov 2020 07:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/bad-papers-polynomial-roots/</guid>
      <description>&lt;p&gt;I was wondering what were exactly the eigenvalues of the Mersenne-Twister random number generator transition matrix. An &lt;a href=&#34;https://arxiv.org/abs/1403.5355&#34;&gt;article&lt;/a&gt; by K. Savvidy sparked my interest on this.&#xA;This article mentioned a poor entropy (sum of log of eigenvalues amplitudes which are greater than 1), with eigenvalues falling almost on the unit circle.&lt;/p&gt;&#xA;&lt;p&gt;The eigenvalues are also the roots of the characteristic polynomial. It turns out, that for jumping ahead in the random number sequence, we use the characteristic polynomial. There is a twist however,&#xA;we use it in F2 (modulo 2), for entropy, we are interested in the characteristic polynomial in Z (no modulo), specified in Appendix A of &lt;a href=&#34;https://dl.acm.org/doi/10.1145/272991.272995&#34;&gt;the Mersenne-Twister paper&lt;/a&gt;. The roots of the two polynomials are of course very different.&lt;/p&gt;</description>
    </item>
    <item>
      <title>42</title>
      <link>https://chasethedevil.github.io/post/42/</link>
      <pubDate>Thu, 05 Dec 2019 20:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/42/</guid>
      <description>&lt;p&gt;Today my 6-years old son came with a math homework. The stated goal was to learn the different ways to make 10 out of smaller numbers. I was impressed. Immediately, I wondered&lt;/p&gt;&#xA;&lt;blockquote&gt;&#xA;&lt;p&gt;how many ways are there to make 10 out of smaller numbers?&lt;/p&gt;&lt;/blockquote&gt;&#xA;&lt;p&gt;This is one of the beauties of maths: a very simple problem, which a 6-years old can understand, may actually be quite fundamental. If you want to solve this in the general case, for any number instead of 10, you end up with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Partition_(number_theory)#Restricted_part_size_or_number_of_parts&#34;&gt;partition function&lt;/a&gt;. And in order to find this, you will probably learn recurrence relations. So what is the answer for 10?&lt;/p&gt;</description>
    </item>
    <item>
      <title>Discrete Sine Transform via the FFT</title>
      <link>https://chasethedevil.github.io/post/discrete_sine_transform_fft/</link>
      <pubDate>Mon, 05 Feb 2018 13:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/discrete_sine_transform_fft/</guid>
      <description>&lt;p&gt;Several months ago, I had a quick look at &lt;a href=&#34;https://papers.ssrn.com/sol3/Papers.cfm?abstract_id=2585529&#34;&gt;a recent paper&lt;/a&gt; describing how to use&#xA;Wavelets to price options under stochastic volatility models with a known characteristic function.&#xA;The more classic method is to use some numerical quadrature directly on the Fourier integral as described &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2362968&#34;&gt;in this paper&lt;/a&gt; for example.&#xA;When I read the paper, I was skeptical about the Wavelet approach, since it looked complicated, and with many additional parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Typo in Hyman non-negative constraint - 28 years later</title>
      <link>https://chasethedevil.github.io/post/typo-in-hyman-non-negative-constraint/</link>
      <pubDate>Tue, 23 May 2017 23:56:42 +0100</pubDate>
      <guid>https://chasethedevil.github.io/post/typo-in-hyman-non-negative-constraint/</guid>
      <description>&lt;p&gt;In their paper &lt;a href=&#34;http://www.ams.org/journals/mcom/1989-52-186/S0025-5718-1989-0962209-1/S0025-5718-1989-0962209-1.pdf&#34;&gt;&amp;ldquo;Nonnegativity-, Monotonicity-, or Convexity-Preserving Cubic and Quintic Hermite Interpolation&amp;rdquo;&lt;/a&gt;, Dougherty, Edelman and Hyman present a simple filter on the first derivatives to maintain positivity of a cubic spline interpolant.&lt;/p&gt;&#xA;&lt;p&gt;Unfortunately, in their main formula for non-negativity, they made a typo: the equation (3.3) is not consistent with the equation (3.1): the  \( \Delta x_{i-1/2} \)  is interverted with  \( \Delta x_{i+1/2} \).&lt;/p&gt;&#xA;&lt;p&gt;It was not obvious to find out which equation was wrong since there is no proof in the paper. Fortunately, the proof is in the reference paper &lt;a href=&#34;http://epubs.siam.org/doi/abs/10.1137/0722023&#34;&gt;&amp;ldquo;Monotone piecewise bicubic interpolation&amp;rdquo;&lt;/a&gt; from Carlson and Fritsch and it is clear then that equation (3.1) is the correct one.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SABR with Andreasen-Huge</title>
      <link>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</link>
      <pubDate>Fri, 24 May 2013 14:17:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/sabr-with-andreasen-huge/</guid>
      <description>I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.&lt;br /&gt;&lt;br /&gt;Around 2 years ago, I saw a presentation from &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDAQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;amp;ei=-VOfUYncL8HB7AbK0YHgBg&amp;amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;amp;sig2=SChIkU-TBR7ECaLdDm1orA&amp;amp;bvm=bv.47008514,d.ZGU&#34;&gt;Andreasen and Huge about how they were able to price/calibrate SABR&lt;/a&gt; by a one-step finite difference technique. At that time, I did not understand much their idea. My mind was too focused on more classical finite differences techniques and not enough on the big picture in their idea. Their idea is quite general and can be applied to much more than SABR. &lt;br /&gt;&lt;br /&gt;Recently there has been some talk and development going on where I work about SABR (a popular way to interpolate the option implied volatility surface for interest rate derivatives), especially regarding the implied volatility wings at low strike, and sometimes on how to price in a negative rates environment. There are actually quite a bit of research papers around this. I am not really working on that part so I just mostly listened. Then a former coworker suggested that the Andreasen Huge method was actually what banks seemed to choose in practice. A few weeks later, the Thalesians (a group for people interested in quantitative finance) announced a presentation by Hagan (one of the inventor of SABR) about a technique that sounded very much like Andreasen-Huge&amp;nbsp; to deal with the initial SABR issues in low rates.&lt;br /&gt;&lt;br /&gt;As the people working on this did not investigate Andreasen-Huge technique, I somehow felt that I had to and that maybe, this time, I would be able to grasp their idea.&lt;br /&gt;&lt;br /&gt;It took me just a few hours to have meaningful results. Here is the price of out of the money vanilla options using alpha = 0.0758194, nu = 0.1, beta = 0.5, rho = -0.1, forward = 0.02, and a maturity of 2 years.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s1600/Screenshot+from+2013-05-24+13:29:24.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;212&#34; src=&#34;http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s400/Screenshot+from+2013-05-24+13:29:24.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s1600/Screenshot+from+2013-05-24+13:30:09.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;215&#34; src=&#34;http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s400/Screenshot+from+2013-05-24+13:30:09.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;I did not have in my home library a way to find the implied volatility for a given price. I knew of 2 existing methods, &lt;a href=&#34;http://www.pjaeckel.webspace.virginmedia.com/ByImplication.pdf&#34;&gt;Jaeckel &#34;By Implication&#34;&lt;/a&gt;, and &lt;a href=&#34;http://scholar.google.fr/citations?view_op=view_citation&amp;amp;hl=fr&amp;amp;user=3GRhH_IAAAAJ&amp;amp;citation_for_view=3GRhH_IAAAAJ:d1gkVwhDpl0C&#34;&gt;Li rational functions&lt;/a&gt; approach. I discovered that Li wrote &lt;a href=&#34;http://www.tandfonline.com/doi/abs/10.1080/14697680902849361&#34;&gt;a new paper&lt;/a&gt; on the subject where he uses a SOR method to find the implied volatility and claims it&#39;s very accurate, very fast and very robust. Furthermore, the same idea can be applied to normal implied volatility. What attracted me to it is the simplicity of the underlying algorithm. Jaeckel&#39;s way is a nice way to do Newton-Raphson, but there seems to be so many things to &#34;prepare&#34; to make it work in most cases, that I felt it would be too much work for my experiment. It took me a few more hours to code Li SOR solvers, but it worked amazingly well for my experiment.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s1600/Screenshot+from+2013-05-24+13%253A31%253A33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;215&#34; src=&#34;http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s400/Screenshot+from+2013-05-24+13%253A31%253A33.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s1600/Screenshot+from+2013-05-24+13%253A37%253A51.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;211&#34; src=&#34;http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s400/Screenshot+from+2013-05-24+13%253A37%253A51.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;At first I had an error in my boundary condition and had no so good results especially with a long maturity. The traps with Andreasen-Huge technique are very much the same as with classical finite differences: be careful to place the strike on the grid (eventually smooth it), and have good boundaries.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Exact Forward in Monte-Carlo</title>
      <link>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</link>
      <pubDate>Mon, 13 May 2013 17:58:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/</guid>
      <description>&lt;p&gt;Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.&lt;/p&gt;&#xA;&lt;p&gt;In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &amp;ldquo;The Equity Option Volatility Smile: a Finite Difference Approach&amp;rdquo; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</title>
      <link>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</link>
      <pubDate>Mon, 22 Apr 2013 18:00:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quasi-monte-carlo--longstaff-schwartz-american-option-price/</guid>
      <description>In the book &lt;i&gt;&lt;a href=&#34;http://books.google.fr/books?id=e9GWUsQkPNMC&amp;amp;lpg=PA461&amp;amp;vq=longstaff&amp;amp;hl=fr&amp;amp;pg=PA459#v=snippet&amp;amp;q=longstaff&amp;amp;f=false&#34;&gt;Monte Carlo Methods in Financial Engineering&lt;/a&gt;&lt;/i&gt;, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.&lt;br /&gt;&lt;br /&gt;However Longstaff and Schwartz seem to just reuse the paths in &lt;a href=&#34;http://rfs.oxfordjournals.org/content/14/1/113.short&#34;&gt;their paper&lt;/a&gt;, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.&lt;br /&gt;&lt;br /&gt;How large is this bias? What is the correct methodology?&lt;br /&gt;&lt;br /&gt;I have tried with Sobol quasi random numbers to evaluate that bias on a simple Bermudan put option of maturity 180 days, exercisable at 30 days, 60 days, 120 days and 180 days using a Black Scholes volatility of 20% and a dividend yield of 6%. As a reference I use &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;ved=0CDcQFjAA&amp;amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1648878&amp;amp;ei=9151UZ3pM4LZPZ-ugeAF&amp;amp;usg=AFQjCNFS9fdRJt9RoerSnb87YDIZmLcCtw&amp;amp;sig2=k8lHjhUe14ep4giVM5Mr5Q&amp;amp;bvm=bv.45512109,d.ZWU&#34;&gt;a finite difference solver based on TR-BDF2&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;I found it particularly difficult to evaluate it: should we use the same number of paths for the 2 methods or should we use the same number of paths for the monte carlo mean computation only? Should we use the same number of paths for regression and for monte carlo mean computation or should the monte carlo mean computation use much more paths?&lt;br /&gt;&lt;br /&gt;I have tried those combinations and was able to clearly see the bias only in one case: a large number of paths for the Monte-Carlo mean computation compared to the number of paths used for the regression using a fixed total number of paths of 256*1024+1, and 32*1024+1 paths for the regression.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;FDM price=2.83858387194312&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Longstaff discarded paths price=2.8385854695510426&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Longstaff reused paths price=2.8386108892756847&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;Those numbers are too good to be a real. If one reduces too much the total number of paths or the number of paths for the regression, the result is not precise enough to see the bias. For example, using 4K paths for the regression leads to 2.83770 vs 2.83767. Using 4K paths for regression and only 16K paths in total leads to 2.8383 vs 2.8387. Using 32K paths for regressions and increasing to 1M paths in total leads to 2.838539 vs 2.838546.&lt;br /&gt;&lt;br /&gt;For this example the Longstaff-Schwartz price is biased low, the slight increase due to path reuse is not very visible and most of the time does not deteriorate the overall accuracy. But as a result of reusing the paths, the Longstaff-Schwartz price might be higher than the real value.</description>
    </item>
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.&lt;br /&gt;&lt;br /&gt;Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an &lt;a href=&#34;http://bad-concurrency.blogspot.co.uk/2012/08/arithmetic-overflow-and-intrinsics.html&#34;&gt;intrinsic function call&lt;/a&gt; and that should be difficult to beat. However what if one is ok for a bit lower accuracy? Could a simple &lt;a href=&#34;http://www.siam.org/books/ot99/OT99SampleChapter.pdf&#34;&gt;Chebyshev polynomial expansion&lt;/a&gt; be faster?&lt;br /&gt;&lt;br /&gt;Out of curiosity, I tried a Chebyshev polynomial expansion with 10 coefficients stored in a final double array. I computed the coefficient using a precise quadrature (Newton-Cotes) and end up with 1E-9, 1E-10 absolute and relative accuracy on [-1,1].&lt;br /&gt;&lt;br /&gt;Here are the results of a simple sum of 10M random numbers:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.75s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.48s for ChebyshevExp sum=1.718281669341388E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;So while this simple implementation is actually faster than Math.exp (but only works within [-1,1]), FastMath from Apache commons maths, that relies on a table lookup algorithm is just faster (in addition to being more precise and not limited to [-1,1]).&lt;br /&gt;&lt;br /&gt;Of course if I use only 5 coefficients, the speed is better, but the relative error becomes around 1e-4 which is unlikely to be satisfying for a finance application.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.78s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.27s for ChebyshevExp sum=1.718193001875838E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;</description>
    </item>
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</link>
      <pubDate>Thu, 11 Apr 2013 16:29:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/&#34;&gt;previous post&lt;/a&gt;, I explored the &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord-Kahl method&lt;/a&gt; to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Root finding in Lord Kahl Method to Compute Heston Call Price</title>
      <link>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</link>
      <pubDate>Tue, 09 Apr 2013 19:49:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price/</guid>
      <description>&lt;p&gt;&lt;figure&gt;&lt;img src=&#34;https://chasethedevil.github.io/post/Screenshot%20from%202013-04-09%2019%2042%2009.png&#34;&gt;&#xA;&lt;/figure&gt;&#xA;&#xA;I just tried to implement &lt;a href=&#34;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=921336&#34;&gt;Lord Kahl algorithm to compute the Heston call price&lt;/a&gt;. The big difficulty of their method is to find the optimal alpha.  That&amp;rsquo;s what make it work or break. The tricky part is that the function  of alpha we want to minimize has multiple discontinuities (it&amp;rsquo;s  periodic in some ways). This is why the authors rely on the computation  of an alpha_max: bracketing is very important, otherwise your optimizer  will jump the discontinuity without even noticing it, while you really  want to stay in the region before the first discontinuity.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;Marsaglia in &lt;!-- raw HTML omitted --&gt;his paper on Normal Distribution&lt;!-- raw HTML omitted --&gt; made the same mistake I initially did while trying to verify &lt;!-- raw HTML omitted --&gt;the accuracy of the normal density&lt;!-- raw HTML omitted --&gt;.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.6 because of the truncation at machine epsilon precision. Using Python mpmath, one can see that:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; mpf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;rsquo;-16.6000000000000014210854715202004&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;This is the more accurate representation if one goes beyond double precision (here 30 digits). And the value of the cumulative normal distribution is:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.4845465199503256054808152068743e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It is different from:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(mpf(&amp;quot;-16.6&amp;quot;))&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.48454651995040810217553910503186e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;where in this case it is really evaluated around -16.6 (up to 30 digits precision). Marsaglia gives this second number as reference. But all the other algorithms will actually take as input the first input. It is more meaningful to compare results using the exact same input. Using human readable but computer truncated numbers is not the best.  The cumulative normal distribution will often be computed using some output of some calculation where one does not have an exact human readable input.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The standard code for Ooura and Schonfelder (as well as Marsaglia) algorithms for the cumulative normal distribution don&amp;rsquo;t use Cody&amp;rsquo;s trick to evaluate the exp(-x&lt;em&gt;x). This function appears in all those implementations because it is part of the dominant term in the usual expansions. Out of curiosity, I replaced this part with Cody trick. For Ooura I also made minor changes to make it work directly on the CND instead of going through the error function erfc indirection. Here are the results without the Cody trick (except for Cody):&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;and with it:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;All 3 algorithms are now of similiar accuracy (note the difference of scale compared to the previous graph), with Schonfelder being a bit worse, especially for x &amp;gt;= -20. If one uses only easily representable numbers (for example -37, -36,75, -36,5, &amp;hellip;) in double precision then, of course, Cody trick importance won&amp;rsquo;t be visible and here is how the 3 algorithms would fare with or without Cody trick:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Schonfelder looks now worse than it actually is compared to Cody and Ooura.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;To conclude, if someone claims that a cumulative normal distribution is up to double precision accuracy and it does not use any tricks to compute exp(-x&lt;/em&gt;x), then beware, it probably is quite a bit less than double precision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cracking the Double Precision Gaussian Puzzle</title>
      <link>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</link>
      <pubDate>Fri, 22 Mar 2013 12:20:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/cracking-the-double-precision-gaussian-puzzle/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;https://chasethedevil.github.io/post/a-double-precision-puzzle-with-the-gaussian/&#34;&gt;previous post&lt;/a&gt;, I stated that some library (SPECFUN by W.D. Cody) computes \(e^{-\frac{x^2}{2}}\) the following way:&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-C&#34; data-lang=&#34;C&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(x &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;1.6&lt;/span&gt;;&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;del &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt; xsq) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; (x &lt;span style=&#34;color:#666&#34;&gt;+&lt;/span&gt; xsq);&#xA;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;result &lt;span style=&#34;color:#666&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; xsq &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#06287e&#34;&gt;exp&lt;/span&gt;(&lt;span style=&#34;color:#666&#34;&gt;-&lt;/span&gt;del &lt;span style=&#34;color:#666&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#40a070&#34;&gt;0.5&lt;/span&gt;);&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;&#xA;&lt;p&gt;where &lt;code class=&#34;code-inline language-C&#34;&gt;&lt;span style=&#34;color:#06287e&#34;&gt;fint&lt;/span&gt;(z)&lt;/code&gt; computes the floor of z.&lt;/p&gt;&#xA;&lt;ol&gt;&#xA;&lt;li&gt;Why 1.6?&lt;/li&gt;&#xA;&lt;/ol&gt;&#xA;&lt;p&gt;An integer divided by 1.6 will be an exact representation of the corresponding number in double: 1.6 because of 16 (dividing by 1.6 is equivalent to multiplying by 10 and dividing by 16 which is an exact operation). It also allows to have something very close to a rounding function: x=2.6 will make xsq=2.5, x=2.4 will make xsq=1.875, x=2.5 will make xsq=2.5. The maximum difference between x and xsq will be 0.625.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GPU computing in Finance</title>
      <link>https://chasethedevil.github.io/post/gpu-computing-in-finance/</link>
      <pubDate>Mon, 15 Oct 2012 16:14:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/gpu-computing-in-finance/</guid>
      <description>&lt;p&gt;Very interesting presentation from Murex about their GPU computing. Some points were:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;GPU demand for mostly exotics pricing and greeks&lt;/li&gt;&#xA;&lt;li&gt;Local vol main model for EQD exotics. Local vol calibrated via PDE approach.&lt;/li&gt;&#xA;&lt;li&gt;Markov functional model becoming main model for IRD.&lt;/li&gt;&#xA;&lt;li&gt;Use of local regression instead of Longstaff Schwartz (or worse CVA like sim of sim).&lt;/li&gt;&#xA;&lt;li&gt;philox RNG from DE Shaw. But the presenter does not seem to know RNGs  very well (recommended Brownian Bridge for Mersenne Twister!).&lt;/li&gt;&#xA;&lt;li&gt;An important advantage of GPU is latency. Grid computing only improves throughput but not latency. GPU improves both.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;&lt;a href=&#34;http://nvidia.fullviewmedia.com/gtc2010/0923-a7-2032.html&#34;&gt;http://nvidia.fullviewmedia.com/gtc2010/0923-a7-2032.html&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Binary Voting</title>
      <link>https://chasethedevil.github.io/post/binary-voting/</link>
      <pubDate>Fri, 07 Sep 2012 17:21:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/binary-voting/</guid>
      <description>How many reports have you had to fill up with a number of stars to choose? How much useless time is spent on figuring the this number just because it is always very ambiguous?&lt;br /&gt;&lt;br /&gt;Some blogger wrote an interesting entry on &lt;a href=&#34;http://davidcelis.com/blog/2012/02/01/why-i-hate-five-star-ratings/&#34;&gt;Why I Hate Five Stars Reviews&lt;/a&gt;. Basically he advocates binary voting instead via like/dislike. Maybe a ternary system via like/dislike/don&#39;t care would be ok too.&lt;br /&gt;&lt;br /&gt;One coworker used to advocate the same for a similar reason: people reading those reports only pay attention to the extremes: the 5 stars or the 0 stars. So if you want to have a voice, you need to express it via 5 or 0, nothing in between.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;</description>
    </item>
    <item>
      <title>Adaptive Quadrature for Pricing European Option with Heston</title>
      <link>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</link>
      <pubDate>Mon, 25 Jun 2012 12:50:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/adaptive-quadrature-for-pricing-european-option-with-heston/</guid>
      <description>The Quantlib code to evaluate the Heston integral for European options is quite nice. It proposes &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=4&amp;amp;ved=0CGoQFjAD&amp;amp;url=http%3A%2F%2Fwww.math.uni-wuppertal.de%2F%7Ekahl%2Fpublications%2FNotSoComplexLogarithmsInTheHestonModel.pdf&amp;amp;ei=MkDoT8HHOaO_0QXS_6SeCQ&amp;amp;usg=AFQjCNFbAMQBLoKRd0BR_-HC0CkP4zrMtg&#34;&gt;Kahl &amp;amp; Jaeckel&lt;/a&gt; method as well as Gatheral method for the complex logarithm. It also contains expansions where it matters so that the resulting code is very robust. One minor issue is that it does not integrate both parts at the same time, and also does not propose Attari method for the Heston integral that is supposed to be more stable.&lt;br /&gt;&lt;br /&gt;I was surprised to find out that out of the money, short expiry options seemed badly mispriced. In the end I discovered it was just that it required sometimes more than 3500 function evaluations to have an accuracy of 1e-6.&lt;br /&gt;&lt;br /&gt;As this sounds a bit crazy, I thought that Jaeckel log transform was the culprit. In reality, it turned out that it was &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CFMQFjAA&amp;amp;url=http%3A%2F%2Fusers.wpi.edu%2F%7Ewalker%2FMA510%2FHANDOUTS%2Fw.gander%2Cw.gautschi%2CAdaptive_Quadrature%2CBIT_40%2C2000%2C84-101.pdf&amp;amp;ei=U0HoT_HWBeXP0QW23O3xBw&amp;amp;usg=AFQjCNH4KRWMprUvL8yBPKRxO_sVNyc2Pg&#34;&gt;Gauss Lobatto Gander &amp;amp; Gautschi implementation&lt;/a&gt;. I tried the simplest algorithm in &lt;a href=&#34;http://www.ii.uib.no/%7Eterje/Papers/bit2003.pdf&#34;&gt;Espelid improved algorithms&lt;/a&gt;: modsim, an adaptive extrapolated Simpson method, and it was 4x faster for the same accuracy. That plus the fact that it worked out of the box (translated to Java) on my problem was impressive.&lt;br /&gt;&lt;br /&gt;Jaeckel log transform (to change the interval from 0,+inf to 0,1) works well, and seems to offer a slight speedup (10% to 15%) for around ATM options, mid to long term for the same accuracy. Unfortunately, it can also slow down by up to 50% the convergence for more OTM options or shorter expiries. So I am not so sure about its interest vs just cutting off the integration at phi=200.</description>
    </item>
    <item>
      <title>Why primitive arrays matter in Java</title>
      <link>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</link>
      <pubDate>Wed, 29 Feb 2012 10:01:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/why-primitive-arrays-matter-in-java/</guid>
      <description>&lt;p&gt;In the past, I have seen that one could greatly improve performance of some Monte-Carlo simulation by using as much as possible &lt;code&gt;double[][]&lt;/code&gt; instead of arrays of objects.&lt;/p&gt;&#xA;&lt;p&gt;It was interesting to read &lt;a href=&#34;http://flavor8.com/index.php/2012/02/25/java-performance-autoboxing-and-data-structure-choice-obviously-matter-but-by-how-much/&#34;&gt;this blog post explaining why that happens&lt;/a&gt;: it is all about memory access.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generating random numbers following a given discrete probability distribution</title>
      <link>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</link>
      <pubDate>Mon, 09 Jan 2012 00:14:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/generating-random-numbers-following-a-given-discrete-probability-distribution/</guid>
      <description>&lt;p&gt;I have never really thought very much about generating random numbers according to a precise discrete distribution, for example to simulate an unfair dice.&lt;/p&gt;&#xA;&lt;p&gt;In finance, we are generally interested in continuous distributions, where there is typically 2 ways:&lt;/p&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;the &lt;a href=&#34;http://en.wikipedia.org/wiki/Inverse_transform_sampling&#34;&gt;inverse transform&lt;/a&gt; (usually computed in a numerical way),&lt;/li&gt;&#xA;&lt;li&gt;and the &lt;a href=&#34;http://en.wikipedia.org/wiki/Rejection_sampling&#34;&gt;acceptance-rejection&lt;/a&gt; method, typically the &lt;a href=&#34;http://en.wikipedia.org/wiki/Ziggurat_algorithm&#34;&gt;ziggurat&lt;/a&gt;.&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;p&gt;The inverse transform is often preferred, because it&amp;rsquo;s usable method for Quasi Monte-Carlo simulations while the acceptance rejection is not.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;I would have thought about the simple way to generate random numbers according to a discrete distribution as first described &lt;a href=&#34;http://www.delicious.com/redirect?url=http%3A//blog.sigfpe.com/2012/01/lossless-decompression-and-generation.html&#34;&gt;here&lt;/a&gt;. But establishing a link with &lt;a href=&#34;http://en.wikipedia.org/wiki/Huffman_coding&#34;&gt;Huffman encoding&lt;/a&gt; is brilliant. Some better performing alternative (unrelated to Huffman) is offered &lt;a href=&#34;http://www.keithschwarz.com/darts-dice-coins/&#34;&gt;there&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Quant Interview &amp; Education</title>
      <link>https://chasethedevil.github.io/post/quant-interview--education/</link>
      <pubDate>Wed, 21 Dec 2011 17:37:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/quant-interview--education/</guid>
      <description>Recently, I interviewed someone for a quant position. I was very surprised to find out that someone who did one of the best master in probabilities and finance in France could not solve a very basic probability problem:&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: left;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-64ejCSg9Y8o/TvIHEF4PgwI/AAAAAAAAFio/uxVwdwVnQ54/s1600/Screenshot+at+2011-12-21+17%253A18%253A28.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;43&#34; src=&#34;http://2.bp.blogspot.com/-64ejCSg9Y8o/TvIHEF4PgwI/AAAAAAAAFio/uxVwdwVnQ54/s400/Screenshot+at+2011-12-21+17%253A18%253A28.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;This is accessible to someone with very little knowledge of probabilities &lt;br /&gt;&lt;br /&gt;When I asked this problem around to co-workers (who have all at least a master in a scientific subject), very few could actually answer it properly. Most of the time, I suspect it is because they did not dedicate enough time to do it properly, and wanted to answer it too quickly.&lt;br /&gt;&lt;br /&gt;It was more shocking that someone just out of school, with a major in probabilities could not answer that properly. It raises the question: what is all this education worth?&lt;br /&gt;&lt;br /&gt;The results were not better as soon as the question was not exactly like what students in those masters are used to, like for example, this simple stochastic calculus question:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-K9mO8qrp1Ow/TvIKO_MTOSI/AAAAAAAAFiw/igPGRdnFNEo/s1600/Screenshot+at+2011-12-21+17%253A19%253A11.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://2.bp.blogspot.com/-K9mO8qrp1Ow/TvIKO_MTOSI/AAAAAAAAFiw/igPGRdnFNEo/s1600/Screenshot+at+2011-12-21+17%253A19%253A11.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;My opinion is that, today in our society, people study for too long. The ideal system for me would be one where people learn a lot in math/physics the first 2 years of university, and then have more freedom in their education, much like a doctorate.&lt;br /&gt;&lt;br /&gt;We still offered the job to this person, because live problem solving is not the most important criteria. Other qualities like seriousness and motivation are much more valuable.</description>
    </item>
    <item>
      <title>Good &amp; Popular Algorithms are Simple</title>
      <link>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</link>
      <pubDate>Thu, 17 Nov 2011 12:28:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/good--popular-algorithms-are-simple/</guid>
      <description>I recently tried to minimise a function according to some constraints. One popular method to minimise a function in several dimensions is &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CB0QFjAA&amp;amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FNelder%25E2%2580%2593Mead_method&amp;amp;ei=PfHETsKYDdG9sAb_q935Cw&amp;amp;usg=AFQjCNEVD7lMV4buMbVCJ3fuiyupPA6B1w&#34;&gt;Nelder-Mead Simplex&lt;/a&gt;. It is quite simple, so simple that I programmed it in Java in 1h30, including a small design and a test. It helped that the original paper from Nelder-Mead is very clear:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-glZ52IB0SkA/TsTxF4VmyFI/AAAAAAAAFhw/zSDQ0Het9MU/s1600/neldermeadalgo.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;184&#34; src=&#34;http://3.bp.blogspot.com/-glZ52IB0SkA/TsTxF4VmyFI/AAAAAAAAFhw/zSDQ0Het9MU/s320/neldermeadalgo.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;However the main issue is that it works only for unconstrained problems. Nelder and Mead suggested to add a penalty, but in practice this does not work so well. For constrained problems, there is an adaptation of the original idea by Box (incredible name for a constrained method) that he called &lt;a href=&#34;http://www.google.com/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;ved=0CB4QFjAA&amp;amp;url=http%3A%2F%2Fcomjnl.oxfordjournals.org%2Fcontent%2F8%2F1%2F42.full.pdf&amp;amp;ei=EvLETqz6LcTvsgbN4I2ADA&amp;amp;usg=AFQjCNHq6JD1ik4rTLO4a1v4G9adbGMRzQ&#34;&gt;the Complex method&lt;/a&gt;, a deliberate pun to the Nelder-Mead Simplex method. The basic idea is to reset the trial point near the fixed boundary if it goes outside. Now this took me much longer to program, as the paper is not as clear, even if the method is still relatively simple. But worst, after a day of deciphering the paper and programming the complex method, I find out that it does not works so well: it does not manage to minimise a simple multidimensional quadratic with a simple bound constraint: f(X)=sum(X)^2 with X &amp;gt;= 0 where X is an N-dimensional vector. In the end, I don&#39;t want to work with such a simple function, but it is a good simple test to see if the method is really working or not. Here is how the function looks with N=2:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-ncMKzKcYdtA/TsT_TEyNHDI/AAAAAAAAFiA/er1pQWNcHXc/s1600/quadfull.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://2.bp.blogspot.com/-ncMKzKcYdtA/TsT_TEyNHDI/AAAAAAAAFiA/er1pQWNcHXc/s320/quadfull.png&#34; width=&#34;302&#34; /&gt;&lt;/a&gt;&lt;/div&gt;And if we restrict to x,y &amp;gt;= 0 it becomes:&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-msyZ9eQ7GnM/TsT_d36SD7I/AAAAAAAAFiI/Kt4QBwjsAE8/s1600/quadpart.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;304&#34; src=&#34;http://1.bp.blogspot.com/-msyZ9eQ7GnM/TsT_d36SD7I/AAAAAAAAFiI/Kt4QBwjsAE8/s320/quadpart.png&#34; width=&#34;320&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;I suspected an error in my program, so I decided to try with scilab, that has also the Box method as part of their &lt;a href=&#34;http://help.scilab.org/docs/5.3.3/en_US/neldermead.html&#34;&gt;neldermead_search&lt;/a&gt; functionality. Scilab also failed to compute the minimum in 8 dimensions of my simple quadratic. &lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-vO6JhSwv3MU/TsT1ge1H96I/AAAAAAAAFh4/DTnsYjnMeHE/s1600/neldermeadalgo1.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/-vO6JhSwv3MU/TsT1ge1H96I/AAAAAAAAFh4/DTnsYjnMeHE/s320/neldermeadalgo1.png&#34; width=&#34;265&#34; /&gt;&lt;/a&gt;&lt;/div&gt;I tried various settings, without ever obtaining a decent result (I expect to find a value near 0).&lt;br /&gt;&lt;br /&gt;There is another algorithm that can find a global minimum, also very popular: the &lt;a href=&#34;http://www.icsi.berkeley.edu/%7Estorn/TR-95-012.pdf&#34;&gt;Differential Evolution&lt;/a&gt;. At first, being a genetic algorithm, you would think it would be complicated to write. But no, the main loop is around 20 lines.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-cNeeYo8EQeQ/TsUB7Ek6rQI/AAAAAAAAFiQ/PTc4qwwAa7E/s1600/dealgo.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;320&#34; src=&#34;http://4.bp.blogspot.com/-cNeeYo8EQeQ/TsUB7Ek6rQI/AAAAAAAAFiQ/PTc4qwwAa7E/s320/dealgo.png&#34; width=&#34;297&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;Those 20 lines are a bit more difficult than Nelder-Mead, but still, when I saw that, I understood that &#34;this is a classic algorithm&#34;. And it does work with constraints easily. How to do this is explained well in K. Price book &#34;Differential Evolution&#34;, and takes only a few lines of code. Here is the result I got in dimension 29:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace; font-size: x-small;&#34;&gt;dim=29 min=1.2601854176573729E-12 genMax=412 x=[3.340096901536317E-8, 8.889252404343621E-8, 7.163904251807348E-10, 9.71847877381699E-9, 2.7423324674150668E-8, 2.4022269439114537E-9, 1.7336434478718816E-11, 7.244238163901778E-9, 1.0013136274729337E-8, 7.412154679865083E-9, 5.4694460144807974E-9, 2.3682413086417524E-9, 4.241739250073559E-7, 4.821920889534676E-10, 2.115396281722523E-9, 8.750883007882899E-8, 2.512011485133975E-9, 4.811507109129279E-9, 1.0752997894113096E-7, 5.120475258343548E-9, 8.404448964497456E-9, 4.1062290228305595E-9, 1.7030766521603753E-8, 5.589430643552073E-9, 8.237098544820173E-10, 3.5796523161196554E-9, 5.186299547466997E-9, 2.430326342762937E-7, 5.493850433494286E-9]&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;It works well, although there is quite a bit of parameters. I noticed that the strategy was especially important.</description>
    </item>
    <item>
      <title>SIMD and Mersenne-Twister</title>
      <link>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</link>
      <pubDate>Sat, 05 Feb 2011 13:18:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/simd-and-mersenne-twister/</guid>
      <description>&lt;p&gt;Since 2007, there is a new kind of Mersenne-Twister (MT) that exploits SIMD architecture, the &lt;a href=&#34;http://www.math.sci.hiroshima-u.ac.jp/%7Em-mat/MT/SFMT/&#34;&gt;SFMT&lt;/a&gt;. The Mersenne-Twister has set quite a standard in random number generation for Monte-Carlo simulations, even though it has flaws.&lt;/p&gt;&#xA;&lt;p&gt;I was wondering if SFMT improved the performance over MT for a Java implementation. There is actually on the same page a decent Java port of the original algorithm. When I ran it, it ended up slower by more than 20% than the classical Mersenne-Twister (32-bit) on a 64-bit JDK 1.6.0.23 for Windows.&lt;/p&gt;</description>
    </item>
    <item>
      <title>The CUDA Performance Myth</title>
      <link>https://chasethedevil.github.io/post/the-cuda-performance-myth/</link>
      <pubDate>Mon, 03 Jan 2011 16:07:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/the-cuda-performance-myth/</guid>
      <description>&lt;p&gt;There is an &lt;a href=&#34;http://arxiv.org/PS_cache/arxiv/pdf/0901/0901.0638v4.pdf&#34;&gt;interesting&lt;/a&gt; article on how to generate efficiently the inverse of the normal cumulative distribution on the GPU. This is useful for Monte-Carlo simulations based on normally distributed variables.&lt;/p&gt;&#xA;&lt;p&gt;Another result of the paper is a method (breakless algorithm) to compute it apparently faster than the very good &lt;a href=&#34;http://www.mth.kcl.ac.uk/~shaww/web_page/papers/Wichura.pdf&#34;&gt;Wichura&amp;rsquo;s AS241&lt;/a&gt; algorithm on the CPU as well keeping a similar precision. The key is to avoid branches (if-then) at the cost of not avoiding log() calls. As the algorithm is very simple, I decided to give it a try in Java.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Another Look at Java Matrix Libraries</title>
      <link>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</link>
      <pubDate>Mon, 29 Nov 2010 12:45:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/</guid>
      <description>&lt;p&gt;A while ago, &lt;a href=&#34;https://chasethedevil.github.io/post/the-pain-of-java-matrix-libraries&#34;&gt;I was already looking&lt;/a&gt; for a good Java Matrix library, complaining that there does not seem any real good one where development is still active: the 2 best ones are in my opinion &lt;a href=&#34;http://math.nist.gov/javanumerics/jama/&#34;&gt;Jama&lt;/a&gt; and &lt;a href=&#34;http://dsd.lbl.gov/~hoschek/colt/&#34;&gt;Colt&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;Recently I tried to &lt;a href=&#34;http://www.wilmott.com/detail.cfm?articleID=345&#34;&gt;price options via RBF&lt;/a&gt; (radial basis functions) based on &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1648878&#34;&gt;TR-BDF2&lt;/a&gt; time stepping.&#xA;This is a problem where one needs to do a few matrix multiplications and inverses (or better, LU solve) in a loop. The size of the matrix is typically 50x50 to 100x100, and one can loop between 10 and 1000 times.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Out of curiosity I decided to give &lt;a href=&#34;http://ojalgo.org/&#34;&gt;ojalgo&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/p/matrix-toolkits-java&#34;&gt;MTJ&lt;/a&gt; a chance. I had read benchmarks (&lt;a href=&#34;http://blog.mikiobraun.de/2009/04/some-benchmark-numbers-for-jblas.html&#34;&gt;here about jblas&lt;/a&gt; and &lt;a href=&#34;http://code.google.com/p/java-matrix-benchmark/wiki/Runtime_2xXeon_2010_08&#34;&gt;here the java matrix benchmark&lt;/a&gt;) where those libraries performed really well.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;On my core i5 laptop under the latest 64bit JVM (Windows 7), I found out that for the 100x100 case, &lt;em&gt;Jama was actually 30% faster than MTJ&lt;/em&gt;, and ojalgo was more than 50% slower. I also found out that I did not like ojalgo API at all. I was quite disappointed by those results.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Diffusion Limited Aggregation Applet</title>
      <link>https://chasethedevil.github.io/post/diffusion-limited-aggregation-applet/</link>
      <pubDate>Wed, 09 Jun 2010 14:05:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/diffusion-limited-aggregation-applet/</guid>
      <description>Yes, I wrote an applet. I know it is very 1990s but, amazingly, it still does the job quite well. Ok, next time I should really use Flash to do this.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/_9RyqGT46Fbk/TA-C4-xEq8I/AAAAAAAAFH8/OwEFEpO4eA8/s1600/dla.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; src=&#34;http://1.bp.blogspot.com/_9RyqGT46Fbk/TA-C4-xEq8I/AAAAAAAAFH8/OwEFEpO4eA8/s320/dla.png&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;The Applet simulates &lt;a href=&#34;http://en.wikipedia.org/wiki/Diffusion-limited_aggregation&#34;&gt;Diffusion Limited Aggregation&lt;/a&gt; as described in Chaos And Fractals from Peitgen, Juergens, and Saupe. It represents ions randomly wandering around (in a Brownian motion) until they are caught by an attractive force in electrochemical deposition experiment. This kind of phenomenon occurs at all scales, for example it happens in the distribution of galaxies. You can play around with the applet at &lt;a href=&#34;http://31416.appspot.com/dla.vm&#34;&gt;http://31416.appspot.com/dla.vm&lt;/a&gt;</description>
    </item>
    <item>
      <title>double[][] Is Fine</title>
      <link>https://chasethedevil.github.io/post/double-is-fine/</link>
      <pubDate>Thu, 26 Nov 2009 14:51:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/double-is-fine/</guid>
      <description>In my previous post, I suggest that keeping a double[] performs better than keeping a double[][] if you do matrix multiplications and other operations.&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;This is actually not true. I benchmarked 3 libraries, Colt (uses double[]), Apache Commons Math (uses double[][]) and Jama (uses double[][] cleverly). At first it looks like Jama has a similar performance as Colt (they avoid [][] slow access by a clever algorithm). But once hotspot hits, the difference is crazy and Jama becomes the fastest (Far ahead).&lt;/div&gt;&lt;br /&gt;&lt;table border=&#34;1&#34;&gt;&lt;tr&gt;&lt;td colspan=&#34;4&#34;  valign=&#34;bottom&#34;  align=&#34;center&#34;  style=&#34; font-size:10pt;&#34;&gt;&lt;b&gt;JDK 1.6.0 Linux 1000x1000 matrix multiplication on Intel Q6600&lt;/b&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;loop index&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;Colt&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;Commons Math&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;Jama&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;1&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;11.880748&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;24.455125&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.828977&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;2&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;11.874975&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;24.265102&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.848916&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;3&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.772616&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.374153&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;9.826572&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;4&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.759679&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.368105&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.655915&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;5&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.799622&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.238928&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.649129&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;6&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.780556&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;14.741863&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.668104&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;7&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.72831&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.509909&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646811&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;8&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.79838&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.724348&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646069&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.726143&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.988762&lt;/td&gt;&lt;br /&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.646052&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;10&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;9.784505&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000;&#34;&gt;15.121782&lt;/td&gt;&lt;td  valign=&#34;bottom&#34;  align=&#34;right&#34;  style=&#34; font-size:10pt; border-top:thin solid #000000; border-bottom:thin solid #000000; border-left:thin solid #000000; border-right:thin solid #000000;&#34;&gt;2.644572&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td  style=&#34;&#34;&gt;&lt;/td&gt;&lt;td colspan=&#34;10&#34;  valign=&#34;bottom&#34;  align=&#34;left&#34;  style=&#34; font-size:10pt;&#34;&gt;We don&#39;t include matrix construction time, and fetching the result. Only the multiplication is taken into account.&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;br /&gt;The difference is less pronounced on smaller matrices, but still there. Jama looks very good in this simple test case. In more real scenarios, the difference is not so obvious. For example Commons Math SVD is faster than Jama one.</description>
    </item>
  </channel>
</rss>
