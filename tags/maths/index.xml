<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Maths on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/maths/</link>
    <description>Recent content in Maths on Chase the Devil</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Fri, 19 Apr 2013 16:48:00 +0000</lastBuildDate>
    <atom:link href="https://chasethedevil.github.io/tags/maths/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.&lt;br /&gt;&lt;br /&gt;Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an &lt;a href=&#34;http://bad-concurrency.blogspot.co.uk/2012/08/arithmetic-overflow-and-intrinsics.html&#34;&gt;intrinsic function call&lt;/a&gt; and that should be difficult to beat. However what if one is ok for a bit lower accuracy? Could a simple &lt;a href=&#34;http://www.siam.org/books/ot99/OT99SampleChapter.pdf&#34;&gt;Chebyshev polynomial expansion&lt;/a&gt; be faster?&lt;br /&gt;&lt;br /&gt;Out of curiosity, I tried a Chebyshev polynomial expansion with 10 coefficients stored in a final double array. I computed the coefficient using a precise quadrature (Newton-Cotes) and end up with 1E-9, 1E-10 absolute and relative accuracy on [-1,1].&lt;br /&gt;&lt;br /&gt;Here are the results of a simple sum of 10M random numbers:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.75s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.48s for ChebyshevExp sum=1.718281669341388E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;So while this simple implementation is actually faster than Math.exp (but only works within [-1,1]), FastMath from Apache commons maths, that relies on a table lookup algorithm is just faster (in addition to being more precise and not limited to [-1,1]).&lt;br /&gt;&lt;br /&gt;Of course if I use only 5 coefficients, the speed is better, but the relative error becomes around 1e-4 which is unlikely to be satisfying for a finance application.&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;0.78s for Math.exp sum=1.7182816693332244E7&lt;br /&gt;0.27s for ChebyshevExp sum=1.718193001875838E7&lt;br /&gt;0.40s for FastMath.exp sum=1.7182816693332244E7&lt;/span&gt;</description>
    </item>
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>&lt;p&gt;Marsaglia in &lt;!-- raw HTML omitted --&gt;his paper on Normal Distribution&lt;!-- raw HTML omitted --&gt; made the same mistake I initially did while trying to verify &lt;!-- raw HTML omitted --&gt;the accuracy of the normal density&lt;!-- raw HTML omitted --&gt;.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.6 because of the truncation at machine epsilon precision. Using Python mpmath, one can see that:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; mpf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;rsquo;-16.6000000000000014210854715202004&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;This is the more accurate representation if one goes beyond double precision (here 30 digits). And the value of the cumulative normal distribution is:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(-16.6)&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.4845465199503256054808152068743e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;It is different from:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&amp;gt;&amp;gt;&amp;gt; ncdf(mpf(&amp;quot;-16.6&amp;quot;))&lt;!-- raw HTML omitted --&gt;mpf(&amp;lsquo;3.48454651995040810217553910503186e-62&amp;rsquo;)&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;where in this case it is really evaluated around -16.6 (up to 30 digits precision). Marsaglia gives this second number as reference. But all the other algorithms will actually take as input the first input. It is more meaningful to compare results using the exact same input. Using human readable but computer truncated numbers is not the best.  The cumulative normal distribution will often be computed using some output of some calculation where one does not have an exact human readable input.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;The standard code for Ooura and Schonfelder (as well as Marsaglia) algorithms for the cumulative normal distribution don&amp;rsquo;t use Cody&amp;rsquo;s trick to evaluate the exp(-x&lt;em&gt;x). This function appears in all those implementations because it is part of the dominant term in the usual expansions. Out of curiosity, I replaced this part with Cody trick. For Ooura I also made minor changes to make it work directly on the CND instead of going through the error function erfc indirection. Here are the results without the Cody trick (except for Cody):&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;and with it:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;All 3 algorithms are now of similiar accuracy (note the difference of scale compared to the previous graph), with Schonfelder being a bit worse, especially for x &amp;gt;= -20. If one uses only easily representable numbers (for example -37, -36,75, -36,5, &amp;hellip;) in double precision then, of course, Cody trick importance won&amp;rsquo;t be visible and here is how the 3 algorithms would fare with or without Cody trick:&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Schonfelder looks now worse than it actually is compared to Cody and Ooura.&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;To conclude, if someone claims that a cumulative normal distribution is up to double precision accuracy and it does not use any tricks to compute exp(-x&lt;/em&gt;x), then beware, it probably is quite a bit less than double precision.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cholesky &amp; Jakarta Commons Math</title>
      <link>https://chasethedevil.github.io/post/cholesky--jakarta-commons-math/</link>
      <pubDate>Fri, 15 May 2009 19:01:00 +0000</pubDate>
      <guid>https://chasethedevil.github.io/post/cholesky--jakarta-commons-math/</guid>
      <description>&lt;p&gt;In Finance, &lt;a href=&#34;http://en.wikipedia.org/wiki/Cholesky_decomposition&#34;&gt;Cholesky&lt;/a&gt; is a useful way to decompose Matrix. It is not so simple to find a BSD licensed code using cholesky (most of them are GPL like &lt;a href=&#34;http://www.google.com/codesearch/p?hl=en#W_wVDN7F3h4/tetrad-4.3.2-0/src/edu/cmu/tetrad/util/MatrixUtils.java&amp;amp;q=cholesky%20SEMEditor&amp;amp;l=1332&#34;&gt;this one&lt;/a&gt;). There is &lt;a href=&#34;http://svn.apache.org/viewvc/commons/proper/math/trunk/src/java/org/apache/commons/math/linear/decomposition/CholeskyDecompositionImpl.java?view=co&#34;&gt;one&lt;/a&gt; in &lt;a href=&#34;http://commons.apache.org/math/&#34;&gt;Apache Commons Maths&lt;/a&gt; library, which is a very interesting library. However for performance, it is still not very practical for some things like Cholesky.&lt;/p&gt;&#xA;&lt;p&gt;Looking at the source one can easily understand why. I did a small (many people will say not representative 1 million loop test) and finds out:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
