<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>maths on Chase the Devil</title>
    <link>https://chasethedevil.github.io/tags/maths/</link>
    <description>Recent content in maths on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>Copyright 2006-2018 Fabien Le Floc&#39;h. This work is licensed under a Creative Commons Attribution 4.0 International License.</copyright>
    <lastBuildDate>Fri, 19 Apr 2013 16:48:00 +0000</lastBuildDate>
    
	<atom:link href="https://chasethedevil.github.io/tags/maths/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>A Fast Exponential Function in Java</title>
      <link>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</link>
      <pubDate>Fri, 19 Apr 2013 16:48:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/</guid>
      <description>In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.
Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an intrinsic function call and that should be difficult to beat.</description>
    </item>
    
    <item>
      <title>From Double Precision Normal Density to Double Precision Cumulative Normal Distribution</title>
      <link>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</link>
      <pubDate>Tue, 02 Apr 2013 14:24:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/from-double-precision-normal-density-to-double-precision-cumulative-normal-distribution/</guid>
      <description>Marsaglia in his paper on Normal Distribution made the same mistake I initially did while trying to verify the accuracy of the normal density.
In his table of values comparing the true value computed by Maple for some values of x to the values computed by Sun or Ooura erfc, he actually does not really use the same input for the comparison. One example is the last number: 16.6. 16.6 does not have an exact representation in double precision, even though it is displayed as 16.</description>
    </item>
    
    <item>
      <title>Cholesky &amp; Jakarta Commons Math</title>
      <link>https://chasethedevil.github.io/post/cholesky-jakarta-commons-math/</link>
      <pubDate>Fri, 15 May 2009 19:01:00 +0000</pubDate>
      
      <guid>https://chasethedevil.github.io/post/cholesky-jakarta-commons-math/</guid>
      <description>In Finance, Cholesky is a useful way to decompose Matrix. It is not so simple to find a BSD licensed code using cholesky (most of them are GPL like this one). There is one in Apache Commons Maths library, which is a very interesting library. However for performance, it is still not very practical for some things like Cholesky.
Looking at the source one can easily understand why. I did a small (many people will say not representative 1 million loop test) and finds out:</description>
    </item>
    
  </channel>
</rss>