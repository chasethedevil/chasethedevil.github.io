<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Chase the Devil</title>
    <link>http://chasethedevil.github.io/post/</link>
    <description>Recent content in Posts on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Jul 2016 09:55:32 +0100</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Number of regressors in a BSDE</title>
      <link>http://chasethedevil.github.io/post/number_of_regressors_in_bdse/</link>
      <pubDate>Tue, 26 Jul 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/number_of_regressors_in_bdse/</guid>
      <description>&lt;p&gt;Last year, I was kindly invited at the workshop on Models and Numerics in Financial Mathematics at the Lorentz center.
It was surprinsgly interesting on many different levels. Beside the relatively large gap between academia and the industry, which this
workshop was trying to address, one thing that struck me is how difficult it was for people of slightly different specialties to communicate.&lt;/p&gt;

&lt;p&gt;It seemed that mathematicians of different countries working on different subjects related to backward stochastic differential equations (BSDEs) would not truly understand each other. I know this is
very subjective, and maybe those mathematicians did not feel this at all. One concrete example is related to the number of regressors needed to solve a BSDE on 10 different variables.
Solving a BSDE on 10 variables for EDF was given as an illustration at the end of an otherwise excellent presentation.
Someone in the audience asked how possibly they could do that in practice since it would involve \(10^{10}\) regression factors. The answer of the speaker was more or less that it was what they do, with no particular trick but with a large computing power, as if \(10^{10}\) was not so big.&lt;/p&gt;

&lt;p&gt;At this point I was really wondering why \(10^{10}\). Usually, when we do regressions in some BSDE like algorithm, for example Longstaff-Schwartz for American Monte-Carlo, we don&amp;rsquo;t consider all the powers of each variables  from 0 to 10 as well as their cross products.
We only consider polynomials of degree 10, that is all the cross-combinations that lead to a total degree of 10 or less. On two variables \(X\) and \(Y\), we care about \(1, X, Y, XY, X^2, Y^2\) but we dont care about \(X^2 Y, X Y^2, X^2 Y^2\).&lt;/p&gt;

&lt;p&gt;We can count then how many factors would be needed for 10 variables.
We can proceed degree by degree and compute how many ordered ways we can add \(N\) non negative integers to produce the given degree \(D\) for each degree and \(N=10\).
This number is simply \( C^{N+D-1}_{D-1} \) where \(C_k^n \) denotes the &lt;a href=&#34;https://en.wikipedia.org/wiki/Binomial_coefficient&#34;&gt;binomial coefficient&lt;/a&gt;. Thus the total number of \(N\) variables is
&lt;div&gt;$$ 1+\sum_{i=1}^N C^{N+i-1}_{i-1} $$&lt;/div&gt;
where 1 stands for the constant coefficient (power of zero).&lt;/p&gt;

&lt;p&gt;For \(N=10\), the total number of factors is 184756.
Although, the total number of factors is large, it is much less than \(10^{10}\). The surprising fact of the workshop is that there were many very advanced mathematicians in the audience, specialists of BSDEs, and none made a comment to help the presenter.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shooting arbitrage - part II</title>
      <link>http://chasethedevil.github.io/post/shooting_arbitrage2/</link>
      <pubDate>Tue, 05 Jul 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/shooting_arbitrage2/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;(/post/shooting_arbitrage/&#34;&gt;previous post&lt;/a&gt;, I looked at de-arbitraging volatilities of options of a specific maturity with the shooting method.
In reality it is not so practical. While the local volatility will be continuous at the given expiry \(T\), it won&amp;rsquo;t be so at the times \( t \lt T \)
because of the interpolation or extrapolation in time. If we consider a single market expiry at time \(T\),
it is standard practice to extrapolate the implied volatility flatly for \(t \lt T\), that is, \(w(y,t) = v_T(y) t\)
 where the variance at time \(T\) is defined as \(v_T(y)= \frac{1}{T}w(y,T)\).
 Plugging this into the local variance formula leads to
&lt;div&gt;$$\sigma^{\star 2}\left(y, t\right) = \frac{ v_T(y)}{1 - \frac{y}{v_T}\frac{\partial v_T}{\partial y}  + \frac{1}{4}\left(-\frac{t^2}{4}-\frac{t}{v_T}+\frac{y^2}{v_T^2}\right)\left(\frac{\partial v_T}{\partial y}\right)^2
    + \frac{t}{2}\frac{\partial^{2} v_T}{\partial y^2}}$$&lt;/div&gt;
for \(t\leq T\). In particular, for \(t=0\), we have
&lt;div&gt;$$\sigma^{\star 2}\left(y, 0\right) = \frac{ v_T(y)}
{1 - \frac{y}{v_T}\frac{\partial v_T}{\partial y}   + \frac{1}{4}\left(\frac{y^2}{v_T^2}\right)\left(\frac{\partial v_T}{\partial y}\right)^2}$$&lt;/div&gt;
But the first derivative is not continuous, and jumps at \(y=y_0\) and \(y=y_1\). The local volatility will jump as well around those points. Thus, in practice, the technique can not be used for pricing under local volatility.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_lv_rk.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;jumping local volatility on Axel Vogt example fixed by shooting&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The local volatility stays very high in the original arbitrage region, very much like a capped local volatility would do. It should be then no surprise that in the equivalent implied volatility
is nearly the same as the one stemming from a capped local variance (we imply the smile from the prices of the local volatility PDE).

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_vol_rk_cap.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;implied volatility on Axel Vogt example fixed by shooting or by capping&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The discontinuity renders the shooting technique useless.
The penalized spline does not have this issue as the resulting local volatility is smooth from \(t=0\) to \(t=T\).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Shooting arbitrage - part I</title>
      <link>http://chasethedevil.github.io/post/shooting_arbitrage/</link>
      <pubDate>Wed, 22 Jun 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/shooting_arbitrage/</guid>
      <description>&lt;p&gt;In my &lt;a href=&#34;(/post/damghani_dearbitraging_a_weak_smile_on_svi/&#34;&gt;previous post&lt;/a&gt;, I looked at de-arbitraging volatilities of options of a specific maturity with SVI (re-)calibration.
The penalty method can be used beyond SVI. For example I interpolate here with a cubic spline on 11 equidistant nodes the original volatility slice that contains arbitrages and then minimize with Levenberg-Marquardt
and the negative local variance denominator penalty on 51 equidistant points. This results in a quite small adjustment to the original volatilities:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_variance_spline.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;implied variance with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Interestingly the denominator looks almost constant, close to zero (in reality it is not constant, just close to zero, scales can be misleading):

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_g_spline.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;local variance denominator g with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The method does not work with too many nodes, for example 25 nodes was too much for the minimizer to do anything, maybe because there is too much interaction between nodes then.&lt;/p&gt;

&lt;p&gt;I wondered then what would be the corresponding implied volatility for a constant denominator of 1E-4, glued to the implied volatility surface at the point where it reaches 1E-4.
&lt;div&gt;$$10^{-4}=1 - \frac{y}{w}\frac{\partial w}{\partial y} + \frac{1}{4}\left(-\frac{1}{4}-\frac{1}{w}+\frac{y^2}{w^2}\right)\left(\frac{\partial w}{\partial y}\right)^2  + \frac{1}{2}\frac{\partial^{2} w}{\partial y^2}$$&lt;/div&gt;
The equation can be easily solved with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods&#34;&gt;Runge Kutta method&lt;/a&gt; by
&lt;a href=&#34;http://www.engr.colostate.edu/~thompson/hPage/CourseMat/Tutorials/CompMethods/Rungekutta.pdf&#34;&gt;reducing it&lt;/a&gt; to a system of first order ordinary differential equations.
As the following figure will show, as the initial conditions are the variance and the slope at the glueing point, the volatility is not continuous anymore at the next point where the denominator goes back to 1E-4. So this is only good
if we replace the whole right wing: not so nice.&lt;/p&gt;

&lt;p&gt;A simple idea is to adjust the initial slope so that the volatility is continuous at the next end-point. An ODE whose initial condition consists in the function values at two end-points is called a two-points boundary problem. A standard method to solve
this kind of problem is just the basic simple idea and it is called the shooting method: we are shooting a projectile from point A so that it lands at point B. Any solver can be used so solve for the slope (secant, Newton, Brent, etc.).&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_variance_rk.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;implied variance with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The volatility is only continuous, not C1 or C2 at A and B, but the local volatility is well defined and continuous, the denominator is just 1E-4 between A and B. The adjustments to the original volatilities
is even smaller.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_g_rk.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;local variance denominator g with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dearbitraging a weak smile on SVI with Damghani&#39;s method</title>
      <link>http://chasethedevil.github.io/post/damghani_dearbitraging_a_weak_smile_on_svi/</link>
      <pubDate>Wed, 15 Jun 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/damghani_dearbitraging_a_weak_smile_on_svi/</guid>
      <description>&lt;p&gt;Yesterday, I wrote about some &lt;a href=&#34;http://chasethedevil.github.io/post/svi_zeliade_arbitrage/&#34;&gt;calendar spread arbitrages with SVI&lt;/a&gt;. Today I am looking at the famous example of butterfly spread arbitrage from Axel Vogt.
&lt;div&gt;$$(a, b, m, \rho, \sigma) = (−0.0410, 0.1331, 0.3586, 0.3060, 0.4153)$$&lt;/div&gt;
The parameters obey the weak no-arbitrage constraint of Gatheral, and yet produce a negative density, or equivalently, a negative denominator in the local variance Dupire formula.
Those parameters are mentioned in Jim Gatheral and Antoine Jacquier paper on &lt;a href=&#34;http://ssrn.com/abstract=2033323&#34;&gt;arbitrage free SVI volatility surfaces&lt;/a&gt; and also in Damghani&amp;rsquo;s paper &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2428532&#34;&gt;dearbitraging a weak smile&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Gatheral proposes to adjust just two of the parameters (corresponding to call wing and minimum variance) and run an optimizer with a large penalty on arbitrage to find the best-fit SVI parameters.
The arbitrage can be easily detected by just computing the local variance denominator, which is not much more costly than computing the implied variance (the analytical derivatives come almost for free):
&lt;div&gt;$$g(y)=1 - \frac{y}{w}\frac{\partial w}{\partial y}    + \frac{1}{4}\left(-\frac{1}{4}-\frac{1}{w}+\frac{y^2}{w^2}\right)\left(\frac{\partial w}{\partial y}\right)^2  + \frac{1}{2}\frac{\partial^{2} w}{\partial y^2}$$&lt;/div&gt;
where \(w\) is the total implied variance in log-moneyness \(y\) coming from the SVI formula.&lt;/p&gt;

&lt;p&gt;It is also possible include this penalty directly in the Nelder-Mead minimization of Zeliade&amp;rsquo;s quasi explicit calibration. In this case, all the parameters will be optimized. It is also not much slower than the more classic minimization without penalty.
While it results a much lower RMSE, the solution might not be as visually pleasing as Gatheral&amp;rsquo;s one.&lt;/p&gt;

&lt;p&gt;I was curious to see what Damghani&amp;rsquo;s technique would produce on this same example. Instead of doing the minimization with the full denominator evaluation, Damghani uses a simpler no-arbitrage necessary constraint
on the first derivative:
&lt;div&gt;$$|\frac{\partial w}{\partial y}|&lt;4 e $$&lt;/div&gt;
and iteratively search for \(e \in [0,1]\) so that no arbitrage remains in \(w\). The standard weak no-arbitrage constraint stands for \(e=1\). Dhamgani thus proposes to enforce a stronger &amp;ldquo;weak&amp;rdquo; constraint during the minimization, in order to de-arbitrage.
Note, the equations in the paper have a typo (everywhere), the \(b\) parameter should be a factor in front of the parenthesis.&lt;/p&gt;

&lt;p&gt;Now, it seems twisted to not enforce directly the real no arbitrage constraint in the minimization, which is actually not much more complex to evaluate, and to instead rely on some weak condition, that we make stronger by steps.
There would be no loop over the minimization needed. Does this produce any good result?

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_variance.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;implied variance with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The graph is clear, the technique produces crap. This is the local variance denominator:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_dearbitraging_g.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;local variance denominator g with Axel Vogt SVI parameters&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Maybe I misunderstood something big in this paper, but so far it was just a waste of time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arbitrage in Zeliade&#39;s SVI example</title>
      <link>http://chasethedevil.github.io/post/svi_zeliade_arbitrage/</link>
      <pubDate>Tue, 14 Jun 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/svi_zeliade_arbitrage/</guid>
      <description>&lt;p&gt;Zeliade wrote an &lt;a href=&#34;http://www.zeliade.com/whitepapers/zwp-0005.pdf&#34;&gt;excellent paper&lt;/a&gt; about the calibration of the SVI parameterization for the volatility surface in 2008. I just noticed recently
that their example calibration actually contained strong calendar spread arbitrages. This is not too surprising if you look at the parameters,
they vary wildly between the first and the second expiry.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;T&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;a&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;b&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;rho&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;m&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;s&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0.082&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.027&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.234&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.068&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.100&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.028&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0.16&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.030&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.125&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.074&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.050&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;0.26&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.032&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.094&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;-1.0&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.093&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;0.041&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The calendar spread arbitrage is very visible in total variance versus log-moneyness graph:
in those coordinates if lines crosses, there is an arbitrage. This is because the total variance should be increasing with the expiry time.

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_zeliade_arb.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Arbitrage in Zeliade&amp;#39;s example&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Why does this happen?&lt;/p&gt;

&lt;p&gt;This is typically because the range of moneyness of actual market quotes for the first expiry is quite narrow, and looks more like a smile than a skew. The problem is that
SVI is then quite bad at extrapolating this smile, likely because the SVI wings were used to fit well the curvature and have nothing to do with any actual market wings.&lt;/p&gt;

&lt;p&gt;A consequence is that that the local volatility will be undefined in the right wing of the second and third expiries, if we keep the first expiry.
It is interesting to look at what happens to the implied volatility if we decide either to:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;set the undefined local volatility to zero&lt;/li&gt;
&lt;li&gt;take the absolute value of the local variance&lt;/li&gt;
&lt;li&gt;search for the closest defined local volatility on the log-moneyness axis&lt;/li&gt;
&lt;li&gt;search for the closest positive local variance on the expiry axis and interpolate it linearly.&lt;/li&gt;
&lt;/ul&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_zeliade_arb_zero.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Total variance after flooring the local variance at zero.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Setting the undefined local volatility to zero makes the second, third, fourth expiry wings higher: the total variance is close to constant between expiries. One could have expected
that a zero vol would lead to a lower implied vol, the opposite happens, because the original local vol is negative, so flooring it zero is like increasing it.&lt;/p&gt;

&lt;p&gt;We can now deduce that taking the absolute value of the local variance is just going to push the implied variance even higher, and will therefore create a larger bias. Similarly, searching for closest local volatility
is not going to improve anything.&lt;/p&gt;

&lt;p&gt;Fixing the local volatility after the facts produces only a forward looking fix: the next expiries are going to be adjusted as a result.&lt;/p&gt;

&lt;p&gt;Instead, in this example, the first maturity should  be adjusted. A simple adjustment would be to cap the total variance of the first expiry so that it is never higher than the next expiry, before computing the local volatility.
Although the resulting implied volatility will not be C2 or even C1 at the cap, the local volatility can be computed analytically on the left side, before the cap, and also analytically after the cap, on the right side.
More care needs to be taken if the next expiry also needs to be capped (for example because there is another calendar spread arbitrage between expiry two and expiry three). In this case, the analytical calculation must be split in three zones: first-nocap + second-nocap, first-cap+second-nocap, first-cap+second-cap.
So in reality having non C2 extrapolation can work well with local volatility if we are careful enough to avoid the artificial spike at the points of discontinuity.&lt;/p&gt;

&lt;p&gt;There is yet another solution to produce a similar effect while still working at the local volatility level: if there is an arbitrage with the previous expiry at a given moneyness,
we compute the local volatility ignoring the previous expiry (eventually extrapolating in constant manner) and we override the previous expiry local volatility for this moneyness.
In terms of implied variance, this would correspond to removing the arbitrageable part of a given expiry, and replacing it with a linear interpolation between encompassing expiries, working backwards in time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dupire Local Volatility with Cash Dividends Part 2</title>
      <link>http://chasethedevil.github.io/post/dupire_cash_dividend_part2/</link>
      <pubDate>Sun, 29 May 2016 17:01:00 +0200</pubDate>
      
      <guid>http://chasethedevil.github.io/post/dupire_cash_dividend_part2/</guid>
      <description>&lt;p&gt;I had a look at how to price under Local Volatility with Cash dividends in &lt;a href=&#34;http://chasethedevil.github.io/post/dupire_cash_dividend/&#34;&gt;my previous post&lt;/a&gt;. I still had a somewhat large error in my FDM price. After too much time, I managed to find the culprit, it was the extrapolation of the prices when applying the jump continuity condition \(V(S,t_\alpha^-) = V(S-\alpha, t_\alpha^+) \) for an asset \(S\) with a cash dividend of amount \(\alpha\) at \( t_\alpha \).&lt;/p&gt;

&lt;p&gt;I stumbled in the meantime on another alternative to compute the Dupire local volatility with cash dividends &lt;a href=&#34;http://www.lorenzobergomi.com/#!blank/cnec&#34;&gt;in the book of Lorenzo Bergomi&lt;/a&gt;, one can rely on the
classic Gatheral formulation in terms of total implied variance \(w(y,T)\) function of log-moneyness:
&lt;div&gt;$$\sigma^{\star 2}\left(y, T\right) = \frac{ \frac{\partial w}{\partial T}}{1 - \frac{y}{w}\frac{\partial w}{\partial y}   + \frac{1}{4}\left(-\frac{1}{4}-\frac{1}{w}+\frac{y^2}{w^2}\right)\left(\frac{\partial w}{\partial y}\right)^2  + \frac{1}{2}\frac{\partial^{2} w}{\partial y^2}} \text{.}$$&lt;/div&gt;&lt;/p&gt;

&lt;p&gt;In this case the total implied variance corresponds to the Black volatility of the pure process (the process without dividend jumps), that is, the Black volatility corresponding to (shifted) market option prices. If the reference data consists of model volatilities for the spot model (with known jumps at dividend dates), the market prices can be obtained by using a good approximation of the spot model, for example &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;the improved Etore-Gobet expansion of this paper&lt;/a&gt;, not with the Black formula directly.&lt;/p&gt;

&lt;p&gt;In theory, it should be more robust to work directly with implied variances as there is not the problem of dealing with a very small numerator and denominator of the option prices equivalent formula. In practice, if we rely on one of the Etore-Gobet expansions, it can be much faster to work directly with option prices if we are careful when computing the ratio, as this ratio can be obtained in closed form. In theory as well, we need to use a fine discretisation in time to represent the pure Black equivalent smile accurately. In practice, if we are not too bothered by a mismatch with the true theoretical model, introducing volatility slices just before/at the dividends is enough to reproduce the market prices exactly, as long as we make sure that those obey the option price continuity at the dividend \(C(S_0,K,t_{\alpha}^-) = C(S_0, K-\alpha,t_{\alpha}^+)\). The difference is that the interpolation in time (linear in total variance) is going to describe a slightly different dynamic from the true spot process. The advantage, is that then, it is much faster.&lt;/p&gt;

&lt;p&gt;I thought it would be interesting to have a look at the various volatilities. My initial spot volatility is a simple smile constant in time:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3_modelvolraw.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Model volatility. Before,At = before the dividend, at the dividend. Log scale for strikes.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Actually it&amp;rsquo;s not all that constant since there is the need to introduce a shift at the dividend date to obey the option price continuity relationship. The pure process model volatility is however constant.

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3_modelvol.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Pure process model volatility.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The equivalent pure process Black vols looks like this:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3_blackvol.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Pure process Black volatility&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Notice how it does not jump at the dividend date, and how the cash dividend results in a lower Black volatility at maturity. The local volatility is:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3_localvol.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the spot model with jump at dividend date = 3.5&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;While the one of the pure process is:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3_purelv.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Pure Process Local Volatility under the spot model with jump at dividend date = 3.5&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;It falls towards zero for low pure strikes or equivalently near the market strike corresponding to the dividend amount.&lt;/p&gt;

&lt;p&gt;So far, this is not too far from the textbook theory. Now it happens that there is another subtlety related to the dividend policy. The dividend policy defines what happens when the spot price is lower than the cash dividend. Haug, Haug and Lewis defined two policies, liquidator and survivor as&lt;/p&gt;

&lt;p&gt;\(V(S,K,t_\alpha^-) = V(0,K, t_\alpha^+) \) for the liquidator - the stock drops to zero.
\(V(S,K,t_\alpha^-) = V(S,K, t_\alpha^+) \) for the survivor - the dividend is not paid.&lt;/p&gt;

&lt;p&gt;Not applying any particular policy would mean that the stock price can become negative:
\(V(S,K,t_\alpha^-) = V(S-\alpha,K, t_\alpha^+) \) also when \( S &amp;lt; \alpha \).&lt;/p&gt;

&lt;p&gt;Which one should we use? The approximation formulae (when not adjusted by the call price at strike 0) actually correspond to the &amp;ldquo;no dividend policy&amp;rdquo; rule. It can be verified numerically on extreme scenarios. It appears then natural that the finite difference scheme should also follow the same policy. And the volatility slice after the dividend date is really a constant size shift of the volatility slice just before.&lt;/p&gt;

&lt;p&gt;It becomes however more surprising if the model of reference is the liquidator (or the survivor) policy, that is, the market option prices are computed with a nearly exact numerical method according to the liquidator policy. In this case the volatility slice after the dividend date is not merely a constant size shift anymore, as the dividend policy will impact the option price continuity relationship. 
&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_liquidator_modelvol.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Liquidator model volatility. Before,At = before the dividend, at the dividend. Notice the difference in the BeforeShifted curve compared to the graph with no dividend policy.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

It turns out, that then, using the same policy in the FDM scheme at the dividend dates will actually create a bias, while discarding the dividend policy will make the method converge to the correct price. How can this be? My interpretation is that the Dupire local volatility already includes the dividend policy effect, and therefore it should not be taken into account once more in the finite difference scheme dividend jump condition. It is only a partial explanation, since imposing a liquidator policy via the jump condition seems to actually never work with Dupire (on non flat vols), even if the Dupire local volatility does not include the dividend policy effect.

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_liquidator_localvol.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the spot model with liquidator policy and jump at dividend date = 3.5&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Of course, this is not true when the volatility is constant until the option maturity (no smile, no Dupire). In this case, the policy must be enforced via the jump condition in the finite difference scheme.&lt;/p&gt;

&lt;p&gt;Note that this effect is not always simple to see, in my example, it is visible, especially on Put options of low strikes, because the volatility surface wings imply a high volatility when the strike is low and the option maturity is relatively long (5 years). For short maturities or lower volatilities, the dividend policy impact on the price is too small to be noticed.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dupire Local Volatility with Cash Dividends</title>
      <link>http://chasethedevil.github.io/post/dupire_cash_dividend/</link>
      <pubDate>Thu, 19 May 2016 17:01:00 +0200</pubDate>
      
      <guid>http://chasethedevil.github.io/post/dupire_cash_dividend/</guid>
      <description>&lt;p&gt;The Dupire equation for local volatility has been derived under the assumption of Martingality, that means no dividends or interest rates.
The extension to continuous dividend yield is described in many papers or books:
&lt;div&gt;$$\sigma^{\star 2}\left(K, T\right) = \frac{\frac{\partial C}{\partial T}+(r_B-q)K \frac{\partial C}{\partial K} + q C(K,T)}{\frac{1}{2}K^2\frac{\partial^2 C}{\partial K^2}}$$&lt;/div&gt;
In this case, \(C\) is a regular market Call option price, that can also be obtained from market volatilities via the Black-Scholes formula.&lt;/p&gt;

&lt;p&gt;With cash dividends however, the Black-Scholes formula is not valid anymore if we suppose that the asset jumps at the dividend date of the dividend amount. There are various relatively accurate
approximations available to price an option supposing a constant (spot) volatility and jumps, for example, &lt;a href=&#34;http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283&#34;&gt;this one&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Labordère, in his paper &lt;a href=&#34;http://papers.ssrn.com/sol3/Papers.cfm?abstract_id=1493306&#34;&gt;Calibration of local stochastic volatility models to market smiles&lt;/a&gt;, describes a mapping to obtain the market local volatility corresponding to the model with jumps, from the local volatility of a pure Martingale. Assuming no interest rates and no proportional dividend, the equations looks particularly simple, it can be simplified to:
&lt;div&gt;$$\sigma^{\star 2}\left(K, T\right) = \frac{\frac{\partial \hat{C}}{\partial T}}{\frac{1}{2}K^2\frac{\partial^2 \hat{C}}{\partial K^2}}$$&lt;/div&gt;
where \( \hat{C} \) is the market Call option price, that can be obtained via an approximation of the spot model with dividend jumps, of strike K and maturity T. Yes, if you look carefully at the Labordere formulae, most of it simplifies.&lt;/p&gt;

&lt;p&gt;While it appears very simple, in practice, it is not so much. For example, let&amp;rsquo;s consider a single maturity smile (a smile, constant in time) as the market reference spot vols. Which volatility should be used in the formula for \( \hat{C} \)? logically, it should be the volatility corresponding to the market option price of strike K and maturity T. The numerical derivatives will therefore make use of 4 distinct volatilities for K, K+dK, K-dK and T+dT. In the pricing formula,
we can wonder if at T+dT, the price should include the eventual additional dividend or not (as it is infinitesimal, probably not).&lt;/p&gt;

&lt;p&gt;It turns out that applying the above formula leads to jumps in time in the local volatility, around the dividend date, even though our initial market vols were flat in time.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the spot model with jump at dividend date = 3.5 on a single constant in time spot smile.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The mistake is not that clear. It turns out that, when using a single volatility slice, extrapolated in constant manner, the option continuity relationship around the dividend maturity is not respected. We must have
&lt;div&gt;$$C(S_0,K,t_{\alpha_i}^-) = C(S_0, K-\alpha_i,t_{\alpha_i}^+)$$&lt;/div&gt;
As the spot model with constant vol obeys this relationship, this implies that the slice immediately before the dividend must be a shifted representation of the slice immediately after in order for this relationship to stay true with a smile. If we add this slice, the error in repricing under local volatility with jumps becomes much smaller, although it is still
quite larger than the regular local volatility, even with a somewhat discontinuous yield.&lt;/p&gt;

&lt;p&gt;
&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the spot model with jump at dividend date = 3.5, introducing a slice before the dividend date to enforce the price continuity relationship.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_ll3shift.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Look, when we shift by the dividend!&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The slightly funny shape of the local volatility in the left wing, before the dividend, is due to linear extrapolation use.&lt;/p&gt;

&lt;p&gt;Interestingly, there is quite a difference in the local volatility for low strikes, depending on the dividend policy, here is how it looks for a single dividend with liquidator vs survivor policy.

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_policy.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the spot model with jump at dividend date = 3.5 on a single constant in time spot smile.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;The analytical approximations lead to another different local volatility from both policies, but don&amp;rsquo;t differ much from each other. Guyon and Labordere approximation based on the skew averaging technique of Piterbarg, not displayed here, is the worst. Gocsei-Sahel approximation has some issues for the lower strikes, the Etore-Gobet expansions on strike or on Lehman and the Zhang approximation are very stable and accurate, except for the very low strikes, where the dividend policy starts playing a more important role.
Those differences  however don&amp;rsquo;t impact the prices very much as the prices with the various methods (excepting the Guyon-Labordère approximation) are very close, and differ of a magnitude much smaller than the error to the true price.&lt;/p&gt;

&lt;p&gt;The continuous yield approach consist in first building the equivalent Black volatility and use the regular Dupire on it. This is qualitatively different from a cash dividend jump, and is closer to a proportional dividend jump. Note that there is still a jump in the continuous yield local volatility:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_yield3.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the forward model.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

The huge spike at 3.5 don&amp;rsquo;t allow us to see much about what&amp;rsquo;s happening around:

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/dupire_labordere_yield3e.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Dupire Local Volatility under the forward model.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;

&lt;p&gt;Overall, when the dividends happens early, the Dupire formula for cash dividends works well, but when the dividends are closer to maturity there is a marked bias in the prices, that does not disappear with more steps in the FDM. Typically, at \(0.7T\), the absolute price error is around 0.01 and more or less constant accross strikes. In contrast, the classic continuous yield Dupire behaves well, despite the spike, and the error decreases with the number of steps, I obtain around 5E-4 with 500 steps.&lt;/p&gt;

&lt;p&gt;I would have expected a much better accuracy from Labordère&amp;rsquo;s approach, it&amp;rsquo;s still not entirely clear to me if there is not an error lurking somewhere. This is how an apparently simple formula can become a nightmare to use in practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: A &lt;a href=&#34;http://chasethedevil.github.io/post/dupire_cash_dividend_part2&#34;&gt;follow-up&lt;/a&gt; to this post where I resolve the mysterious remaining error.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SVI, SABR, or parabola on AAPL?</title>
      <link>http://chasethedevil.github.io/post/svi_sabr_or_parabola/</link>
      <pubDate>Thu, 12 May 2016 19:32:42 +0200</pubDate>
      
      <guid>http://chasethedevil.github.io/post/svi_sabr_or_parabola/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;http://chasethedevil.github.io/post/least_squares_spline&#34;&gt;previous post&lt;/a&gt;, I took a look at least squares spline and parabola fits on AAPL 1m options market volatilities. I would have imagined SVI to fit even  better since it has 5 parameters, and SABR to do reasonably well.&lt;/p&gt;

&lt;p&gt;It turns out that the simple parabola has the lowest RMSE, and SVI is not really better than SABR on that example.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/svi_sabr_parabola.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;SVI, SABR, least squares parabola fitted to AAPL 1m options&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Note that this is just one single example, unlikely to be representative of anything, but I thought this was interesting that in practice, a simple parabola can compare favorably to more complex models.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adaptive Filon quadrature for stochastic volatility models</title>
      <link>http://chasethedevil.github.io/post/filon_for_heston/</link>
      <pubDate>Thu, 12 May 2016 19:08:18 +0200</pubDate>
      
      <guid>http://chasethedevil.github.io/post/filon_for_heston/</guid>
      <description>&lt;p&gt;A while ago, I have applied a relatively simple adaptive Filon quadrature to the problem of &lt;a href=&#34;http://papers.ssrn.com/abstract=2620166&#34;&gt;volatility swap pricing&lt;/a&gt;. The &lt;a href=&#34;https://www.google.fr/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=1&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwi18brlh9XMAhWIVhoKHdnGA2UQFggdMAA&amp;amp;url=http%3A%2F%2Fwww.ams.org%2Fmcom%2F1968-22-101%2FS0025-5718-1968-0225485-5%2FS0025-5718-1968-0225485-5.pdf&amp;amp;usg=AFQjCNEQvSMm6vOaXIX2MqAJ-GQt79QRiA&amp;amp;sig2=HLHd-rc74qnCuo5yp1Q13A&#34;&gt;Filon quadrature&lt;/a&gt; is an old quadrature from 1928 that allows to integrate oscillatory integrand like \(f(x)\cos(k x) \) or \(f(x)\sin(k x) \). It turns out that combined with an adaptive Simpson like method, it has many advantages over more generic adaptive quadrature methods like Gauss-Lobatto, which is often used on similar problems.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://dl.acm.org/citation.cfm?id=321029&#34;&gt;Flinn&lt;/a&gt; derived a similar quadrature, but taking into account the derivative values, which are likely to be easily available on those problems. This produces a even more robust quadrature and of higher convergence order.&lt;/p&gt;

&lt;p&gt;I was curious how practical this would be on Heston or other stochastic volatility models with a known characteristic function. It turns out it produces a very competitive performance-wise and very stable option pricing method: it can be up to &lt;strong&gt;five times faster&lt;/strong&gt; than an adaptive Gauss-Lobatto within a calibration. I found it faster and more robust than the Cos method (especially as it is quite tricky to guess properly the truncation of the Cos method).&lt;/p&gt;

&lt;p&gt;If you are interested, you will find much more details in &lt;a href=&#34;http://chasethedevil.github.io/lefloch_heston_filon.pdf&#34;&gt;this document&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Least Squares Rational Function</title>
      <link>http://chasethedevil.github.io/post/rational_fit/</link>
      <pubDate>Thu, 21 Apr 2016 16:37:24 +0200</pubDate>
      
      <guid>http://chasethedevil.github.io/post/rational_fit/</guid>
      <description>&lt;p&gt;In my paper &lt;a href=&#34;http://ssrn.com/abstract=2420757&#34;&gt;&amp;ldquo;Fast and Accurate Analytic Basis Point Volatility&amp;rdquo;&lt;/a&gt;,
I use a table of Chebyshev polynomials to provide an accurate representation of some function. This is
an idea I first saw in the &lt;a href=&#34;http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package&#34;&gt;Faddeeva package&lt;/a&gt; to
represent the cumulative normal distribution with high accuracy, and high performance. It is also
simple to find out the Chebyshev polynomials, and which intervals are the most appropriate for those, which
makes this technique quite appealing.&lt;/p&gt;

&lt;p&gt;Still, it would have been nice to have also a more visually appealing rational function representation, as &lt;a href=&#34;http://www.kcl.ac.uk/nms/depts/mathematics/research/finmath/publications/2007Shaw.pdf&#34;&gt;W. Shaw&lt;/a&gt;
did for the cumulative normal distribution (again). Popular algorithms to find the best rational function representation
seem to be minimax or Remez. But I could not find an open-source library where those were implemented. There is an
interesting implementation in &lt;a href=&#34;http://www.chebfun.org&#34;&gt;chebfun&lt;/a&gt; but this depends on Matlab.&lt;/p&gt;

&lt;p&gt;The Numerical recipe book provides a simple algorithm in &lt;a href=&#34;http://www.aip.de/groups/soe/local/numres/bookcpdf/c5-13.pdf&#34;&gt;chapter 5.13&lt;/a&gt;, not looking for the best possible rational function, but just for one
that would be &amp;ldquo;good enough&amp;rdquo;. Interestingly, the first part of the algorithm merely computes a least squares solution
on some chebyshev like nodes. I however quickly noticed funny behaviors with the code: it could produce a worse fit
for a higher order numerator or denominator. Then I tried some &lt;a href=&#34;http://www.scientificpython.net/pyblog/rational-least-squares-approximation&#34;&gt;least squares python code&lt;/a&gt; which ended up being just buggy:
I am not sure what the code actually does with all the numpy and scipy magic, it gives solutions with poles in the data, and clearly not the least squares solution. I can&amp;rsquo;t fully understand why one would propose such a code.&lt;/p&gt;

&lt;p&gt;It turns out that I had an alternative very basic least squares polynomial fit implementation, which is based on &lt;a href=&#34;http://math.stackexchange.com/questions/924482/least-squares-regression-matrix-for-rational-functions&#34;&gt;this matrix representation&lt;/a&gt;.
I wondered if it would be as prone to errors as Numerical recipe code (where they use SVD internally to solve).&lt;/p&gt;

&lt;p&gt;The answer is: it depends. It depends on the solver used. If I use a QR solver, then the implementation looks robust on my test data,
much more than Numerical recipe code. If I use LU, it just fails in some cases. If I use SVD, it is sometimes better sometimes worse than Numerical recipe.&lt;/p&gt;

&lt;p&gt;Interestingly, I thought, that, maybe, a &lt;a href=&#34;https://en.wikipedia.org/wiki/Gradient_descent&#34;&gt;gradient descent&lt;/a&gt; could allow to regain the lost accuracy with SVD, using
as starting point, the SVD guess. However, it does not work, it just converges more or less to the same (bad) solution.&lt;/p&gt;

&lt;p&gt;Another interesting point, is that the using QR decomposition (instead of SVD)
in the Numerical recipe implementation resulted in a much better solution, better even than my basic least squares fit,
which looked robust, but was actually not so much.&lt;/p&gt;

&lt;p&gt;Armed with this improved Numerical recipe code (labeled NRI), here is a comparison of fit of my naive code (with QR) against the improved numerical recipe
(NRI) for a polynomial of degree 20. We can visually see the difference (when we zoom)!&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/rational_fit_value.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Least squares polynomial fit of degree 20 zoomed.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The RMSE difference on the full interval [0,1] on 1000 equidistant points is huge:&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Method&lt;/th&gt;
&lt;th&gt;RMSE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Polynomial Naive&lt;/td&gt;
&lt;td&gt;0.234&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Polynomial NRI&lt;/td&gt;
&lt;td&gt;0.039&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Rational   NRI&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;A &lt;sup&gt;10&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt; rational function (numerator of degree 10, denominator of degree 10) gives a much better fit than a polynomial of degree 20. It is interesting to look at the error
visually to understand how large is the difference:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/rational_fit_error.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Least squares polynomial fit error of degree 20.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;I still can draw a conclusion to my quest for a rational function approximation: I won&amp;rsquo;t find a good one with the
change of variables I am using in my paper, as I would imagine the least squares solution to be at worst around one order of magnitudes
 away from the minimax. The errors I got suggest I would need a few rational functions, maybe 3 or more, and then it does not look
all that appealing compared to the table of Chebyshev polynomials.&lt;/p&gt;

&lt;p&gt;I thought this was a good example of how relatively simple numerical tasks can be challenging in practice.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt; - The paper now contains a solution for the normal volatility inversion problem with only 3 rational functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Least Squares Spline for Volatility Interpolation</title>
      <link>http://chasethedevil.github.io/post/least_squares_spline/</link>
      <pubDate>Fri, 19 Feb 2016 18:29:33 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/least_squares_spline/</guid>
      <description>&lt;p&gt;I am experimenting a bit with least squares splines. Existing algorithms (for example from the NSWC Fortran library) usually work
with B-splines, a relatively simple explanation of how it works is given in &lt;a href=&#34;http://www.geometrictools.com/Documentation/BSplineCurveLeastSquaresFit.pdf&#34;&gt;this paper&lt;/a&gt; (I think this is how De Boor coded it in the NSWC library).
Interestingly there is &lt;a href=&#34;http://educ.jmu.edu/~lucassk/Papers/Spline3.pdf&#34;&gt;an equivalent formulation in terms of standard cubic splines&lt;/a&gt;, although it seems that the
pseudo code on that paper has errors.&lt;/p&gt;

&lt;p&gt;Least squares splines give a very good fit for option implied volatilities with only a few parameters.
In theory, the number of parameters is N+2 where N is the number of interpolation points.
I tried on some of my AAPL 1 month option chain, with only 3 points (so 2 splines or 5 free parameters).&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/least_squares_spline.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;least squares spline on 1m AAPL options.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;It would be interesting to add the natural constraints so that it can be linearly extrapolated. Maybe for next time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Mystic Parabola</title>
      <link>http://chasethedevil.github.io/post/mystic_parabola/</link>
      <pubDate>Tue, 16 Feb 2016 22:13:53 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/mystic_parabola/</guid>
      <description>&lt;p&gt;I recently had some fun trying to work directly with the option chain from the &lt;a href=&#34;http://www.nasdaq.com/symbol/aapl/option-chain&#34;&gt;Nasdaq website&lt;/a&gt;.
The data there is quite noisy, but a simple parabola can still give an amazing fit. I will consider the options of maturity two years as illustration.
I also relied on a simple implied volatility algorithm that can be summarized in the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute a rough guess for the forward price by using interest, borrow curves and by extrapolating the dividends.&lt;/li&gt;
&lt;li&gt;Imply the forward from the European Put-Call parity relationship on the mid prices of the two strikes closes to the rough forward guess. A simple linear interpolation between the two strikes can be used to compute the forward.&lt;/li&gt;
&lt;li&gt;Compute the Black implied volatilities as if the option were European using P. Jaeckel algorithm.&lt;/li&gt;
&lt;li&gt;Calibrate the proportional dividend amount or the growth rate by minimizing, for example with a Levenberg-Marquardt minimizer, the difference between model and mid-option prices corresponding to the three strikes closest to the forward. The parameters in this case are effectively the dividend amount and the volatilities for Put and Call options (the same volatility is used for both options). The initial guess stems directly from the two previous steps. American option prices are computed by the finite difference method.&lt;/li&gt;
&lt;li&gt;Solve numerically the volatilities one by one with the TOMS748 algorithm so that the model prices match the market mid out-of-the-money option prices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then I just fit a least squares parabola in variance on log-moneyness, using options trading volumes as weights and obtain the following figure:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/mystic_parabola.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;least squares parabola on 2y AAPL options.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Isn&amp;rsquo;t the fit just amazing?
Even if I found it surprising, it&amp;rsquo;s probably not so surprising. The curve has to be smooth, somewhat monotone, and will be therefore like a parabola near the money. While there is no guarantee it will fit that well far away, it&amp;rsquo;s actually a matter of scale. Short maturities will lead to not so great fit in the wings, while long maturities will correspond to a narrower range of scaled strikes and match better a parabola.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yahoo Finance Implied Volatility</title>
      <link>http://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</link>
      <pubDate>Wed, 03 Feb 2016 16:45:58 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://finance.yahoo.com/q/op?s=GOOG&amp;amp;date=1457049600&#34;&gt;option chain&lt;/a&gt; on Yahoo finance shows an implied volatility number for each call or put option in the last column.
I was wondering a bit how they computed that number. I did not exactly find out their methodology, especially since we don&amp;rsquo;t even know the daycount convention used, but
I did find that it was likely just garbage.&lt;/p&gt;

&lt;p&gt;A red-herring is for example the large discrepancy between put vols and call vols. For example strike 670, call vol=50%, put vol=32%.
This suggests that the two are completely decoupled, and they use some wrong forward (spot price?) to obtain those numbers. If I compute
the implied volatilities using put-call parity close to the money to find out the implied forward price, I end up with ask vols of 37% and 34% or call and put mid vols of 33%.
By considering the put-call parity, I assume European option prices, which is not correct in this case. It turns out however, that with the low interest rates we live in, there is nearly zero additional value due to the American early exercise.&lt;/p&gt;

&lt;p&gt;I am not sure what use people can have of Yahoo implied volatilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Tufte overrated?</title>
      <link>http://chasethedevil.github.io/post/is_tufte_overrated/</link>
      <pubDate>Wed, 03 Feb 2016 16:11:30 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/is_tufte_overrated/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.edwardtufte.com/tufte/&#34;&gt;Tufte&lt;/a&gt; proposes interesting guidelines to present data, or even to design written semi-scientific papers or books. Some advices
are particularly relevant like the careful use of colors (don&amp;rsquo;t use all the colors of the rainbow just because you can), and
in general don&amp;rsquo;t add lines in a graph or designs that are not directly relevant to the message that needs to be conveyed. There is also a parallel
with Feynman message against (Nasa) &lt;a href=&#34;http://www.zdnet.com/article/death-by-powerpoint/&#34;&gt;Powerpoint presentations&lt;/a&gt;. But other inspirations, are somewhat doubtful.
He seems to have a fetish for &lt;a href=&#34;http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000hB&#34;&gt;old texts&lt;/a&gt;. They might be considered pretty, or interesting in some ways, but
I don&amp;rsquo;t find them particularly easy to read. They look more like esoteric books rather than practical books. If you want to write
the new Bible for your new cult, it&amp;rsquo;s probably great, not so sure it&amp;rsquo;s so great for a more simple subject.
Also somewhat surprisingly, his own website is not very well designed, it looks like a maze and very end of 90s.&lt;/p&gt;

&lt;p&gt;I experimented a bit with the &lt;a href=&#34;https://tufte-latex.github.io/tufte-latex/&#34;&gt;Tufte latex template&lt;/a&gt;. It produced &lt;a href=&#34;http://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID2712316_code1514784.pdf?abstractid=2711720&amp;amp;mirid=1&#34;&gt;that document&lt;/a&gt; for example. But someone rightfully pointed out to me
that the reference style was not really elegant, that it did not look like your typical nice science paper references. Furthermore,
 using superscript so much could be annoying for someone used to read math and consider superscript numbers as math symbols.
In general, there seems to be a conflict between the use of Latex and many Tufte guidelines:
Latex does not encourage you to lay out one by one each piece,
something the good old desktop publishing software allow you to do quite well.&lt;/p&gt;

&lt;p&gt;I was also wondering a bit on what design to use for a book. And I realised that the best layout to consider is simply the layout
of a book I enjoyed to read. For example, I like the recent SIAM book design, I find that it gives enough space to read the text
and the maths without having the impression of deciphering some codex, and without headache. It turns out there is even a &lt;a href=&#34;http://www.siam.org/books/authors/p_handbook8.php&#34;&gt;latex template&lt;/a&gt; available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear and Flat forward interpolation with cash dividends</title>
      <link>http://chasethedevil.github.io/post/linear_flat_forward_interpolation/</link>
      <pubDate>Tue, 19 Jan 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/linear_flat_forward_interpolation/</guid>
      <description>&lt;p&gt;When the dividend curve is built from discrete cash dividends, the dividend yield is discontinuous at the dividend time as the asset price jumps from the dividend amount.
This can be particularly problematic for numerical schemes like finite difference methods. In deed, a finite difference grid
will make use of the forward yield (eventually adjusted to the discretisation scheme), which explodes then.
Typically, if one is not careful about this, then increasing the number of time steps does not increase accuracy anymore, as
the spike just becomes bigger on a smaller time interval. A simple work-around is to limit the resolution to one day.
This means that intraday, we interpolate the dividend yield.&lt;/p&gt;

&lt;p&gt;If we simply interpolate the yields linearly intraday, then the yield becomes continuous again, and numerical schemes will work much better.
But if we take a look at the actual curve of &amp;ldquo;forward&amp;rdquo; yields, it becomes sawtooth shaped!

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/linear_flat_forward.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;effective forward drift used in the finite difference grid with 4 time-steps per day&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

On the above figure, we can see the Dirac like forward yield if we work with the direct formulas, while interpolating intraday allows to smooth out the initial Dirac overall the interval corresponding to 1-day.&lt;/p&gt;

&lt;p&gt;In reality, one should use flat forward interpolation instead, where the forward yield is maintained constant during the day. The forward rate is defined as
&lt;div&gt;$$f(t_0,t_1)= \frac{r(t_1) t_1 -r(t_0) t_0}{t_1-t_0}$$&lt;/div&gt;
where the continuously compounded rate \(r\) is defined so that \(Z(0,t)= e^{-r(t)t}\).
In the case of the Black-Scholes drift, the drift rate is defined so that the forward price (not to confuse with the forward rate) \(F(0,t)= e^{-q(t)t}\).&lt;/p&gt;

&lt;p&gt;The flat forward interpolation is equivalent to a linear interpolation on the logarithm of discount factors.
In ACT/365, let \(t_0=\max\left(0,\frac{365}{\left\lceil 365 t \right\rceil-1}\right), t_1 = \frac{365}{\left\lceil 365 t \right\rceil}\), the interpolated yield is:
&lt;div&gt;$$\bar{q}(0,t)t = q(t_0)t_0\frac{t_1-t}{t_1-t_0} + q(t_1)t_1\frac{t-t_0}{t_1-t_0}\text{.}$$&lt;/div&gt;
Another work-around would be to model this via proportional dividends instead of a &amp;ldquo;continuous&amp;rdquo; yield curve.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>