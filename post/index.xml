<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Chase the Devil</title>
    <link>http://chasethedevil.github.io/post/</link>
    <description>Recent content in Posts on Chase the Devil</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Feb 2016 22:13:53 +0100</lastBuildDate>
    <atom:link href="http://chasethedevil.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Mystic Parabola</title>
      <link>http://chasethedevil.github.io/post/mystic_parabola/</link>
      <pubDate>Tue, 16 Feb 2016 22:13:53 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/mystic_parabola/</guid>
      <description>&lt;p&gt;I recently had some fun trying to work directly with the option chain from the &lt;a href=&#34;http://www.nasdaq.com/symbol/aapl/option-chain&#34;&gt;Nasdaq website&lt;/a&gt;.
The data there is quite noisy, but a simple parabola can still give an amazing fit. I will consider the options of maturity two years as illustration.
I also relied on a simple implied volatility algorithm that can be summarized in the following steps:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Compute a rough guess for the forward price by using interest, borrow curves and by extrapolating the dividends.&lt;/li&gt;
&lt;li&gt;Imply the forward from the European Put-Call parity relationship on the mid prices of the two strikes closes to the rough forward guess. A simple linear interpolation between the two strikes can be used to compute the forward.&lt;/li&gt;
&lt;li&gt;Compute the Black implied volatilities as if the option were European using P. Jaeckel algorithm.&lt;/li&gt;
&lt;li&gt;Calibrate the proportional dividend amount or the growth rate by minimizing, for example with a Levenberg-Marquardt minimizer, the difference between model and mid-option prices corresponding to the three strikes closest to the forward. The parameters in this case are effectively the dividend amount and the volatilities for Put and Call options (the same volatility is used for both options). The initial guess stems directly from the two previous steps. American option prices are computed by the finite difference method.&lt;/li&gt;
&lt;li&gt;Solve numerically the volatilities one by one with the TOMS748 algorithm so that the model prices match the market mid out-of-the-money option prices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Then I just fit a least squares parabola in variance on log-moneyness, using options trading volumes as weights and obtain the following figure:&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/mystic_parabola.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;least squares parabola on 2y AAPL options.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Isn&amp;rsquo;t the fit just amazing?
Even if I found it surprising, it&amp;rsquo;s probably not so surprising. The curve has to be smooth, somewhat monotone, and will be therefore like a parabola near the money. While there is no guarantee it will fit that well far away, it&amp;rsquo;s actually a matter of scale. Short maturities will lead to not so great fit in the wings, while long maturities will correspond to a narrower range of scaled strikes and match better a parabola.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Yahoo Finance Implied Volatility</title>
      <link>http://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</link>
      <pubDate>Wed, 03 Feb 2016 16:45:58 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/yahoo_finance_implied_volatility/</guid>
      <description>&lt;p&gt;The &lt;a href=&#34;https://finance.yahoo.com/q/op?s=GOOG&amp;amp;date=1457049600&#34;&gt;option chain&lt;/a&gt; on Yahoo finance shows an implied volatility number for each call or put option in the last column.
I was wondering a bit how they computed that number. I did not exactly find out their methodology, especially since we don&amp;rsquo;t even know the daycount convention used, but
I did find that it was likely just garbage.&lt;/p&gt;

&lt;p&gt;A red-herring is for example the large discrepancy between put vols and call vols. For example strike 670, call vol=50%, put vol=32%.
This suggests that the two are completely decoupled, and they use some wrong forward (spot price?) to obtain those numbers. If I compute
the implied volatilities using put-call parity close to the money to find out the implied forward price, I end up with ask vols of 37% and 34% or call and put mid vols of 33%.
By considering the put-call parity, I assume European option prices, which is not correct in this case. It turns out however, that with the low interest rates we live in, there is nearly zero additional value due to the American early exercise.&lt;/p&gt;

&lt;p&gt;I am not sure what use people can have of Yahoo implied volatilities.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Is Tufte overrated?</title>
      <link>http://chasethedevil.github.io/post/is_tufte_overrated/</link>
      <pubDate>Wed, 03 Feb 2016 16:11:30 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/is_tufte_overrated/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://www.edwardtufte.com/tufte/&#34;&gt;Tufte&lt;/a&gt; proposes interesting guidelines to present data, or even to design written semi-scientific papers or books. Some advices
are particularly relevant like the careful use of colors (don&amp;rsquo;t use all the colors of the rainbow just because you can), and
in general don&amp;rsquo;t add lines in a graph or designs that are not directly relevant to the message that needs to be conveyed. There is also a parallel
with Feynman message against (Nasa) &lt;a href=&#34;http://www.zdnet.com/article/death-by-powerpoint/&#34;&gt;Powerpoint presentations&lt;/a&gt;. But other inspirations, are somewhat doubtful.
He seems to have a fetish for &lt;a href=&#34;http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000hB&#34;&gt;old texts&lt;/a&gt;. They might be considered pretty, or interesting in some ways, but
I don&amp;rsquo;t find them particularly easy to read. They look more like esoteric books rather than practical books. If you want to write
the new Bible for your new cult, it&amp;rsquo;s probably great, not so sure it&amp;rsquo;s so great for a more simple subject.
Also somewhat surprisingly, his own website is not very well designed, it looks like a maze and very end of 90s.&lt;/p&gt;

&lt;p&gt;I experimented a bit with the &lt;a href=&#34;https://tufte-latex.github.io/tufte-latex/&#34;&gt;Tufte latex template&lt;/a&gt;. It produced &lt;a href=&#34;http://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID2712316_code1514784.pdf?abstractid=2711720&amp;amp;mirid=1&#34;&gt;that document&lt;/a&gt; for example. But someone rightfully pointed out to me
that the reference style was not really elegant, that it did not look like your typical nice science paper references. Furthermore,
 using superscript so much could be annoying for someone used to read math and consider superscript numbers as math symbols.
In general, there seems to be a conflict between the use of Latex and many Tufte guidelines:
Latex does not encourage you to lay out one by one each piece,
something the good old desktop publishing software allow you to do quite well.&lt;/p&gt;

&lt;p&gt;I was also wondering a bit on what design to use for a book. And I realised that the best layout to consider is simply the layout
of a book I enjoyed to read. For example, I like the recent SIAM book design, I find that it gives enough space to read the text
and the maths without having the impression of deciphering some codex, and without headache. It turns out there is even a &lt;a href=&#34;http://www.siam.org/books/authors/p_handbook8.php&#34;&gt;latex template&lt;/a&gt; available.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear and Flat forward interpolation with cash dividends</title>
      <link>http://chasethedevil.github.io/post/linear_flat_forward_interpolation/</link>
      <pubDate>Tue, 19 Jan 2016 09:55:32 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/linear_flat_forward_interpolation/</guid>
      <description>&lt;p&gt;When the dividend curve is built from discrete cash dividends, the dividend yield is discontinuous at the dividend time as the asset price jumps from the dividend amount.
This can be particularly problematic for numerical schemes like finite difference methods. In deed, a finite difference grid
will make use of the forward yield (eventually adjusted to the discretisation scheme), which explodes then.
Typically, if one is not careful about this, then increasing the number of time steps does not increase accuracy anymore, as
the spike just becomes bigger on a smaller time interval. A simple work-around is to limit the resolution to one day.
This means that intraday, we interpolate the dividend yield.&lt;/p&gt;

&lt;p&gt;If we simply interpolate the yields linearly intraday, then the yield becomes continuous again, and numerical schemes will work much better.
But if we take a look at the actual curve of &amp;ldquo;forward&amp;rdquo; yields, it becomes sawtooth shaped!

&lt;figure &gt;
    
        &lt;img src=&#34;http://chasethedevil.github.io/post/linear_flat_forward.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;effective forward drift used in the finite difference grid with 4 time-steps per day&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

On the above figure, we can see the Dirac like forward yield if we work with the direct formulas, while interpolating intraday allows to smooth out the initial Dirac overall the interval corresponding to 1-day.&lt;/p&gt;

&lt;p&gt;In reality, one should use flat forward interpolation instead, where the forward yield is maintained constant during the day. The forward rate is defined as
&lt;div&gt;$$f(t_0,t_1)= \frac{r(t_1) t_1 -r(t_0) t_0}{t_1-t_0}$$&lt;/div&gt;
where the continuously compounded rate \(r\) is defined so that \(Z(0,t)= e^{-r(t)t}\).
In the case of the Black-Scholes drift, the drift rate is defined so that the forward price (not to confuse with the forward rate) \(F(0,t)= e^{-q(t)t}\).&lt;/p&gt;

&lt;p&gt;The flat forward interpolation is equivalent to a linear interpolation on the logarithm of discount factors.
In ACT/365, let \(t_0=\max\left(0,\frac{365}{\left\lceil 365 t \right\rceil-1}\right), t_1 = \frac{365}{\left\lceil 365 t \right\rceil}\), the interpolated yield is:
&lt;div&gt;$$\bar{q}(0,t)t = q(t_0)t_0\frac{t_1-t}{t_1-t_0} + q(t_1)t_1\frac{t-t_0}{t_1-t_0}\text{.}$$&lt;/div&gt;
Another work-around would be to model this via proportional dividends instead of a &amp;ldquo;continuous&amp;rdquo; yield curve.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Moved to hugo</title>
      <link>http://chasethedevil.github.io/post/moved-to-hugo/</link>
      <pubDate>Sun, 20 Dec 2015 21:00:57 +0100</pubDate>
      
      <guid>http://chasethedevil.github.io/post/moved-to-hugo/</guid>
      <description>&lt;p&gt;I moved my blog from blogger to &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt;. Blogger really did not evolve since Google take-over in 2003. Wordpress is today much nicer and prettier. It&amp;rsquo;s clear that Google did not invest at all, possibly because blogs are passé. Compared to mid 2000, there are very few blogs today. Even programming blogs are scarce. It could be interesting to quantify this. My theory is that it is the direct consequence of the popularity of social networks, and especially facebook (possibly also stackoverflow for programmers): people don&amp;rsquo;t have time anymore to write as their extra-time is used on social networks. Similarly I noticed that almost nobody comments anymore to the point that even Disqus is very rarely used, and again I attribute that to the popularity of sites like reddit. This is why I did not bother with a comment section on my blog, just email me or tweet about it instead.&lt;/p&gt;

&lt;p&gt;I was always attracted by the static web sites concept, because there is actually very little things that ought to be truely dynamic from a individual point of view. Dynamic hosting also tends to be problematic in the long-run, for example I never found the time to upgrade my chord search engine to the newer Google appengine and now it&amp;rsquo;s just off. I used to freeze my personal website (created with a dynamic templating tool Velocity, django, etc.) with a python script. So a static blog was the next logical step, and these days, it&amp;rsquo;s quite popular. Static blogs put the author fully in control of the content and its presentation. &lt;a href=&#34;http://jekyllrb.com&#34;&gt;Jekyll&lt;/a&gt; started the trend along with github allowing good old personal websites. It offers a modern looking blog, with very little configuration steps. I tried Hugo instead because it&amp;rsquo;s written in &lt;a href=&#34;http://golang.com&#34;&gt;the Go language&lt;/a&gt;. It&amp;rsquo;s much faster, but I don&amp;rsquo;t really care about that for something of the size of my blog. I was curious however how good was the Go language on real world projects, and I knew I could always customize it if I ever needed to. Interestingly, I did stumble on a few panics (core dump equivalent where the program just crashes, in this case the hugo local server), something that does not happen with Java based tools or even with Ruby or Python based tools. Even though I like the Go language more and more (I am doing some pet project with it - I believe in the focus on fast compilation and simple language), I found this a bit alarming. This is clearly a result of the errors versus exceptions choice, as it&amp;rsquo;s up to the programmer to handle the errors properly and not panic unnecessarily (I even wonder if it makes any sense to panic for a server).&lt;/p&gt;

&lt;p&gt;Anyway I think it looks better now, maybe a bit too minimalist. I&amp;rsquo;ll add details when I have more time.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Controlling the SABR wings with Hagan PDE </title>
      <link>http://chasethedevil.github.io/post/controlling-the-sabr-wings-with-hagan-pde/</link>
      <pubDate>Tue, 15 Dec 2015 10:56:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/controlling-the-sabr-wings-with-hagan-pde/</guid>
      <description>&lt;p&gt;On the &lt;a href=&#34;http://www.wilmott.com/messageview.cfm?catid=4&amp;amp;threadid=78001&amp;amp;FTVAR_MSGDBTABLE=&amp;amp;STARTPAGE=4&#34;&gt;Wilmott forum&lt;/a&gt;, Pat Hagan has recently suggested to cap the equivalent local volatility in order to control the wings and better match CMS prices. It also helps making the SABR approximation better behaved as the expansion is only valid when&lt;br /&gt;&lt;div&gt;$$ 1 + 2\frac{\rho\nu}{\alpha}y(K)+\frac{\nu^2}{\alpha^2}y^2(K) $$&lt;/div&gt;&lt;div&gt;is close to 1.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;div&gt;In the PDE approach (especially the non transformed one), it is very simple, one just needs to update the equivalent local vol as&amp;nbsp;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;$$\alpha K^\beta \min\left(M, \sqrt{1 + 2\frac{\rho\nu}{\alpha}y(K)+\frac{\nu^2}{\alpha^2}y^2(K)}\right)$$&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;While it is straightforward to include in the PDE, it is more difficult to derive a good approximation. The zero-th order behaves as expected, but the first order formula has a unnatural kink, likely because of the non differentiability due to the min function.&amp;nbsp;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The following graphs presents the non capped PDE, the capped PDE with M=4*nu (PDEC4) and M=6*nu (PDEC6) as well as the approximation (Andersen Ratcliffe / Gatheral first order) where I have only taken care of the right wing. The SABR parameters are alpha = 0.0630, beta = 0.7, rho = -0.363, nu = 0.421, T = 10, f = 0.0439.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-7N928DyGhHY/Vm_ibyWlVcI/AAAAAAAAIP0/YF7Mfcpm4w4/s1600/Screenshot%2Bfrom%2B2015-12-15%2B10-25-16.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;400&#34; src=&#34;http://1.bp.blogspot.com/-7N928DyGhHY/Vm_ibyWlVcI/AAAAAAAAIP0/YF7Mfcpm4w4/s400/Screenshot%2Bfrom%2B2015-12-15%2B10-25-16.png&#34; width=&#34;385&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We can see that the higher the cap is, the closer we are to the standard SABR PDE, and the lower the cap is, the flatter are the wings.&lt;br /&gt;&lt;br /&gt;The approximation matches well ATM (it is then equivalent to standard SABR PDE) but then has a discontinuous derivative for the K that reaches the threshold M. Far away, it matches very well again.&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Broken Internet?</title>
      <link>http://chasethedevil.github.io/post/broken-internet/</link>
      <pubDate>Mon, 09 Nov 2015 13:40:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/broken-internet/</guid>
      <description>&lt;p&gt;There is something funny going on with upcoming generic top level domains (gTLDs), they seem to be looked up in a strange manner (at least on latest Linux). For example:&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ping chrome&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;or&lt;br /&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ping nexus&amp;nbsp;&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;returns 127.0.53.53.&lt;br /&gt;&lt;br /&gt;While existing &lt;a href=&#34;https://www.name.com/new-gtld&#34;&gt;official gTLD&lt;/a&gt;s don&amp;rsquo;t (&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;ping dental&lt;/span&gt; returns &amp;ldquo;unknown host&amp;rdquo; as expected). I first thought it was a network misconfiguration, but as &lt;a href=&#34;https://groups.google.com/forum/#!msg/public-dns-discuss/bzhTQnFqE6I/E9F46xhka98J&#34;&gt;I am not the only one to notice this&lt;/a&gt;, it&amp;rsquo;s likely a genuine internet issue.&lt;br /&gt;&lt;br /&gt;Strange times.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Holiday&#39;s read - DFW - Everything and more</title>
      <link>http://chasethedevil.github.io/post/holidays-read---dfw---everything-and-more/</link>
      <pubDate>Sun, 01 Nov 2015 17:55:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/holidays-read---dfw---everything-and-more/</guid>
      <description>&lt;p&gt;I am ambivalent towards David Foster Wallace. He can write the most creative sentences and make innocuous subjects very interesting. At the same time, i never finished his book &lt;a href=&#34;https://en.wikipedia.org/wiki/Infinite_Jest&#34;&gt;Infinite Jest&lt;/a&gt;, partly because the characters names are too awkward for me so that i never exactly remember who is who, but also because the story itself is a bit too crazy.&lt;br /&gt;I knew however that &lt;a href=&#34;http://www.amazon.com/Everything-More-Compact-History-Infinity/dp/0393339289&#34;&gt;a non fiction book on the subject of infinity&lt;/a&gt; written by him would make for a very interesting read. And I have not been disappointed. It&amp;rsquo;s in between maths and philosophy going back to the Greeks up to Gödel through a lot of Cantor following more or less the historical chronology.&lt;br /&gt;Most of it is easy to read and follow, except the last part around sets and transfinite numbers. This last part is actually quite significant as it tries to explain why we still have no satisfying theory around the problems raised by infinity especially in the context of a Sets theory.&amp;nbsp;I did not expect to learn much around the subject, I was disappointed. The book showed me how naive I was and how tricky the concept of infinity can be.&lt;br /&gt;While I found the different explanations around &lt;a href=&#34;https://en.wikipedia.org/wiki/Zeno%27s_paradoxes&#34;&gt;Zeno&amp;rsquo;s paradox of the arrow&lt;/a&gt; very clever, there is one other view possible: the arrow really does not move at each instant (you could think of those as a snapshot) but an interval of time is just not a simple succession of instants. This is not so far of Aristotle attack, but the key here is around what is an interval really. DFW suggests slightly this interpretation as well p144 but it&amp;rsquo;s not very explicit.&lt;br /&gt;I had not heard about Kronecker&amp;rsquo;s conception that only integers were mathematically real (against decimals, irrationals, infinite sets). I find it very appropriate in the frame of computer science. Everything ends up as finite integers (a binary representation) and we are always confronted to the process of transforming the continuous, that despite all its conceptual issues is often simpler to reason in to solve concrete problems, to the finite discrete.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Crank-Nicolson and Rannacher Issues with Touch options</title>
      <link>http://chasethedevil.github.io/post/crank-nicolson-and-rannacher-issues-with-touch-options/</link>
      <pubDate>Wed, 30 Sep 2015 13:34:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/crank-nicolson-and-rannacher-issues-with-touch-options/</guid>
      <description>&lt;p&gt;I just stumbled upon this particularly illustrative case where the Crank-Nicolson finite difference scheme behaves badly, and the Rannacher smoothing (2-steps backward Euler) is less than ideal: &lt;a href=&#34;http://www.investopedia.com/terms/d/doubleonetouch.asp&#34;&gt;double one touch&lt;/a&gt; and &lt;a href=&#34;http://www.investopedia.com/terms/d/doublenotouch.asp&#34;&gt;double no touch&lt;/a&gt; options.&lt;br /&gt;&lt;br /&gt;It is particularly evident when the option is sure to be hit, for example when the barriers are narrow, that is our delta should be around zero as well as our gamma. Let&amp;rsquo;s consider a double one touch option with spot=100, upBarrier=101, downBarrier=99.9, vol=20%, T=1 month and a payout of 50K.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-skVEtpSetds/VgvDcK5MycI/AAAAAAAAIIc/BPj70_3z4lo/s1600/Screenshot%2Bfrom%2B2015-09-30%2B13%253A11%253A13.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;326&#34; src=&#34;http://3.bp.blogspot.com/-skVEtpSetds/VgvDcK5MycI/AAAAAAAAIIc/BPj70_3z4lo/s400/Screenshot%2Bfrom%2B2015-09-30%2B13%253A11%253A13.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Crank-Nicolson shows big spikes in the delta near the boundary&lt;/td&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;&lt;br /&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-w0esoFRdaSA/VgvD6QgMDwI/AAAAAAAAIIk/-qdQ6BcTAmU/s1600/Screenshot%2Bfrom%2B2015-09-30%2B13%253A13%253A33.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;325&#34; src=&#34;http://3.bp.blogspot.com/-w0esoFRdaSA/VgvD6QgMDwI/AAAAAAAAIIk/-qdQ6BcTAmU/s400/Screenshot%2Bfrom%2B2015-09-30%2B13%253A13%253A33.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Rannacher shows spikes in the delta as well&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Crank-Nicolson spikes are so high that the price is actually a off itself.&lt;br /&gt;&lt;br /&gt;The Rannacher smoothing reduces the spikes by 100x but it&amp;rsquo;s still quite high, and would be higher had we placed the spot closer to the boundary. The gamma is worse. Note that we applied the smoothing only at maturity. In reality as the barrier is continuous, the smoothing should really be applied at each step, but then the scheme would be not so different from a simple Backward Euler.&lt;br /&gt;&lt;br /&gt;In contrast, with a proper second order finite difference scheme, there is no spike.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-mj0mUfRCSJk/VgvGiUPP1nI/AAAAAAAAIIw/KKK9sXTrne4/s1600/Screenshot%2Bfrom%2B2015-09-30%2B13%253A24%253A27.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;325&#34; src=&#34;http://2.bp.blogspot.com/-mj0mUfRCSJk/VgvGiUPP1nI/AAAAAAAAIIw/KKK9sXTrne4/s400/Screenshot%2Bfrom%2B2015-09-30%2B13%253A24%253A27.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Delta with the TR-BDF2 finite difference method - the scale goes from -0.00008 to 0.00008.&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-okMVRlfdJGw/VgvGsntsjbI/AAAAAAAAII4/xNVchYODHGU/s1600/Screenshot%2Bfrom%2B2015-09-30%2B13%253A24%253A42.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;325&#34; src=&#34;http://1.bp.blogspot.com/-okMVRlfdJGw/VgvGsntsjbI/AAAAAAAAII4/xNVchYODHGU/s400/Screenshot%2Bfrom%2B2015-09-30%2B13%253A24%253A42.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Delta with the Lawson-Morris finite difference scheme - the scale goes from -0.00005 to 0.00005&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Both &lt;a href=&#34;http://www.risk.net/journal-of-computational-finance/technical-paper/2330321/tr-bdf2-for-fast-stable-american-option-pricing&#34;&gt;TR-BDF2&lt;/a&gt; and Lawson-Morris (based on a local Richardson extrapolation of backward Euler) have a very low delta error, similarly, their gamma is very clean. This is reminiscent of the behavior on American options, but the effect is magnified here.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Clouds</title>
      <link>http://chasethedevil.github.io/post/clouds/</link>
      <pubDate>Wed, 02 Sep 2015 15:36:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/clouds/</guid>
      <description>&lt;p&gt;I was wondering how to generate some nice cloudy like texture with a simple program. I first thought about using the Brownian motion, but of course if one uses it raw, with one pixel representing one movement in time, it&amp;rsquo;s just going to look like a very noisy and grainy picture like this:&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-QfeLi5F03oQ/Vebs3C-XWyI/AAAAAAAAIG4/5q1kKip0PlA/s1600/normal_rng.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://4.bp.blogspot.com/-QfeLi5F03oQ/Vebs3C-XWyI/AAAAAAAAIG4/5q1kKip0PlA/s200/normal_rng.png&#34; width=&#34;200&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Normal noise&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;There is however a nice continuous representation of the Brownian motion : the Paley-Wiener representation&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-fkDLapfROaY/Vebtgtm2BCI/AAAAAAAAIHA/-EgKgEF_rEM/s1600/Screenshot%2Bfrom%2B2015-09-02%2B14-37-02.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;31&#34; src=&#34;http://1.bp.blogspot.com/-fkDLapfROaY/Vebtgtm2BCI/AAAAAAAAIHA/-EgKgEF_rEM/s400/Screenshot%2Bfrom%2B2015-09-02%2B14-37-02.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-JdA4jtT3J-A/VebtgjUKcfI/AAAAAAAAIHE/4HCx4BaAPDY/s1600/Screenshot%2Bfrom%2B2015-09-02%2B14-36-42.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;60&#34; src=&#34;http://1.bp.blogspot.com/-JdA4jtT3J-A/VebtgjUKcfI/AAAAAAAAIHE/4HCx4BaAPDY/s400/Screenshot%2Bfrom%2B2015-09-02%2B14-36-42.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;This can produce an interesting smooth pattern, but it is just 1D. In the following picture, I apply it to each row (the column index being time), and then for each column (the row index being time). Of course this produces a symmetric picture, especially as I reused the same random numbers&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-CzAUnTU3V7w/VebuSjnBuLI/AAAAAAAAIHQ/t6a1FGCsIMA/s1600/constructive.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://4.bp.blogspot.com/-CzAUnTU3V7w/VebuSjnBuLI/AAAAAAAAIHQ/t6a1FGCsIMA/s200/constructive.png&#34; width=&#34;200&#34; /&gt;&lt;/a&gt;&lt;/div&gt;If I use new random numbers for the columns, it is still symmetric, but destructive rather than constructive.&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-zb3pUtY1ZMo/VebvK0C1xdI/AAAAAAAAIHY/A0-vaxGMwdI/s1600/destructive.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://3.bp.blogspot.com/-zb3pUtY1ZMo/VebvK0C1xdI/AAAAAAAAIHY/A0-vaxGMwdI/s200/destructive.png&#34; width=&#34;200&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;It turns out that spatial processes are something more complex than I first imagined. It is not a simple as using a N-dimensional Brownian motion, as it would produce a very similar picture as the 1-dimensional one. But &lt;a href=&#34;http://www.maths.uq.edu.au/~kroese/ps/MCSpatial.pdf&#34;&gt;this paper has a nice overview of spatial processes&lt;/a&gt;. Interestingly they even suggest to generate a Gaussian process using a &lt;a href=&#34;https://en.wikipedia.org/wiki/Precision_%28statistics%29&#34;&gt;Precision matrix&lt;/a&gt; (inverse of covariance matrix). I never thought about doing such a thing and I am not sure what is the advantage of such a scheme.&lt;br /&gt;&lt;br /&gt;There is a standard graphic technique to generate nice textures, originating from Ken Perlin for Disney, it is called simply &lt;a href=&#34;https://en.wikipedia.org/wiki/Perlin_noise&#34;&gt;Perlin Noise&lt;/a&gt;. It turns out that several web pages in the top Google results &lt;a href=&#34;https://en.wikipedia.org/wiki/Talk%3APerlin_noise&#34;&gt;confuse&lt;/a&gt; simple Perlin noise with fractal sum of noise that Ken Perlin also helped popularize (see his slides: &lt;a href=&#34;http://www.noisemachine.com/talk1/20.html&#34;&gt;standard Perlin noise&lt;/a&gt;, &lt;a href=&#34;http://www.noisemachine.com/talk1/21.html&#34;&gt;fractal noise&lt;/a&gt;). Those pages also believe that the later is simpler/faster. But there are two issues with fractal sum of noise: the first one is that it relies on an existing noise function - you need to first build one (it can be done with a random number generator and an interpolator), and the second one is that it ends up being more complex to program and likely to evaluate as well, see for example the code needed &lt;a href=&#34;http://devmag.org.za/2009/04/25/perlin-noise/&#34;&gt;here&lt;/a&gt;. The fractal sum of noise is really a complementary technique.&lt;br /&gt;&lt;br /&gt;The insight of Perlin noise is to not generate random color values that would be assigned to shades of grey as in my examples, but to generate random gradients, and interpolate on those gradient in a smooth manner. In computer graphics they like the cosine function to give a little bit of non-linearity in the colors. A good approximation, usually used as a replacement in this context is &lt;a href=&#34;http://codeplea.com/simple-interpolation&#34;&gt;3x^2 - 2x^3&lt;/a&gt;.  It&amp;rsquo;s not much more complicated than that, &lt;a href=&#34;http://webstaff.itn.liu.se/~stegu/TNM022-2005/perlinnoiselinks/perlin-noise-math-faq.html&#34;&gt;this web page&lt;/a&gt; explains it in great details. It can be programmed in a few lines of code.&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-Yo5dgKoD5P8/Veb6rAQ9jAI/AAAAAAAAIHo/uISQjpVzLfI/s1600/perlin_bw.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://3.bp.blogspot.com/-Yo5dgKoD5P8/Veb6rAQ9jAI/AAAAAAAAIHo/uISQjpVzLfI/s200/perlin_bw.png&#34; width=&#34;200&#34; /&gt;&lt;/a&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-MovHUFAGbnI/Veb6rsjhkBI/AAAAAAAAIHs/4raZRohDySs/s1600/perlin_color.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;200&#34; src=&#34;http://4.bp.blogspot.com/-MovHUFAGbnI/Veb6rsjhkBI/AAAAAAAAIHs/4raZRohDySs/s200/perlin_color.png&#34; width=&#34;200&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-__3z1MEB92o/Veb65IEgVZI/AAAAAAAAIH4/1B9npSW9XKo/s1600/Screenshot%2Bfrom%2B2015-09-02%2B15-33-11.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;512&#34; src=&#34;http://3.bp.blogspot.com/-__3z1MEB92o/Veb65IEgVZI/AAAAAAAAIH4/1B9npSW9XKo/s640/Screenshot%2Bfrom%2B2015-09-02%2B15-33-11.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;very procedural and non-optimized Go code for Perlin noise&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Go for Monte-Carlo</title>
      <link>http://chasethedevil.github.io/post/go-for-monte-carlo/</link>
      <pubDate>Sat, 22 Aug 2015 16:13:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/go-for-monte-carlo/</guid>
      <description>&lt;p&gt;I have &lt;a href=&#34;http://chasethedevil.blogspot.fr/2015/04/modern-programming-language-for-monte.html&#34;&gt;looked&lt;/a&gt; a few months ago already at Julia, Dart, Rust and Scala programming languages to see how practical they could be for a simple Monte-Carlo option pricing.&lt;br /&gt;&lt;br /&gt;I forgot &lt;a href=&#34;https://golang.org/&#34;&gt;the Go language&lt;/a&gt;. I had tried it 1 or 2 years ago, and at that time, did not enjoy it too much. Looking at Go 1.5 benchmarks on the&lt;a href=&#34;http://benchmarksgame.alioth.debian.org/&#34;&gt; computer language shootout&lt;/a&gt;, I was surprised that it seemed so close to Java performance now, while having a GC that guarantees pauses of less 10ms and consuming much less memory.&lt;br /&gt;&lt;br /&gt;I am in general a bit skeptical about those benchmarks, some can be rigged. A few years ago, I &lt;a href=&#34;http://chasethedevil.blogspot.fr/2009/01/end-of-rings-around-plain-java-better.html&#34;&gt;tried my hand at the thread ring&lt;/a&gt; test, and found that it actually performed fastest on a single thread while it is supposed to measure the language threading performance. I looked yesterday at one Go source code (I think it was for pidigits) and saw that it just called a C library (gmp) to compute with big integers. It&amp;rsquo;s no surprise then that Go would be faster than Java on this test.&lt;br /&gt;&lt;br /&gt;So what about my small Monte-Carlo test?&lt;br /&gt;Well it turns out that Go is quite fast on it:&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;Multipl. Rust&amp;nbsp;&amp;nbsp;&amp;nbsp; Go&lt;br /&gt;1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.005&amp;nbsp; 0.007&lt;br /&gt;10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.03&amp;nbsp;&amp;nbsp; 0.03&lt;br /&gt;100&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 0.21&amp;nbsp;&amp;nbsp; 0.29&lt;/span&gt;&lt;br /&gt;&lt;span style=&#34;font-family: &amp;quot;Courier New&amp;quot;,Courier,monospace;&#34;&gt;1000&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; 2.01&amp;nbsp;&amp;nbsp; 2.88&lt;/span&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;It is faster than Java/Scala and not too far off Rust, except if one uses FastMath in Scala, then the longest test is slighly faster with Java (not the other ones).&lt;br /&gt;&lt;br /&gt;There are some issues with the Go language: there is no operator overloading, which can make matrix/vector algebra more tedious and there is no generic/template. The later is somewhat mitigated by the automatic interface implementation. And fortunately for the former, complex numbers are a standard type. Still, automatic differentiation would be painful.&lt;br /&gt;&lt;br /&gt;Still it was extremely quick to grasp and write code, because it&amp;rsquo;s so simple, especially when compared to Rust. But then, contrary to Rust, there is not as much safety provided by the language. Rust is quite impressive on this side (but unfortunately that implies less readable code). I&amp;rsquo;d say that Go could become a serious alternative to Java.&lt;br /&gt;&lt;br /&gt;I also found an interesting minor performance issue with the default Go &lt;a href=&#34;https://golang.org/src/math/rand/rand.go&#34;&gt;Rand.Float64&lt;/a&gt;, the library convert an Int63 to a double precision number this way:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;func (r *Rand) Float64() float64 {&lt;/pre&gt;&lt;pre&gt;  f := float64(r.Int63()) / (1 &amp;lt;&amp;lt; 63)&lt;br /&gt;  if f == 1 {&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L128&#34;&gt;    &lt;/span&gt;f = 0&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L129&#34;&gt;  }&lt;/span&gt;&lt;br /&gt;&lt;span class=&#34;ln&#34; id=&#34;L130&#34;&gt;  &lt;/span&gt;return f&lt;br /&gt; }&lt;/pre&gt;&lt;br /&gt;I was interested in having a number in (0,1) and not [0,1), so I just used the conversion pattern from MersenneTwister 64 code:&lt;br /&gt;&lt;br /&gt;&lt;pre&gt;f := (float64(r.Int63() &amp;gt;&amp;gt; 11) + 0.5) * (1.0/4503599627370496.0)&lt;/pre&gt;&lt;pre&gt;&amp;nbsp;&lt;/pre&gt;The reasoning behind this later code is that the mantissa is 52 bits, and this is the most accuracy we can have between 0 and 1. There is no need to go further, this also avoids the issue around 1. It&amp;rsquo;s also straightforward that is will preserve the uniform property, while it&amp;rsquo;s not so clear to me that r.Int63()/2^63 is going to preserve uniformity as double accuracy is higher around 0 (as the exponent part can be used there) and lesser around 1: there is going to be much more multiple identical results near 1 than near 0.&lt;br /&gt;&lt;br /&gt;It turns out that the if check adds 5% performance penalty on this test, likely because of processor caching issues. I was surprised by that since there are many other ifs afterwards in the code, for the inverse cumulative function, and for the payoff.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bumping Correlations</title>
      <link>http://chasethedevil.github.io/post/bumping-correlations/</link>
      <pubDate>Sat, 25 Jul 2015 18:36:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/bumping-correlations/</guid>
      <description>&lt;p&gt;In his book &amp;ldquo;&lt;i&gt;Monte Carlo Methods in Finance&lt;/i&gt;&amp;rdquo;, P. Jäckel explains a simple way to clean up a correlation matrix. When a given correlation matrix is not positive semi-definite, the idea is to do a &lt;a href=&#34;https://en.wikipedia.org/wiki/Singular_value_decomposition&#34;&gt;singular value decomposition&lt;/a&gt; (SVD), replace the negative eigenvalues by 0, and renormalize the corresponding eigenvector accordingly.&lt;br /&gt;&lt;br /&gt;One of the cited applications is &amp;ldquo;&lt;i&gt;stress testing and scenario analysis for market risk&lt;/i&gt;&amp;rdquo; or &amp;ldquo;&lt;i&gt;comparative pricing in order to ascertain the extent of correlation exposure for multi-asset derivatives&lt;/i&gt;&amp;rdquo;, saying that &amp;ldquo;&lt;i&gt;In many of these cases we end up with a matrix that is no longer positive semi-definite&lt;/i&gt;&amp;rdquo;.&lt;br /&gt;&lt;br /&gt;It turns out that if one bumps an invalid correlation matrix (the input), that is then cleaned up automatically, the effect can be a very different bump. Depending on how familiar you are with SVD, this could be more or less obvious from the procedure,&lt;br /&gt;&lt;br /&gt;As a simple illustration I take the matrix representing 3 assets A, B, C with rho_ab = -0.6, rho_ac = rho_bc = -0.5.&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.60000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.60000&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.50000&lt;br /&gt;&amp;nbsp; -0.50000&amp;nbsp; -0.50000&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;For those rho_ac and rho_bc, the correlation matrix is not positive definite unless rho_ab in in the range (-0.5, 1). One way to verify this is to use the fact that positive definiteness is equivalent to a positive determinant. The determinant will be 1 - 2*0.25 - rho_ab^2 + 2*0.25*rho_ab.&lt;br /&gt;&lt;br /&gt;After using P. Jaeckel procedure, we end up with: &lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56299&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.56299&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46745&lt;br /&gt;&amp;nbsp; -0.46745&amp;nbsp; -0.46745&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;If we bump now rho_bc by 1% (absolute), we end up after cleanup with:&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56637&amp;nbsp; -0.47045&lt;br /&gt;&amp;nbsp; -0.56637&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46081&lt;br /&gt;&amp;nbsp; -0.47045&amp;nbsp; -0.46081 &amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;It turns out that rho_bc has changed by only 0.66% and rho_ac by -0.30%, rho_ab by -0.34%. So our initial bump (0,0,1) has been translated to a bump (-0.34, -0.30, 0.66). In other words, it does not work to compute sensitivities.&lt;br /&gt;&lt;br /&gt;One can optimize to obtain the nearest correlation matrix in some norm. Jaeckel proposes a hypersphere decomposition based optimization, using as initial guess the SVD solution. &lt;a href=&#34;https://nickhigham.wordpress.com/2013/02/13/the-nearest-correlation-matrix/&#34;&gt;Higham proposed a specific algorithm&lt;/a&gt; just for that purpose. It turns out that on this example, they will converge to the same solution (if we use the same norm). I tried out of curiosity to see if that would lead to some improvement. The first matrix becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56435&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.56435&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46672&lt;br /&gt;&amp;nbsp; -0.46672&amp;nbsp; -0.46672&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;And the bumped one becomes&lt;br /&gt;&lt;br /&gt;&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.56766&amp;nbsp; -0.46984&lt;br /&gt;&amp;nbsp; -0.56766&amp;nbsp;&amp;nbsp; 1.00000&amp;nbsp; -0.46002&lt;br /&gt;&amp;nbsp; -0.46984&amp;nbsp; -0.46002&amp;nbsp;&amp;nbsp; 1.00000&lt;br /&gt;&lt;br /&gt;We find back the same issue, rho_bc has changed by only 0.67%, rho_ac by -0.31% and rho_ab by -0.33%. We also see that the SVD correlation or the real near correlation matrix are quite close, as noticed by P. Jaeckel.&lt;br /&gt;&lt;br /&gt;Of course, one should apply the bump directly to the cleaned up matrix, in which case it will actually work as expected, unless our bump produces another non positive definite matrix, and then we would have correlation leaking a bit everywhere. It&amp;rsquo;s not entirely clear what kind of meaning the risk figures would have then.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Andreasen Huge extrapolation</title>
      <link>http://chasethedevil.github.io/post/andreasen-huge-extrapolation/</link>
      <pubDate>Mon, 13 Jul 2015 17:35:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/andreasen-huge-extrapolation/</guid>
      <description>&lt;p&gt;There are not many arbitrage free extrapolation schemes. Benaim et al. extrapolation is one of the few that claims it. However, despite the paper&amp;rsquo;s title, it is not truely arbitrage free. The density might be positive, but the forward is not preserved by the implied density. It can also lead to wings that don&amp;rsquo;t obey Lee&amp;rsquo;s moments condition.&lt;br /&gt;&lt;br /&gt;On a Wilmott forum, &lt;a href=&#34;http://www.wilmott.com/messageview.cfm?catid=4&amp;amp;threadid=95309&#34;&gt;P. Caspers proposed&lt;/a&gt; the following counter-example based on extrapolating SABR: \( \alpha=15\%, \beta=80\%, \nu=50\%, \rho=-48\%, f=3\%, T=20.0 \). He cut this smile at 2.5% and 6% and used the BDK extrapolation scheme with mu=nu=1.&lt;br /&gt;&lt;br /&gt;A truly arbitrage free extrapolation can be obtained through &lt;a href=&#34;http://ssrn.com/abstract=1694972&#34;&gt;Andreasen Huge volatility interpolation&lt;/a&gt;, making sure the grid is wide enough to allow extrapolation. Their method is basically a one step finite difference implicit Euler scheme applied to a local volatility parameterization that has as many parameters than prices. The method is presented with piecewise constant local volatility, but actually used with piecewise linear local volatility in their example.&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://3.bp.blogspot.com/-Cp1keeZvJjY/VaPaq-_ttVI/AAAAAAAAIFU/3M-0n5N4eqw/s1600/Screenshot-Untitled%2BWindow-5.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://3.bp.blogspot.com/-Cp1keeZvJjY/VaPaq-_ttVI/AAAAAAAAIFU/3M-0n5N4eqw/s400/Screenshot-Untitled%2BWindow-5.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Smile&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-mG5xhhikk5Y/VaPWmDv875I/AAAAAAAAIFA/X8SU8NUmEAE/s1600/Screenshot-Untitled%2BWindow-4.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://2.bp.blogspot.com/-mG5xhhikk5Y/VaPWmDv875I/AAAAAAAAIFA/X8SU8NUmEAE/s400/Screenshot-Untitled%2BWindow-4.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Density with piecewise linear local volatility&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;There is still a tiny oscillation that makes the density negative, but one understands why typical extrapolations fail on the example: the change in density must be very steep.&lt;br /&gt;Note that moving the left extrapolation point even closer to the forward might fix BDK negative density, but we  are already very close, and we can really wonder if going closer is  really a good idea since we would effectively use a somewhat arbitrary  extrapolation in most of the interpolation zone.&lt;br /&gt;&lt;br /&gt;It turns out that we can also use a cubic spline local volatility with linear extrapolation, and the density would look then: &lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://2.bp.blogspot.com/-UytrsK6ficA/VaPWr-BEHmI/AAAAAAAAIFI/BtzCxV0MCSI/s1600/Screenshot-Untitled%2BWindow-3.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;283&#34; src=&#34;http://2.bp.blogspot.com/-UytrsK6ficA/VaPWr-BEHmI/AAAAAAAAIFI/BtzCxV0MCSI/s400/Screenshot-Untitled%2BWindow-3.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Density with cubic spline local volatility&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;Interestingly, the right side of the density is much better captured.&lt;br /&gt;The wiggle persists, although it is smaller. This is likely due to the fact that I am using a cubic spline on top of the finite difference prices (in order to have a C2 density). Using a better C2 convexity preserving interpolation would likely remove this artefact.&lt;br /&gt;&lt;br /&gt;Those figures also show why relying just on extrapolation to fix SABR is not necessarily a good idea: even a real arbitrage free extrapolation will make a not so meaningful density. The proper solution is to really use &lt;a href=&#34;http://chasethedevil.blogspot.fr/2013/12/arbitrage-free-sabr-another-view-on.html&#34;&gt;Hagan&amp;rsquo;s arbitrage free SABR PDE&lt;/a&gt;, which would be as nearly fast in this case.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unintuitive behavior of the Black-Scholes formula - negative volatilities in displaced diffusion extrapolation</title>
      <link>http://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</link>
      <pubDate>Tue, 07 Jul 2015 16:43:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/unintuitive-behavior-of-the-black-scholes-formula---negative-volatilities-in-displaced-diffusion-extrapolation/</guid>
      <description>&lt;p&gt;I am looking at various extrapolation schemes of the implied volatilities. An interesting one I stumbled upon is due to Kahale. Even if &lt;a href=&#34;http://nkahale.free.fr/papers/Interpolation.pdf&#34;&gt;his paper&lt;/a&gt; is on interpolation, there is actually a small paragraph on using the same kind of function for extrapolation. His idea is to simply lookup the standard deviation \( \Sigma \) and the forward \(f\) corresponding to a given market volatility and slope:
$$ c_{f,\Sigma} = f N(d_1) - k N(d_2)$$
with
$$ d_1 = \frac{\log(f/k)+\Sigma^2 /2}{\Sigma} $$&lt;/p&gt;

&lt;p&gt;We have simply:
$$ c&amp;rsquo;(k) = - N(d_2)$$&lt;/p&gt;

&lt;p&gt;He also proves that we can always find those two parameters for any \( k_0 &amp;gt; c_0 &amp;gt; 0,  -1 &amp;lt; c_0&amp;rsquo; &amp;lt; 0 \)&lt;/p&gt;

&lt;p&gt;Then I had the silly idea of trying to match with a put&amp;nbsp; instead of a call for the left wing (as those are out-of-the-money, and therefore easier to invert numerically). It turns out that it works in most cases in practice and produces relatively nice looking extrapolations, but it does not always work. This is because contrary to the call, the put value is bounded with \(f\).
$$ p_{f,\Sigma} = k N(-d_2) - f N(-d_1)$$&lt;/p&gt;

&lt;p&gt;Inverting \( p_0&amp;rsquo; \) is going to lead to a specific \( d_2 \), and you are not guaranteed that you can push \( f \) high and have \( p_{f, \Sigma} \) large enough to match \( p_0 \). As example we can just take \(p_0 \geq k N(-d_2)\) which will only be matched if \( f \leq 0 \).&lt;/p&gt;

&lt;p&gt;This is slightly unintuitive as put-call parity would suggest some kind of equivalence. The problem here is that we would need to consider the function of \(k\) instead of \(f\) for it to work, so we can&amp;rsquo;t really work with a put directly.&lt;/p&gt;

&lt;p&gt;Here are the two different extrapolations on Kahale own example:
&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-UcomxGsx_r0/VZvSdzoBBVI/AAAAAAAAIEU/_V542xidbgc/s1600/Screenshot-Untitled%2BWindow.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-UcomxGsx_r0/VZvSdzoBBVI/AAAAAAAAIEU/_V542xidbgc/s400/Screenshot-Untitled%2BWindow.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing with calls (blue doted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-sDl37fFAImE/VZvSd5eWCgI/AAAAAAAAIEQ/tOUG7nHrg_Q/s1600/Screenshot-Untitled%2BWindow-1.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-sDl37fFAImE/VZvSd5eWCgI/AAAAAAAAIEQ/tOUG7nHrg_Q/s400/Screenshot-Untitled%2BWindow-1.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing with puts (blue doted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
Displaced diffusion extrapolation is sometimes advocated. It is not the same as Kahale extrapolation: In Kahale, only the forward variable is varying in the Black-Scholes formula, and there is no real underlying stochastic process. In a displaced diffusion setting, we would adjust both strike and forward, keeping put-call parity at the formula level. But unfortunately, it suffers from the same kind of problem: it can not always be solved for slope and price. When it can however, it will give a more consistent extrapolation.
I find it interesting that some smiles can not be extrapolated by displaced diffusion in a C1 manner except if one allows negative volatilities in the formula (in which case we are not anymore in a pure displaced diffusion setting).
&lt;table align=&#34;center&#34; cellpadding=&#34;0&#34; cellspacing=&#34;0&#34; class=&#34;tr-caption-container&#34; style=&#34;margin-left: auto; margin-right: auto; text-align: center;&#34;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;http://4.bp.blogspot.com/-VSy7uTzu56U/VZwJUDT5iJI/AAAAAAAAIEo/lsi8EakZ-kA/s1600/Screenshot-Untitled%2BWindow-2.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: auto; margin-right: auto;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;297&#34; src=&#34;http://4.bp.blogspot.com/-VSy7uTzu56U/VZwJUDT5iJI/AAAAAAAAIEo/lsi8EakZ-kA/s400/Screenshot-Untitled%2BWindow-2.png&#34; width=&#34;400&#34; /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class=&#34;tr-caption&#34; style=&#34;text-align: center;&#34;&gt;Extrapolation of the left wing using negative displaced diffusion volatilities (blue dotted line)&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Desktops in 2015</title>
      <link>http://chasethedevil.github.io/post/linux-desktops-in-2015/</link>
      <pubDate>Wed, 24 Jun 2015 18:53:00 +0000</pubDate>
      
      <guid>http://chasethedevil.github.io/post/linux-desktops-in-2015/</guid>
      <description>&lt;p&gt;I seem to &lt;a href=&#34;http://chasethedevil.blogspot.fr/2014/05/kde-xfce-gnome-shell-in-2014.html&#34;&gt;never be entirely happy with any of the linux desktops&lt;/a&gt; these days. I have used XFCE on Ubuntu quite a bit in the past year, it mostly works, but I still had minor annoyances:&lt;br /&gt;- sometimes (rarely) my laptop would not wake up from sleep.&lt;br /&gt;- notifications sometimes keep popping up too much.&lt;br /&gt;- on my desktop, experienced strong tearing issues with the Radeon graphic card, except with some very specific combination of video player settings and desktop settings (and then I had annoying redraw issue when pushing volume up/down in movies).&lt;br /&gt;&lt;br /&gt;I am satisfied with two different approaches since:&lt;br /&gt;- OpenSuse 13.2 with KDE 4. I use that on my desktop, all issues are gone, and the integration of KDE in OpenSuse is clearly the best I have experienced. In contrast, KDE 5 on Ubuntu was a disaster for me. I also managed to fuck up the apt dependencies so much that I thought it would be simpler to reinstall a new distribution.&lt;br /&gt;- Mate on Ubuntu 15.04. Very impressed so far. It&amp;rsquo;s probably what Gnome should have been instead of going to 3.0. Even if there are nice aspects of the Gnome shell, Mate is fast, pretty, user friendly, much better than Cinnamon. There are even a few layouts to choose (most of them are good), here is &amp;ldquo;Eleven with Mate menu&amp;rdquo; (it installed and setup the Plank dock automatically for that layout, more traditional layouts without dock are available):&lt;br /&gt;&lt;br /&gt;&lt;div class=&#34;separator&#34; style=&#34;clear: both; text-align: center;&#34;&gt;&lt;a href=&#34;http://1.bp.blogspot.com/-aCnzz5ujbA4/VYrfl3lhz1I/AAAAAAAAIDw/5qV5NrGn7a8/s1600/Mate-Eleven.png&#34; imageanchor=&#34;1&#34; style=&#34;margin-left: 1em; margin-right: 1em;&#34;&gt;&lt;img border=&#34;0&#34; height=&#34;358&#34; src=&#34;http://1.bp.blogspot.com/-aCnzz5ujbA4/VYrfl3lhz1I/AAAAAAAAIDw/5qV5NrGn7a8/s640/Mate-Eleven.png&#34; width=&#34;640&#34; /&gt;&lt;/a&gt;&lt;/div&gt;&lt;br /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>