<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.101.0" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2023. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/godaddy-sold-my-domain-name/">Godaddy sold my domain name</a>
  </h1>
  <time datetime="2013-06-24T19:11:00Z" class="post-date">Mon, Jun 24, 2013</time>
  <p>I discovered that suddenly emails sent to me bounced back yesterday. I logged in my godaddy account and to my surprise saw that I did not own any domain name anymore. I looked at my emails to see if I had received a warning as is usually the case when your domain is about to expire. There was none recent, the most recent was from may 2011, the last time I had renewed my domain.</p>
<p>I then tried to buy again the same domain name only to discover it was already taken! The whois record indicated a day old registration through godaddy itself.</p>
<p>It&rsquo;s no coincidence that godaddy sells for 3 times the price the possibility to try to take over a domain as soon as it will expire. I find particularly dishonest that in this case they fail to warn their own customers that their domain is about to expire. As a result of this policy, someone else will take over the domain through them for a much higher price. A conflict of interest.</p>
<p>From now on I will not register a domain through a registrar that offers the service to snatch up a domain.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/scala-build-tool--sbt/">Scala Build Tool : SBT</a>
  </h1>
  <time datetime="2013-06-19T18:01:00Z" class="post-date">Wed, Jun 19, 2013</time>
  <p>It&rsquo;s been a while since I do a pet project in Scala, and today, after many trials before, I decided to give another go at Jetbrain Idea for Scala development, as Eclipse with the Scala plugin tended to crash a little bit too often for my taste (resulting sometimes in loss of a few lines of code). I could have just probably updated eclipse and the scala plugin, mine were not very old, but not the latest.</p>
<p>But it was just an opportunity to try Idea. I somehow always failed before to setup properly the scala support in Idea while it seemed to just work in Eclipse. I had difficulties making it find my scala compiler. After some google searches, I found that  <a href="http://www.scala-sbt.org/">SBT</a>, the scala build tool<!-- raw HTML omitted --> could create automatically a scala project for Idea (a hint to make it work with a project under Scala 2.10 is to put the plugins.sbt file in ~/.sbt/plugins).</p>
<p>It was reasonably easy to create a simple build.sbt file for my project. I added some dependencies (it handles them like Ivy, from Maven repositories), and was pleased to find you could also just put your jars in lib directory if you did not want/could not find some maven repository.</p>
<p>The tool is quick to launch, does not get in the way. So far the experience has been much much nicer than Gradle that we use now at work, which I find painfully slow to start, check dependencies, and extremely complicated to customize to your needs. It&rsquo;s also nicer than Maven, which I always found painful as soon as one wanted a small specific behaviour.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/the-finite-difference-theta-scheme-optimal-theta/">The Finite Difference Theta Scheme Optimal Theta</a>
  </h1>
  <time datetime="2013-06-18T15:02:00Z" class="post-date">Tue, Jun 18, 2013</time>
  <p>The theta finite difference scheme is a common generalization of Crank-Nicolson. In finance, the <a href="http://www.amazon.com/Wilmott-Quantitative-Finance-Volume-Edition/dp/0470018704/ref=sr_1_1?ie=UTF8&amp;qid=1371557569&amp;sr=8-1&amp;keywords=paul+wilmott+on+quantitative+finance">book from Wilmott</a>, a <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fwww.javaquant.net%2Fpapers%2Ffdpaper.pdf&amp;ei=DE7AUf-XHo24hAfkvICwBg&amp;usg=AFQjCNHD0qmjyMZtzbLfao3YHCFwySYixg&amp;sig2=9DdLJ9FoVCXoeaus5JykQg&amp;bvm=bv.47883778,d.ZG4">paper from A. Sepp</a>, <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;sqi=2&amp;ved=0CDcQFjAB&amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F4451%2Fv1n2a1b.pdf&amp;ei=6k7AUZXXFYXLhAeF3YDYDA&amp;usg=AFQjCNFq4dBzQ54M34qd0DMQ4pgSCcjfQg&amp;sig2=AR3zjeDQ3WIvWOvt-1tNeg&amp;bvm=bv.47883778,d.ZG4">one from Andersen-Ratcliffe</a> present it. Most of the time, it&rsquo;s just a convenient way to handle implicit \(\theta=1\), explicit \(\theta=0\) and Crank-Nicolson \(\theta=0.5\) with the same algorithm.</p>
<p>Wilmott makes an interesting remark: one can choose a theta that will cancel out higher order terms in the local truncation error and therefore should lead to increased accuracy. $$\theta = \frac{1}{2}- \frac{(\Delta x)^2}{12 b \Delta t} $$
where \(b\) is the diffusion coefficient.
This leads to \(\theta &lt; \frac{1}{2}\), which means the scheme is not unconditionally stable anymore but needs to obey (see Morton &amp; Mayers p 30):
$$b \frac{\Delta t}{(\Delta x)^2} \leq \frac{5}{6}$$
and to ensure that \(\theta \geq 0 \):
$$b \frac{\Delta t}{(\Delta x)^2} \geq \frac{1}{6}$$</p>
<p>Crank-Nicolson has a similar requirement to ensure the absence of oscillations given non smooth initial value, but because it is unconditionality stable, the condition is actually much weaker if \(b\) depends on \(x\). Crank-Nicolson will be oscillation free if \(b(x_{j0}) \frac{\Delta t}{(\Delta x)^2} &lt; 1\) where \(j0\) is the index of the discontinuity, while the theta scheme needs to be stable, that is \(\max(b) \frac{\Delta t}{(\Delta x)^2} \leq \frac{5}{6}\)</p>
<p>This is a much stricter condition if \(b\) varies a lot, as it is the case for the <a href="https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/">arbitrage free SABR PDE</a> where \(\max(b) &gt; 200 b_{j0}\)
<figure><img src="/post/snapshot23.png"/>
</figure>

The advantages of such a scheme are then not clear compared to a simpler explicit scheme (eventually predictor corrector), that will have a similar constraint on the ratio \( \frac{\Delta t}{(\Delta x)^2} \).</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/simple-can-scala-do-this-questions/">Simple &#39;Can Scala Do This?&#39; Questions</a>
  </h1>
  <time datetime="2013-06-11T00:28:00Z" class="post-date">Tue, Jun 11, 2013</time>
  <p>Today, a friend asked me if Scala could pass primitives (such as Double) by reference. It can be useful sometimes instead of creating a full blown object. In Java there is commons lang MutableDouble. It could be interesting if there was some optimized way to do that.</p>
<p>One answer could be: it&rsquo;s not functional programming oriented and therefore not too surprising this is not encouraged in Scala.</p>
<p>Then he wondered if we could use it for C#.</p>
<p>I know this used to be possible in Scala 1.0, I believe it&rsquo;s not anymore since 2.x. This was a cool feature, especially if they had managed to develop strong libraries around it. I think it was abandoned to focus on other things, because of lack of resources, but it&rsquo;s sad.</p>
<p>Later today, I tried to use the nice syntax to return multiple values from a method:
<code>var (a,b) = mymethod(1)</code></p>
<p>I noticed you then could not do:
<code>(a,b) = mymethod(2)</code></p>
<p>So declaring a var seems pointless in this case.</p>
<p>One way to achieve this is to:</p>
<p><code>var tuple = mymethod(1)</code>
<code>var a = tuple._1</code>
<code>var b = tuple._2</code></p>
<p>This does not look so nice.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/akima-for-yield-curve-interpolation-/">Akima for Yield Curve Interpolation ?</a>
  </h1>
  <time datetime="2013-06-03T00:07:00Z" class="post-date">Mon, Jun 3, 2013</time>
   

On my test of <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CDgQFjAB&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D2175002&amp;ei=d8GrUb70CJCChQeA7IHoCg&amp;usg=AFQjCNHHizgzORef228lnYX3HygLb9okAg&amp;sig2=NYbhD30aD7sD8TS7CYodzw&amp;bvm=bv.47244034,d.ZG4">yield curve interpolations</a>, focusing on parallel delta versus sequential delta, <a href="http://200.17.213.49/lib/exe/fetch.php/wiki:internas:biblioteca:akima.pdf">Akima</a> is the worst of the lot. I am not sure why this interpolation is still popular when most alternatives seem much better. Hyman presented some of the issues with Akima in <a href="http://epubs.siam.org/doi/abs/10.1137/0904045">his paper</a> in 1983. <br /><br />In the following graph, a higher value is a higher parallel-vs-sequential difference. <br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-y1jkd6Pu4Y8/UavBtMgvLQI/AAAAAAAAGc4/CDRwTqv-suc/s1600/snapshot18.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="272" src="http://4.bp.blogspot.com/-y1jkd6Pu4Y8/UavBtMgvLQI/AAAAAAAAGc4/CDRwTqv-suc/s400/snapshot18.png" width="400" /></a></div><div class="separator" style="clear: both; text-align: center;"></div>That plus the Hagan-West example of a tricky curve looks a bit convoluted with it (although it does not have any negative forward).<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-kKk7NEYtUaw/UavB245C8OI/AAAAAAAAGdA/elBq-es3jWY/s1600/snapshot19.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="225" src="http://4.bp.blogspot.com/-kKk7NEYtUaw/UavB245C8OI/AAAAAAAAGdA/elBq-es3jWY/s400/snapshot19.png" width="400" /></a></div>I have used Quantlib implementation, those results make me wonder if there is not something wrong with the boundaries.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/2-ways-for-an-accurate-barrier-with-finite-difference/">2 Ways for an Accurate Barrier with Finite Difference </a>
  </h1>
  <time datetime="2013-06-02T00:46:00Z" class="post-date">Sun, Jun 2, 2013</time>
   

I had explored the issue of pricing a barrier using finite difference discretization of the Black-Scholes PDE a few years ago. Briefly, for explicit schemes, one just need to place the barrier on the grid and not worry about much else, but for implicit schemes, either the barrier should be placed on the grid and the grid&nbsp; <b>truncated </b>at the barrier, or a <b>fictitious point</b> should be introduced to force the correct price at the barrier level (0, typically).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-BDNo4x-nxVw/Uap5CCA8BJI/AAAAAAAAGco/SxT1WlDrIbA/s1600/barrier_grid.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="135" src="http://1.bp.blogspot.com/-BDNo4x-nxVw/Uap5CCA8BJI/AAAAAAAAGco/SxT1WlDrIbA/s400/barrier_grid.png" width="400" /></a></div><br />The fictitious point approach is interesting for the case of varying rebates, or when the barrier moves around. I first saw this idea in the book "Paul Wilmott on Quantitative Finance".<br /><br />Recently, I noticed that Hagan made use of the ficitious point approach in its "Arbitrage free SABR" paper, specifically he places the barrier in the middle of 2 grid points. There is very little difference between truncating the grid and the fictitious point for a constant barrier.<br /><br />In this specific case there is a difference because there are 2 additional ODE solved on the same grid, at the boundaries. I was especially curious if one could place the barrier exactly at 0 with the fictitious point, because then one would potentially need to evaluate coefficients for negative values. It turns out you can, as values at the fictitious point are actually not used: the mirror point inside is used because of the mirror boundary conditions.<br /><br />So the only difference is the evaluation of the first derivative at the barrier (used only for the ODE): the fictitious point uses the value at barrier+h/2 where h is the space between two points at the same timestep, while the truncated barrier uses a value at barrier+h (which can be seen as standard forward/backward first order finite difference discretization at the boundaries). For this specific case, the fictitious point will be a little bit more precise for the ODE.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/">SABR with the new Hagan PDE Approach</a>
  </h1>
  <time datetime="2013-05-28T15:56:00Z" class="post-date">Tue, May 28, 2013</time>
   

At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it's just like a regular PDE with proper boundary conditions.<br /><br />I was wondering how it compared to Andreasen Huge results.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s1600/snapshot14.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s640/snapshot14.png" height="304" width="640" /></a></div><br /><br />My first implementation was quite slow. I postulated it was likely the Math.pow function calls. It turns out they could be reduced a great deal. As a result, it's now quite fast. But it would still be much slower than Andreasen Huge. Typically, one might use 40 time steps, while Andreasen Huge is 1, so it could be around a 40 to 1 ratio. In practice it's likely to be less than 10x slower, but still.<br /><br />While looking at the implied volatilities I found something intriguing with Andreasen Huge: the implied volatilities from the refined solution using the corrected forward volatility look further away from the Hagan implied volatilitilies than without adjustment, and it's quite pronounced at the money.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s1600/snapshot15.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s640/snapshot15.png" height="304" width="640" /></a></div>Interestingly, the authors don't plot that graph in their paper. They  plot a similar graph of their own closed form analytic formula, that is  in reality used to compute the forward volatility. I suppose that  because they calibrate and price through their method, they don't really  care so much that the ATM prices don't match Hagan original formula.<br /><br />We can see something else on that graph: Hagan PDE boundary is not as nice as Andreasen Huge boundary for high strikes (they use a Hagan like approx at the boundaries, this is why it crosses the Hagan implied volatilities there). <br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s1600/snapshot16.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s640/snapshot16.png" height="304" width="640" /></a></div><br /><br />If we use a simple option gamma = 0 boundary in Andreasen Huge, this results in a very similar shape as the Hagan PDE. This is because the option price is effectively 0 at the boundary.<br />Hagan chose a specifically taylored Crank-Nicolson scheme. I was wondering how it fared when I reduced the number of time-steps. <br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s1600/snapshot17.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s400/snapshot17.png" height="190" width="400" /></a></div><br />The answer is: not good. This is the typical Crank-Nicolson issue. It could be interesting to adapt the method to use Lawson-Morris-Goubet or TR-BDF2, or a simple Euler Richardson extrapolation. This would allow to use less time steps, as in practice, the accuracy is not so bad with 10 time steps only.<br /><br />What I like about the Hagan PDE approach is that the implied vols and the probability density converge well to the standard Hagan formula, when there is no negative density problem, for example for shorter maturities. This is better than Andreasen Huge, where there seems to be always 1 vol point difference. However their method is quite slow compared to the original simple analytic formula.<br /><br /><b>Update March 2014</b> - I have now a paper around this "<a href="http://ssrn.com/abstract=2402001">Finite Difference Techniques for Arbitrage Free SABR</a>"



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/sabr-with-andreasen-huge/">SABR with Andreasen-Huge</a>
  </h1>
  <time datetime="2013-05-24T14:17:00Z" class="post-date">Fri, May 24, 2013</time>
   

I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.<br /><br />Around 2 years ago, I saw a presentation from <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;ei=-VOfUYncL8HB7AbK0YHgBg&amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;sig2=SChIkU-TBR7ECaLdDm1orA&amp;bvm=bv.47008514,d.ZGU">Andreasen and Huge about how they were able to price/calibrate SABR</a> by a one-step finite difference technique. At that time, I did not understand much their idea. My mind was too focused on more classical finite differences techniques and not enough on the big picture in their idea. Their idea is quite general and can be applied to much more than SABR. <br /><br />Recently there has been some talk and development going on where I work about SABR (a popular way to interpolate the option implied volatility surface for interest rate derivatives), especially regarding the implied volatility wings at low strike, and sometimes on how to price in a negative rates environment. There are actually quite a bit of research papers around this. I am not really working on that part so I just mostly listened. Then a former coworker suggested that the Andreasen Huge method was actually what banks seemed to choose in practice. A few weeks later, the Thalesians (a group for people interested in quantitative finance) announced a presentation by Hagan (one of the inventor of SABR) about a technique that sounded very much like Andreasen-Huge&nbsp; to deal with the initial SABR issues in low rates.<br /><br />As the people working on this did not investigate Andreasen-Huge technique, I somehow felt that I had to and that maybe, this time, I would be able to grasp their idea.<br /><br />It took me just a few hours to have meaningful results. Here is the price of out of the money vanilla options using alpha = 0.0758194, nu = 0.1, beta = 0.5, rho = -0.1, forward = 0.02, and a maturity of 2 years.<br /><div class="separator" style="clear: both; text-align: center;"></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s1600/Screenshot+from+2013-05-24+13:29:24.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="212" src="http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s400/Screenshot+from+2013-05-24+13:29:24.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s1600/Screenshot+from+2013-05-24+13:30:09.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="215" src="http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s400/Screenshot+from+2013-05-24+13:30:09.png" width="400" /></a></div>I did not have in my home library a way to find the implied volatility for a given price. I knew of 2 existing methods, <a href="http://www.pjaeckel.webspace.virginmedia.com/ByImplication.pdf">Jaeckel "By Implication"</a>, and <a href="http://scholar.google.fr/citations?view_op=view_citation&amp;hl=fr&amp;user=3GRhH_IAAAAJ&amp;citation_for_view=3GRhH_IAAAAJ:d1gkVwhDpl0C">Li rational functions</a> approach. I discovered that Li wrote <a href="http://www.tandfonline.com/doi/abs/10.1080/14697680902849361">a new paper</a> on the subject where he uses a SOR method to find the implied volatility and claims it's very accurate, very fast and very robust. Furthermore, the same idea can be applied to normal implied volatility. What attracted me to it is the simplicity of the underlying algorithm. Jaeckel's way is a nice way to do Newton-Raphson, but there seems to be so many things to "prepare" to make it work in most cases, that I felt it would be too much work for my experiment. It took me a few more hours to code Li SOR solvers, but it worked amazingly well for my experiment.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s1600/Screenshot+from+2013-05-24+13%253A31%253A33.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="215" src="http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s400/Screenshot+from+2013-05-24+13%253A31%253A33.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s1600/Screenshot+from+2013-05-24+13%253A37%253A51.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="211" src="http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s400/Screenshot+from+2013-05-24+13%253A37%253A51.png" width="400" /></a></div><br />At first I had an error in my boundary condition and had no so good results especially with a long maturity. The traps with Andreasen-Huge technique are very much the same as with classical finite differences: be careful to place the strike on the grid (eventually smooth it), and have good boundaries.<br /><br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/">Large Steps in Schobel-Zhu/Heston the Lazy Way</a>
  </h1>
  <time datetime="2013-05-17T12:46:00Z" class="post-date">Fri, May 17, 2013</time>
   

<a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1485403&amp;ei=ODqVUY-8GY2v7AaSsIHgDw&amp;usg=AFQjCNGxk1TqaYu0mxni-OQib6V6lU-M0g&amp;sig2=xzLPCiO5kdF97KN4Tz474A&amp;bvm=bv.46471029,d.ZGU">Van Haastrecht, Lord and Pelsser</a> present an effective way to price derivatives by Monte-Carlo under the Schobel-Zhu model (as well as under the Schobel-Zhu-Hull-White model). It's quite similar to Andersen QE scheme for Heston in spirit.<br /><br />In their paper they evolve the (log) asset process together with the volatility process, using the same discretization times. A while ago, when looking at&nbsp; <a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;bvm=bv.46471029,d.ZGU">Joshi and Chan</a> large steps for Heston, I noticed that, inspired by Broadie-Kaya exact Heston scheme, they present the idea to evolve the variance process using small steps and the asset process using large steps (depending on the payoff) using the integrated variance value computed by small steps. The asset steps correspond to payoff evaluation dates&nbsp; At that time I had applied this idea to Andersen QE scheme and it worked reasonably well.<br /><br />So I tried to apply the same logic to Schobel Zhu, and my first tests show that it works too. Interestingly, the speed gain is about 2x. Here are the results for a vanilla call option of different strikes.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s1600/Screenshot+from+2013-05-17+12:33:07.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="276" src="http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s400/Screenshot+from+2013-05-17+12:33:07.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Similar Error between long and short asset steps</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s1600/Screenshot+from+2013-05-17+12:32:50.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="276" src="http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s400/Screenshot+from+2013-05-17+12:32:50.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Long steps take around 1/2 the time to compute</td></tr></tbody></table>I would have expected the difference in performance to increase when the step size is decreasing, but it's not the case on my computer.<br /><br />It's not truly large steps like <a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;bvm=bv.46471029,d.ZGU">Joshi and Chan</a> do in their integrated double gamma scheme as the variance is still discretized in relatively small steps in my case, but it seems like a good, relatively simple optimization. A while ago, I did also implement the full Joshi and Chan scheme, but it's really interesting if one is always looking for long steps: it is horribly slow when the step size is small, which might occur for many exotic payoffs, while Andersen QE scheme perform almost as well as log-Euler in terms of computational cost.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/">Exact Forward in Monte-Carlo</a>
  </h1>
  <time datetime="2013-05-13T17:58:00Z" class="post-date">Mon, May 13, 2013</time>
  <p>Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.</p>
<p>In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &ldquo;The Equity Option Volatility Smile: a Finite Difference Approach&rdquo; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices</p>
<p>Code has been updated and roll-backed, people have complained around it. But nobody really made the effort to simply write clearly what&rsquo;s going on, or even write a unit test around it. So it was just FUD, until <a href="http://ssrn.com/abstract=2264327">this paper</a>.</p>
<p>In short, for log-Euler, one can use the intuitive forward drift rate: r1<em>t1-r0</em>t0 (ratio of discount factors), but for Euler, one need to use a less intuitive forward drift rate to reproduce a nearly exact forward price.</p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/16/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/18/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
