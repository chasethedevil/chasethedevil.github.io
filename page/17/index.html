<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.123.7">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 60618. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/levenberg-marquardt--constraints-by-domain-transformation/">Levenberg Marquardt &amp; Constraints by Domain Transformation</a>
  </h1>
  <time datetime="2013-12-17T15:27:00Z" class="post-date">Tue, Dec 17, 2013</time>
   

The Fortran <a href="http://www.netlib.org/minpack/">minpack</a> library has a good <a href="http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg-Marquardt</a> minimizer, so good, that it has been ported to many programming languages. Unfortunately it does not support contraints, even simple bounds.<br /><br />One way to achieve this is to transform the domain via a bijective function. For example, \(a+\frac{b-a}{1+e^{-\alpha t}}\) will transform \(]-\infty, +\infty[\) to ]a,b[. Then how should one choose \(\alpha\)?<br /><br />A large \(\alpha\) will make tiny changes in \(t\) appear large. A simple rule is to ensure that \(t\) does not create large changes in the original range ]a,b[, for example we can make \(\alpha t \leq 1\), that is \( \alpha t= \frac{t-a}{b-a} \).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s1600/Screenshot+from+2013-12-17+13:02:22.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="293" src="http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s400/Screenshot+from+2013-12-17+13:02:22.png" width="400" /></a></div><br /><br />In practice, for example in the calibration of the<a href="http://en.wikipedia.org/wiki/Heston_model"> Double-Heston</a> model on real data, a naive \( \alpha=1 \) will converge to a RMSE of 0.79%, while our choice will converge to a RMSE of 0.50%. Both will however converge to the same solution if the initial guess is close enough to the real solution. Without any transform, the RMSE is also 0.50%. The difference in error might not seem large but this results in vastly different calibrated parameters. Introducing the transform can significantly change the calibration result, if not done carefully.<br /><br />Another simpler way would be to just impose a cap/floor on the inputs, thus ensuring that nothing is evaluated where it does not make sense. In practice, it however will not always converge as well as the unconstrained problem: the gradient is not defined at the boundary. On the same data, the Schobel-Zhu, unconstrained converges with RMSE 1.08% while the transform converges to 1.22% and the cap/floor to 1.26%. The Schobel-Zhu example is more surprising since the initial guess, as well as the results are not so far:<br />             <style>  <!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:"Liberation Sans"; font-size:x-small }    --> </style>     <br /><table border="0" cellspacing="0" cols="2"> <colgroup span="2" width="85"></colgroup> <tbody><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Initial volatility (v0)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">18.1961174789</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Long run volatility (theta)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Speed of mean reversion (kappa)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">101.2291161766</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Vol of vol (sigma)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">35.2221829015</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Correlation (rho)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">-73.7995231799</span></td> </tr><tr>  <td align="LEFT" height="17" style="border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">ERROR MEASURE</span></td>  <td align="RIGHT" style="border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1.0614889526</span></td> </tr></tbody></table><br />             <style>  <!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:"Liberation Sans"; font-size:x-small }    --> </style>     <br /><table border="0" cellspacing="0" cols="2"> <colgroup span="2" width="85"></colgroup> <tbody><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Initial volatility (v0)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">17.1295934569</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Long run volatility (theta)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Speed of mean reversion (kappa)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">67.9818356373</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Vol of vol (sigma)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">30.8491256097</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Correlation (rho)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">-74.614636128</span></td> </tr><tr>  <td align="LEFT" height="17" style="border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">ERROR MEASURE</span></td>  <td align="RIGHT" style="border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1.2256421987</span></td> </tr></tbody></table><br />The initial guess is kappa=61% theta=11% sigma=26% v0=18% rho=-70%. Only the kappa is different in the two results, and the range on the kappa is (0,2000) (it is expressed in %), much larger than the result. In reality, theta is the issue (in (0,1000)). Forbidding a negative theta has an impact on how kappa is picked. The only way to be closer<br /><br />Finally, a third way is to rely on a simple penalty: returning an arbitrary large number away from the boundary. In our examples this was no better than the transform or the cap/floor.<br /><br />Trying out the various ways, it seemed that allowing meaningless parameters, as long as they work mathematically produced the best results with Levenberg-Marquardt, particularly, allowing for a negative theta in Schobel-Zhu made a difference.


  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach/">Arbitrage Free SABR - Another View on Hagan Approach</a>
  </h1>
  <time datetime="2013-12-14T00:56:00Z" class="post-date">Sat, Dec 14, 2013</time>
  <p>Several months ago, I took a look at <a href="/post/sabr-with-the-new-hagan-pde-approach">two interesting recent ways</a> to price under SABR with no arbitrage:</p>
<ul>
<li>One way is due to <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;ei=F4yrUoL7Kq2M7AasuIFg&amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;sig2=-fFTrJR1wY1elyXBC1EC0A&amp;bvm=bv.57967247,d.ZGU">Andreasen and Huge</a>, where they find an equivalent local volatility expansion, and then use a one-step finite difference technique to price.</li>
<li>The other way is due to Hagan himself, where he numerically solves an approximate PDE in the probability density, and then price with options by integrating on this density.</li>
</ul>
<p>It turns out that the two ways are much closer than I first thought. Hagan PDE in the probability density is actually just the <a href="http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Planck</a> (forward) equation.
<figure><img src="/post/Screenshot%20from%202013-12-13%2023%2026%2043.png"/>
</figure>

The \(\alpha D(F)\) is just the equivalent local volatility. Andreasen and Huge use nearly the same local volatility formula but without the exponential part (that is often negligible except for long maturities), directly in Dupire forward PDE:
<figure><img src="/post/Screenshot%20from%202013-12-13%2023%2031%2018.png"/>
</figure>

A common derivation (for example in <a href="http://www.amazon.com/The-Volatility-Surface-Practitioners-Finance/dp/0471792519/ref=sr_1_1?ie=UTF8&amp;qid=1386974691&amp;sr=8-1&amp;keywords=Gatheral+the+volatility">Gatheral book</a> of the Dupire forward PDE is to actually use the Fokker-Planck equation in the probability density integral formula. Out of curiosity, I tried to price direcly with Dupire forward PDE and the Hagan local volatility formula, using just linear boundary conditions. Here are the results on Hagan own example:
<figure><img src="/post/hagan_dens1.png"/>
</figure>

<figure><img src="/post/hagan_iv.png"/>
</figure>

The Local Vol direct approach overlaps the Density approach nearly exactly, except at the high strike boundary, when it comes to probability density measure or to implied volatility smile. On Andreasen and Huge data, it gives the following:
<figure><img src="/post/ah_dens1.png"/>
</figure>

<figure><img src="/post/ah_iv1.png"/>
</figure>
</p>
<p>One can see that the one step method approximation gives the overall same shape of smile, but shifted, while the PDE, in local vol or density matches the Hagan formula at the money.</p>
<p>Hagan managed to derive a slightly more precise local volatility by going through the probability density route, and his paper formalizes his model in a clearer way: the probability density accumulates at the boundaries. But in practice, this formalism does not seem to matter. The forward Dupire way is more direct and slightly faster. This later way also allows to use alternative boundaries, like Andreasen-Huge did.</p>
<p><strong>Update March 2014</strong> - I have now a paper around this &ldquo;<a href="http://ssrn.com/abstract=2402001">Finite Difference Techniques for Arbitrage Free SABR</a>&rdquo;</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/american-option-on-forwardfutures/">American Option on Forward/Futures</a>
  </h1>
  <time datetime="2013-11-21T11:17:00Z" class="post-date">Thu, Nov 21, 2013</time>
  <p>Prices of a Future contract and a Forward contract are the same under the Black-Scholes assumptions (deterministic rates) but the price of options on Futures or options on Forwards might still differ. I did not find this obvious at first.</p>
<p>For example, when the underlying contract expiration date (Futures, Forward) is different from the option expiration date. For a Future Option, the Black-76 formula can be used, the discounting is done from the option expiry date, because one receives the cash on expiration due to the margin account. For a Forward Option, the discounting need to be done from the Forward contract expiry date.</p>
<p>European options prices will be the same when the underlying contract expiration date is the same as the option expiration date. However, this is not true for American options: the immediate exercise will need to be discounted to the Forward expiration date for a Forward underlying, not for a Future.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/">Spikes in Heston/Schobel-Zhu Local Volatility</a>
  </h1>
  <time datetime="2013-11-20T13:33:00Z" class="post-date">Wed, Nov 20, 2013</time>
  <p>Using precise vanilla option pricing engine for Heston or Schobel-Zhu, like the Cos method with enough points and a large enough truncation can still lead to spikes in the Dupire local volatility (using the variance based formula).</p>
<p><figure><img src="/post/Screenshot%20from%202013-11-20%2012%2053%2032.png"/><figcaption>
            <h4>Local volatility</h4>
        </figcaption>
</figure>
 <figure><img src="/post/Screenshot%20from%202013-11-20%2012%2054%2039.png"/><figcaption>
             <h4>Implied volatility</h4>
         </figcaption>
 </figure>
</p>
<p>The large spikes in the local volatility 3d surface are due to constant extrapolation, but there are spikes even way before the extrapolation takes place at longer maturities. Even if the Cos method is precise, it seems to be not enough, especially for large strikes so that the second derivative over the strike combined with the first derivative over time can strongly oscillate.</p>
<p>After wondering about possible solutions (using a spline on the implied volatilities), the root of the error was much simpler: I used a too small difference to compute the numerical derivatives (1E-6). Moving to 1E-4 was enough to restore a smooth local volatility surface.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/">Local Stochastic Volatility with Monte-Carlo</a>
  </h1>
  <time datetime="2013-10-16T16:14:00Z" class="post-date">Wed, Oct 16, 2013</time>
   

I always imagined local stochastic volatility to be complicated, and thought it would be very slow to calibrate.<br /><br />After reading a bit about it, I noticed that the calibration phase could just consist in calibrating independently a Dupire local volatility model and a stochastic volatility model the usual way.<br /><br />One can then choose to compute on the fly the local volatility component (not equal the Dupire one, but including the stochastic adjustment) in the Monte-Carlo simulation to price a product. <br /><br />There are two relatively simple algorithms that are remarkably similar, one by Guyon and Henry-Labordère in "<a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1885032">The Smile Calibration Problem Solved</a>":<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s1600/Screenshot+from+2013-10-16+15:51:46.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="157" src="http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s400/Screenshot+from+2013-10-16+15:51:46.png" width="400" /></a></div><br />And one from Van der Stoep, Grzelak &amp; Oosterlee "<a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fabstract%3D2278122&amp;ei=255eUqaEDMaxhAfdqoBI&amp;usg=AFQjCNF2KqSTT2ouvAyiA2J77foOFTzMKw&amp;sig2=fzb4vlDPp49Hp1oT5Wja4A&amp;bvm=bv.54176721,d.ZG4">The Heston Stochastic-Local Volatility Model: Efficient Monte Carlo Simulation</a>":<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s1600/Screenshot+from+2013-10-16+16:01:01.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="63" src="http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s400/Screenshot+from+2013-10-16+16:01:01.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s1600/Screenshot+from+2013-10-16+15:59:08.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="256" src="http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s400/Screenshot+from+2013-10-16+15:59:08.png" width="400" /></a></div><br />In the particle method, the delta function is a regularizing kernel approximating the Dirac function. If we use the notation of the second paper, we have a = psi.<br /><br />The methods are extremely similar, the evaluation of the expectation is slightly different, but even that is not very different. The disadvantage is that all paths are needed at each time step. As a payoff is evaluated over one full path, this is quite memory intensive.<br /><br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston-schobel-zhu-bates-double-heston-fit/">Heston, Schobel-Zhu, Bates, Double-Heston Fit</a>
  </h1>
  <time datetime="2013-10-07T19:35:00Z" class="post-date">Mon, Oct 7, 2013</time>
   

I did some experiments fitting Heston, Schobel-Zhu, Bates and Double-Heston to a real world equity index implied volatility surface. I used a global optimizer (differential evolution).<br /><br />To my surprise, the Heston fit is quite good: the implied volatility error is less than 0.42% on average. Schobel-Zhu fit is also good (0.47% RMSE), but a bit worse than Heston. Bates improves quite a bit on Heston although it has 3 more parameters, we can see the fit is better for short maturities (0.33% RMSE). Double-Heston has the best fit overall but it is not that much better than simple Heston while it has twice the number of parameters, that is 10 (0.24 RMSE). Going beyond, for example Triple-Heston, does not improve anything, and the optimization becomes very challenging. With Double-Heston, I already noticed that kappa is very low (and theta high) for one of the processes, and kappa is very high (and theta low) for the other process: so much that I had to add a penalty to enforce constraints in my local optimizer. The best fit is at the boundary for kappa and theta. So double Heston already feels over-parameterized.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-C_IidKxPcx4/UlKmCDJFLmI/AAAAAAAAG0o/PxOjIyGqdQQ/s1600/heston_cos3d.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="480" src="http://3.bp.blogspot.com/-C_IidKxPcx4/UlKmCDJFLmI/AAAAAAAAG0o/PxOjIyGqdQQ/s640/heston_cos3d.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Heston volatility error</td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-rvjEFAbCS_o/UlKmXZc--7I/AAAAAAAAG0s/yomyHTMJU18/s1600/schobelzhu_cos3d.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="480" src="http://3.bp.blogspot.com/-rvjEFAbCS_o/UlKmXZc--7I/AAAAAAAAG0s/yomyHTMJU18/s640/schobelzhu_cos3d.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Schobel-Zhu  volatility error</td></tr></tbody></table><br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"></div><br /><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-_628A9dt_nU/UlKmf4oWujI/AAAAAAAAG00/b8fU_hw_oOY/s1600/bates_cos3d.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="480" src="http://4.bp.blogspot.com/-_628A9dt_nU/UlKmf4oWujI/AAAAAAAAG00/b8fU_hw_oOY/s640/bates_cos3d.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Bates  volatility error</td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-ioz8CuCWNmU/UlKmonoZozI/AAAAAAAAG08/eOTk8KiqxD0/s1600/double_heston_cos3d.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="480" src="http://4.bp.blogspot.com/-ioz8CuCWNmU/UlKmonoZozI/AAAAAAAAG08/eOTk8KiqxD0/s640/double_heston_cos3d.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Double Heston  volatility error</td></tr></tbody></table><br /><br />Another advantage of Heston is that one can find tricks to find a good initial guess for a local optimizer.<br /><br /><b>Update October 7: </b>My initial fit relied only on differential evolution and was not the most stable as a result. Adding Levenberg-Marquardt at the end stabilizes the fit, and improves the fit a lot, especially for Bates and Double Heston. I updated the graphs and conclusions accordingly. Bates fit is not so bad at all.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/second-cumulant-of-heston/">Second Cumulant of Heston</a>
  </h1>
  <time datetime="2013-10-03T17:27:00Z" class="post-date">Thu, Oct 3, 2013</time>
  <p>I recently stumbled upon an error in the various papers related to the Heston <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CEEQFjAC&amp;url=http%3A%2F%2Fta.twi.tudelft.nl%2Fmf%2Fusers%2Foosterle%2Foosterlee%2FCOS.pdf&amp;ei=qYxNUqryFqXJ0QW5u4HYDA&amp;usg=AFQjCNGaMK8Lotud1DP5qReeLWgpoCA0aA&amp;sig2=fljLmTq0WhG8SCgzaOxXxA&amp;bvm=bv.53537100,d.d2k">Cos method</a> regarding the second cumulant. It is used to define the boundaries of the Cos method. Letting phi be Heston characteristic function, the cumulant generating function is:
$$g(u) = \log(\phi(-iu))$$</p>
<p>And the second cumulant is defined a:
$$c_2 = g&rsquo;&rsquo;(0)$$</p>
<p>Compared to a numerical implementation, the c_2 from the paper is really off in many use cases.</p>
<p>This is where <a href="/post/maxima-for-symbolic-calculus">Maxima</a> comes useful, even if I had to simplify the results by hand. It leads to the following analytical formula:
$$c_2 = \frac{v_0}{4\kappa^3}{ 4 \kappa^2 \left(1+(\rho\sigma t -1)e^{-\kappa t}\right) + \kappa \left(4\rho\sigma(e^{-\kappa t}-1)-2\sigma^2 t e^{-\kappa t}\right)+\sigma^2(1-e^{-2\kappa t}) }\\+ \frac{\theta}{8\kappa^3} { 8 \kappa^3 t - 8 \kappa^2 \left(1+ \rho\sigma t + (\rho\sigma t-1)e^{-\kappa t}\right) + 2\kappa \left( (1+2e^{-\kappa t})\sigma^2 t+8(1-e^{-\kappa t})\rho\sigma \right) \\+ \sigma^2(e^{-2\kappa t} + 4e^{-\kappa t}-5) }$$</p>
<p>In contrast, the paper formula was:
<figure><img src="/post/Screenshot%20from%202013-10-03%2017%2023%2028.png"/>
</figure>
</p>
<p>I saw this while trying to calibrate Heston on a bumped surface: the results were very different with the Cos method than with the other methods. The short maturities were mispriced, except if one pushed the truncation level L to 24 (instead of the usual 12), and as a result one would also need to significantly raise the number of points used in the Cos method. With the corrected formula, it works well with L=12.</p>
<p>Here is an example of failure on a call option of strike, spot 1.0 and maturity 1.0 and not-so-realistic Heston parameters $$\kappa=0.1, \theta=1.12, \sigma=1.0, v_0=0.2, \rho=-0377836$$ using 200 points:</p>
<table>
<thead>
<tr>
<th>Formula</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>New</td>
<td>0.1640581405</td>
</tr>
<tr>
<td>Paper</td>
<td>0.1743425406</td>
</tr>
<tr>
<td>N=30 with 10000 points</td>
<td>0.1640581423</td>
</tr>
</tbody>
</table>
<p><strong>Update March 2014</strong> - this is now described in my paper <a href="http://ssrn.com/abstract=2362968">Fourier Integration and Stochastic Volatility Calibration</a>.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/maxima-for-symbolic-calculus/">Maxima for Symbolic Calculus</a>
  </h1>
  <time datetime="2013-10-02T15:06:00Z" class="post-date">Wed, Oct 2, 2013</time>
  <p>A few years ago, I found an interesting open source symbolic calculus software called <a href="http://www-fourier.ujf-grenoble.fr/~parisse/giac.html%E2%80%8E">Xcas</a>. It can however be quickly limited, for example, it does not seem to work well to compute Taylor expansions with several embedded functions. Google pointed me to another popular open source package, <a href="http://maxima.sourceforge.net/">Maxima</a>. It looks a bit rudimentary (command like interface), but formulas can actually be very easily exported to latex with the tex command. Here is a simple example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>(%i14) D(x):=sqrt((lambda-rho*eta*x)^2+(-x^2+x)*eta^2);
</span></span><span style="display:flex;"><span>(%i15) G(x) := (lambda - rho*eta*x - D(x))/(lambda - rho*eta*x +D(x));
</span></span><span style="display:flex;"><span>(%i16) tex(taylor((1-exp(-t*D(x)))/(1-G(x)*exp(-t*D(x)))*(lambda - rho*eta*x - D(x)),x,0,3));</span></span></code></pre></div>
<p>$$-{{\left(e^{t,\lambda}-1\right),\eta^2,x}\over{2,e^{t,\lambda},\lambda}}+{{\left(\left(4,e^{t,\lambda},t,\eta^3,\rho+\left(4,\left(e^{t,\lambda}\right)^2-4,e^{t,\lambda}\right),\eta^2\right),\lambda^2+\left(\left(-4,\left(e^{t,\lambda}\right)^2+4,e^{t,\lambda}\right),\eta^3,\rho-2,e^{t,\lambda},t,\eta^4\right),\lambda+\left(\left(e^{t,\lambda}\right)^2-1\right),\eta^4\right),x^2}\over{8,\left(e^{t,\lambda}\right)^2,\lambda^3}}+{{\left(\left(8,\left(e^{t,\lambda}\right)^2,t^2,\eta^4,\rho^2-16,\left(e^{t,\lambda}\right)^2,t,\eta^3,\rho\right),\lambda^4+\left(16,\left(e^{t,\lambda}\right)^2,t,\eta^4,\rho^2+\left(-8,\left(e^{t,\lambda}\right)^2,t^2,\eta^5+\left(16,\left(e^{t,\lambda}\right)^3-16,\left(e^{t,\lambda}\right)^2\right),\eta^3\right),\rho+16,\left(e^{t,\lambda}\right)^2,t,\eta^4\right),\lambda^3+\left(\left(-16,\left(e^{t,\lambda}\right)^3+16,\left(e^{t,\lambda}\right)^2\right),\eta^4,\rho^2+\left(-16,\left(e^{t,\lambda}\right)^2-8,e^{t,\lambda}\right),t,\eta^5,\rho+2,\left(e^{t,\lambda}\right)^2,t^2,\eta^6+\left(-8,\left(e^{t,\lambda}\right)^3+8,e^{t,\lambda}\right),\eta^4\right),\lambda^2+\left(\left(12,\left(e^{t,\lambda}\right)^3-12,e^{t,\lambda}\right),\eta^5,\rho+\left(2,\left(e^{t,\lambda}\right)^2+4,e^{t,\lambda}\right),t,\eta^6\right),\lambda+\left(-2,\left(e^{t,\lambda}\right)^3-\left(e^{t,\lambda}\right)^2+2,e^{t,\lambda}+1\right),\eta^6\right),x^3}\over{32,\left(e^{t,\lambda}\right)^3,\lambda^5}}+\cdots $$
Regarding Taylor expansion, there seems to be quite a few options possible, but I found that the default expansion was already relatively easy to read. XCas produced less readable expansions, or just failed.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/martin-odersky-teaches-scala-to-the-masses/">Martin Odersky teaches Scala to the Masses</a>
  </h1>
  <time datetime="2013-09-17T20:11:00Z" class="post-date">Tue, Sep 17, 2013</time>
  <p>I tried today the <a href="https://www.coursera.org/course/progfun">Scala courses on coursera</a> by the Scala creator, Martin Odersky. I was quite impressed by the quality: I somehow believed Scala to be only a better Java, now I think otherwise. Throughout the course, even though it all sounds very basic, you understand the key concepts of Scala and why functional programming + OO concepts are a natural idea. What&rsquo;s nice about Scala is that it avoids the functional vs OO or even the functional vs procedural debate by allowing both, because both can be important, at different scales. Small details can be (and probably should be) procedural for efficiency, because a processor is a processor, but higher level should probably be more functional (immutable) to be clearer, easier to evolve and more easily parallelized.</p>
<p>I recently saw a very good example at work recently of how mutability could be very problematic, with no gain in this case because it was high level (and likely just the result of being too used to OO concepts).</p>
<p>I believe it will make my code more functional programming oriented in the future, especially at the high level.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/setting-values-in-java-enum---a-bad-idea/">Setting Values in Java Enum - A Bad Idea</a>
  </h1>
  <time datetime="2013-09-12T10:06:00Z" class="post-date">Thu, Sep 12, 2013</time>
   

My Scala habits have made me create a stupid bug related to Java enums. In Scala, the concept of <a href="http://www.scala-lang.org/old/node/107">case classes</a> is very neat and recently, I just confused enum in Java with what I sometimes do in Scala case classes.<br /><br />I wrote an enum with a setter like:<br /><br />&nbsp;<span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;&nbsp; public static enum BlackVariateType {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; V0,<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ZERO_DERIVATIVE;<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; private double volSquare;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; public double getBlackVolatilitySquare() {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; return volSquare;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; public void setBlackVolatilitySquare(double volSquare) {<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; this.volSquare = volSquare;<br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; }</span></span><br /><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;&nbsp; }</span></span><br /><br />Here, calling setBlackVolatilitySquare will override any previous value, and thus, if several parts are calling it with different values, it will be a mess as there is only a single instance.<br /><br />I am not sure if there is actually one good use case to have a setter on an enum. This sounds like a very dangerous practice in general. Member variables allowed should be only final. <br /><br />



  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/16/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/18/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
