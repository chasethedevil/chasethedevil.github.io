<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.123.7">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 60618. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/vol_monotonicity_in_practice/">Monotonicity of the Black-Scholes Option Prices in Practice</a>
  </h1>
  <time datetime="2024-09-29T12:56:42&#43;0100" class="post-date">Sun, Sep 29, 2024</time>
  <p>It is well known that vanilla option prices must increase when we increase the implied volatility. Recently, a post on the Wilmott forums wondered about the true accuracy of Peter Jaeckel implied volatility solver, whether it was truely IEEE 754 compliant. In fact, the author noticed some inaccuracy in the option price itself. Unfortunately I can not reply to the forum, its login process does not seem to be working anymore, and so I am left to blog about it.</p>
<p>It can be quite challenging to evaluate the accuracy. For example, one potential error that the author makes is to use numbers that are not exactly representable in IEEE 754 standard such as 4.45 or 0.04. In Julia, and I believe in Python mpmath as well, there is a difference between a BigFloat(4.45) and a BigFloat(&ldquo;4.45&rdquo;):</p>
<p><code>BigFloat(4.45)  = 4.45000000000000017763568394002504646778106689453125 BigFloat(&quot;4.45&quot;)= 4.449999999999999999999999999999999999999999999999999999999999999999999999999972</code></p>
<p>If we work only on usual 64-bit floats, we likely want to estimate the accuracy of an implementation using BigFloat(4.45) as input rather than BigFloat(&ldquo;4.45&rdquo;). To compute a vanilla option price, the naive way would be through the cumulative normal distribution function (the textbook way), which looks like</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">function</span> bs(strike, forward, voltte)
</span></span><span style="display:flex;"><span>    d1 <span style="color:#666">=</span> (log(forward<span style="color:#666">/</span>strike)<span style="color:#666">+</span>voltte<span style="color:#666">^</span><span style="color:#40a070">2</span> <span style="color:#666">/</span> <span style="color:#40a070">2</span>)<span style="color:#666">/</span>voltte
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">return</span> forward<span style="color:#666">*</span>cdf(Normal(),d1)<span style="color:#666">-</span>strike<span style="color:#666">*</span>cdf(Normal(),d1<span style="color:#666">-</span>voltte)
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">end</span></span></span></code></pre></div>
<p>It turns out that this produces relatively high error for out of the money prices. In fact, the prices are far from being monotonic if we increase the vol by 1 ulp a hundred times we obtain:
<figure><img src="/post/bs_price.png"/>
</figure>
</p>
<p>Peter Jaeckel proposes several improvements, the first simple one is to use the scaled complementary error function <code>erfcx</code> rather than the cumulative normal distribution function directly. A second improvement is to use Taylor expansions for small prices. It turns out that on the example with strike K=2.0, forward F=1.0, vol=0.125, the Taylor expansion is being used.</p>
<figure><img src="/post/bs_price_jaeckel.png"/>
</figure>

<p>The error is much lower, with Taylor being even a little bit better. But the price is still not monotonic.</p>
<p>How would the implied vol corresponding to those prices look like? If we apply Peter Jaeckel implied vol solver to the naive Black-Scholes formula, we end up with an accuracy limited by the naive formula.
<figure><img src="/post/bs_vol_naive.png"/>
</figure>
</p>
<p>If we apply it to the exact prices or to erfcx or to Taylor, we still end up with a non monotonic implied vol:</p>
<figure><img src="/post/bs_vol_jaeckel.png"/>
</figure>

<p>There is no visible difference in accuracy if we start from the erfcx prices and apply the SR Householder solver of <a href="https://github.com/jherekhealy/AQFED.jl/tree/master/src/black">Jherek Healy</a>:</p>
<figure><img src="/post/bs_vol_erfcx.png"/>
</figure>

<p>It should not be too surprising: if we compose the Black-Scholes formula with the associated solver from Peter Jaeckel, we do not obtain the identity function. Because the option prices are not monotonic with the vol, there are several vols which may be solution to the same price, and the solver is guaranteed to not solve exactly, which leads to the numerical noise we see on the plots. In contrast, it is remarkable that if we compose exp with log we have a nice monotonic shape:</p>
<figure><img src="/post/logexp.png"/>
</figure>

<p>The prerequisite for a more accurate solver would be a Black-Scholes function which is actually monotonic.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/copilot_vs_chatgpt_optimal_step_size/">Copilot vs ChatGPT on the Optimal Finite Difference Step-Size</a>
  </h1>
  <time datetime="2024-07-25T12:56:42&#43;0100" class="post-date">Thu, Jul 25, 2024</time>
  <p>When computing the derivative of a function by finite difference, which step size is optimal? The answer depends on the kind of difference (forward, backward or central), and the degree of the derivative (first or second typically for finance).</p>
<p>For the first derivative, the result is very quick to find (it&rsquo;s on <a href="https://en.wikipedia.org/wiki/Numerical_differentiation">wikipedia</a>). For the second derivative, it&rsquo;s more challenging. The <a href="https://paulklein.ca/newsite/teaching/Notes_NumericalDifferentiation.pdf">Lecture Notes</a> of Karen Kopecky provide an answer. I wonder if Copilot or ChatGPT would find a good solution to the question:</p>
<blockquote>
<p>&ldquo;What is the optimal step size to compute the second derivative of a function by centered numerical differentiation?&rdquo;</p>
</blockquote>
<p>Here is the answer of copilot:
<figure><img src="/post/step_size_copilot.png"/>
</figure>

and the one of chatGPT:
<figure><img src="/post/step_size_chatgpt1.png"/>
</figure>

and a second answer from chatGPT:
<figure><img src="/post/step_size_chatgpt2.png"/>
</figure>
</p>
<p>Copilot always fails to provide a valid answer. ChatGPT v4 proves to be quite impressive: it is able to reproduce some of the reasoning presented in the lecture notes.</p>
<p>Interestingly, with centered difference, for a given accuracy, the optimal step size is different for the first and for the second derivative. It may, at first, seem like a good idea to use a different size for each. In reality, when both the first and second derivative of the function are needed (for example the Delta and Gamma greeks of a financial product), it is rarely a good idea to use a different step size for each. Firstly, it will be slower since more the function will be evaluated at more points. Secondly, if there is a discontinuity in the function or in its derivatives, the first derivative may be estimated without going over the discontinuity while the second derivative may be estimated going over the discontinuity, leading to puzzling results.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/cos_method_truncation/">News on the COS Method Truncation</a>
  </h1>
  <time datetime="2024-05-13T20:56:42&#43;0100" class="post-date">Mon, May 13, 2024</time>
  <p>The COS method is a fast way to price vanilla European options under stochastic volatility models with a known characteristic function. There are alternatives, explored in <a href="/post/the-cos-method-for-heston/">previous</a> <a href="/post/attari-lord-kahl--cos-methods-comparison-on-heston/">blog</a> <a href="/post/making-classic-heston-integration-faster-than-the-cos-method/">posts</a>. A main advantage of the COS method is its simplicity. But this comes at the expense of finding the correct values for the truncation level and the (associated) number of terms.</p>
<p>A related issue of the COS method, or its more fancy wavelet cousin the SWIFT method, is to require a huge (&gt;65K) number of points to reach a reasonable accuracy for some somewhat extreme choices of Heston parameters. I provide an example in <a href="https://arxiv.org/abs/2401.01758">a recent paper</a> (see Section 5).</p>
<p>Gero Junike recently wrote <a href="https://arxiv.org/abs/2109.01030">several</a> <a href="https://arxiv.org/abs/2303.16012">papers</a> on how to find good estimates for those two parameters. Gero derives a slightly different formula for the put option, by centering the distribution on \( \mathbb{E}[\ln S] \). It is closer to my own <a href="https://arxiv.org/abs/2005.13248">improved COS formula</a>, where I center the integration on the forward. The estimate for the truncation is larger than the one we are used to (for example using the estimate based on 4 cumulants of Mike Staunton), and the number of points is very conservative.</p>
<p>The bigger issue with this new estimate, is that it relies on an integration of a function of the characteristic function, very much like the original problem we are trying to solve (the price of a vanilla option). This is in order to estimate the \( ||f^{(20)}||_{\infty} \). Interestingly, evaluating this integral is not always trivial, the <a href="https://github.com/machakann/DoubleExponentialFormulas.jl">double exponential quadrature in Julia</a> fails. I found that reusing the transform from \( (0,\infty) \) to (-1,1) of Andersen and Lake along with a Gauss-Legendre quadrature on 128 points seemed to be ok (at least for some values of the Heston parameters, it may be fragile, not sure).</p>
<p>While very conservative, it seems to produce the desired accuracy on the extreme example mentioned in the paper, it leads to N=756467 points and a upper truncation at b=402.6 for a relative tolerance of 1E-4. Of course, on such examples the COS method is not fast anymore. For comparison, the <a href="https://fbe.unimelb.edu.au/__data/assets/pdf_file/0008/2591783/223.pdf">Joshi-Yang technique</a> with 128 points produces the same accuracy in 235 μs as the COS method in 395 ms on this example, that is a factor of 1000 (on many other examples the COS method behaves significantly better of course).</p>
<p>Furthermore, as stated in Gero Junike&rsquo;s paper, the estimate fails for less smooth distributions such as the one of the Variance Gamma (VG) model.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/unrealistic_variance_swaps_under_schobel_zhu/">Variance Swap Term-Structure under Schobel-Zhu</a>
  </h1>
  <time datetime="2024-03-26T12:56:42&#43;0100" class="post-date">Tue, Mar 26, 2024</time>
  <p>I never paid too much attention to it, but the term-structure of variance swaps is not always realistic under the Schobel-Zhu stochastic volatility model.</p>
<p>This is not fundamentally the case with the Heston model, the Heston model is merely extremely limited to produce either a flat shape or a downward sloping exponential shape.</p>
<p>Under the Schobel-Zhu model, the price of a newly issued variance swap reads
$$	V(T) = \left[\left(v_0-\theta\right)^2-\frac{\eta^2}{2\kappa}\right]\frac{1-e^{-2\kappa T}}{2\kappa T}+2\theta(v_0-\theta)\frac{1-e^{-\kappa T}}{\kappa T}+\theta^2+\frac{\eta^2}{2\kappa},,$$
where \( \eta \) is the vol of vol.</p>
<p>When \( T \to \infty \), we have \( V(T) \to \theta^2 + \frac{\eta^2}{2\kappa} \).
Unless \( \kappa \) is very large, or the vol of vol is very small, the second term will often dominate. In plain words, the prices of long-term variance swaps are almost purely dictated by the vol of vol when the speed of mean reversion is lower than 1.0.</p>
<p>Below is an example of fit to the market. If the vol of vol and kappa are exogeneous and we calibrate only the initial vol v0, plus the long term vol theta to the term-structure of variance swaps, then we end up with upward shapes for the term-structure, regardless of theta. Only when we add the vol of vol to the calibration, we find a reasonable solution. The solution is however not really better than the one corresponding to the Heston fit with only 2 parameters. It thus looks overparameterized.</p>
<figure><img src="/post/varswap_sz_russel.png"/><figcaption>
            <h4>Term-structure of variance swap prices on Russell 2000 index</h4>
        </figcaption>
</figure>

<p>There are some cases where the Schobel-Zhu model allows for a better fit than Heston, and makes use of the flexibility due to the vol of vol.</p>
<figure><img src="/post/varswap_sz_spx.png"/><figcaption>
            <h4>Term-structure of variance swap prices on SPX 500 index</h4>
        </figcaption>
</figure>

<p>It is awkward that the variance swap term-structure depends on the vol of vol in the Schobel-Zhu model.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/new_basket_approximation_and_cash_dividends/">New Basket Expansions and Cash Dividends</a>
  </h1>
  <time datetime="2024-03-23T09:56:42&#43;0100" class="post-date">Sat, Mar 23, 2024</time>
  <p>In the <a href="/post/new_asian_approximation/">previous post</a>, I presented a <a href="https://arxiv.org/abs/2106.12971">new stochastic expansion</a> for the prices of Asian options. The stochastic expansion is generalized to basket options in the paper, and <a href="https://arxiv.org/abs/2106.12971">can thus be applied</a> on the problem of pricing vanilla options with cash dividends.</p>
<p>I have updated the paper with comparisons to more direct stochastic expansions for pricing vanilla options with cash dividends, such as the one of <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1687590">Etoré and Gobet</a>, and <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283">my own refinement</a> on it.</p>
<p>The second and third order basket expansions turn out to be significantly more accurate than the previous, more direct stochastic expansions, even on extreme examples.</p>
<figure><img src="/post/basket_expansion_pln_zhang.png"/><figcaption>
            <h4>Extreme case with large volatility (80% on 1 year) and two unrealistically large dividends of 25 (spot=100). VGn and VLn are stochastic expansions of order n using two different proxies. EG3 and LL3 are the third order expansions of Etore-Gobet and Le Floc&#39;h. Deelstra is the refined Curran moment matching technique.</h4>
        </figcaption>
</figure>

<figure><img src="/post/basket_expansion_pln_gs.png"/><figcaption>
            <h4>Long maturity (10y) with dividend (amount=2) every six month. VGn and VLn are stochastic expansions of order n using two different proxies. EG3 and LL3 are the third order expansions of Etore-Gobet and Le Floc&#39;h. Deelstra is the refined Curran moment matching technique.</h4>
        </figcaption>
</figure>


  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/new_asian_approximation/">New Approximations for the Prices of Asian and basket Options</a>
  </h1>
  <time datetime="2024-03-17T12:56:42&#43;0100" class="post-date">Sun, Mar 17, 2024</time>
  <p>Many years ago, I had <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2698283">applied the stochastic expansion</a> technique of <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1687590">Etore and Gobet</a> to a refined proxy, in order to produce more accurate prices for vanilla options with cash dividends under the Black-Scholes model with deterministic jumps at the dividend dates. Any approximation for vanilla basket option prices can also be applied on this problem, and the sophisticated Curran geometric conditioning was found to be particularly competitive in <a href="https://arxiv.org/abs/2106.12971">The Pricing of Vanilla Options with Cash Dividends as a Classic Vanilla Basket Option Problem</a>.</p>
<p>Recently, I had the idea of <a href="https://arxiv.org/abs/2402.17684">applying the same stochastic expansion technique to the prices of Asian options</a>, and more generally to the prices of vanilla basket options. It works surprising well for Asian options. A main advantage is that the expansion is straightforward to implement: there is no numerical solving or numerical quadrature needed.</p>
<figure><img src="/post/asian_expansion_vg3.png"/><figcaption>
            <h4>VGn and VLn are stochastic expansions of order n using two different proxies</h4>
        </figcaption>
</figure>

<p>It works a little bit less well for basket options. Even though I found a better proxy for those, the expansions behave less well with large volatility (really, large total variance), regardless of the proxy. I notice now this was also true for the case of discrete dividends where, clearly the accuracy deteriorates somewhat significantly in &ldquo;extreme&rdquo; examples such as a vol of 80% for an option maturity of 1 year (see <a href="https://arxiv.org/pdf/2106.12971.pdf">Figure 3</a>).</p>
<p>I did not compare the use of the direct stochastic expansion for discrete dividends, and the one going through the vanilla basket expansion, maybe for a next post.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/logeuler_not_exact/">Easy Mistake With the Log-Euler Discretization On Black-Scholes</a>
  </h1>
  <time datetime="2024-03-11T20:56:42&#43;0100" class="post-date">Mon, Mar 11, 2024</time>
  <p>In the Black-Scholes model with a term-structure of volatilities, the Log-Euler Monte-Carlo scheme is not necessarily exact.</p>
<p>This happens if you have two assets \(S_1\) and \(S_2\), with two different time varying volatilities \(\sigma_1(t), \sigma_2(t) \). The covariance from the Ito isometry from \(t=t_0\) to \(t=t_1\) reads $$ \int_{t_0}^{t_1} \sigma_1(s)\sigma_2(s) \rho ds, $$ while a naive log-Euler discretization may use
$$ \rho  \bar\sigma_1(t_0) \bar\sigma_2(t_0)  (t_1-t_0). $$
In practice, the \( \bar\sigma_i(t_0) \) are calibrated such that the vanilla option prices are exact, meaning
$$ \bar{\sigma}_i^2(t_0)(t_1-t_0) = \int_{t_0}^{t_1} \sigma_i^2(s) ds.$$</p>
<p>As such the covariance of the log-Euler scheme does not match the covariance from the Ito isometry unless the volatilities are constant in time. This means that the prices of European vanilla basket options is going to be slightly off, even though they are not path dependent.</p>
<p>The fix is of course to use the true covariance matrix between \(t_0\) and \(t_1\).</p>
<p>The issue may also happen if instead of using the square root of the covariance matrix in the Monte-Carlo discretization, the square root of the correlation matrix is used.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/roughness_of_jumps/">Roughness of Pure Jumps</a>
  </h1>
  <time datetime="2023-12-18T20:56:42&#43;0100" class="post-date">Mon, Dec 18, 2023</time>
  <p>In my previous <a href="https://chasethedevil.github.io/post/roughness_of_stochastic_volatility/">blog post</a>, I looked at the roughness of the SVCJ stochastic volatility model with jumps (in the volatility). In this model, the jumps occur randomly, but at discrete times. And with typical parameters used in the litterature, the jumps are not so frequent. It is thus more interesting to look at the roughness of pure jump processes, such as the <a href="https://engineering.nyu.edu/sites/default/files/2018-09/CarrJournalofBusiness2002.pdf">CGMY process</a>.</p>
<p>The CGMY process is more challenging to simulate. I used the approach based on the characteristic function described in <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1983134">Simulating Levy Processes from Their Characteristic Functions and Financial Applications</a>. Ballota and Kyriakou add some variations based on FFT pricing of the characteristic function in <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1951537">Monte Carlo simulation of the CGMY process and option pricing</a> and pay much care about a proper truncation range. Indeed, I found that the truncation range was key to simulate the process properly and not always trivial to set up especially for \(Y \in (0,1) \). I however did not  implement any automated range guess as I am merely interested in very specific use cases, and I used the COS method instead of FFT.</p>
<p>I also directly analyze the roughness of a sub-sampled asset path (the exponential of the CGMY process), and not of some volatility path as I was curious if pure jump processes would mislead roughness estimators. I simulated the paths corresponding to parameters given in Fang thesis <a href="https://repository.tudelft.nl/islandora/object/uuid%3A9aa17357-af21-4c09-86a2-3904ced4b873">The COS Method - An Efficient Fourier Method for Pricing Financial Derivatives</a>: C = 1, G = 5, M = 5, Y = 0.5 or Y = 1.5.</p>
<figure><img src="/post/cgmy_path.png"/>
</figure>

<p>And the corresponding Hurst index estimate via the Cont-Das method:
<figure><img src="/post/cgmy_estimate.png"/>
</figure>
</p>
<p>Similarly the Gatheral-Rosenbaum estimate has no issue finding H=0.5.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/roughness_of_stochastic_volatility/">Roughness of Stochastic Volatility with Jumps</a>
  </h1>
  <time datetime="2023-12-07T20:56:42&#43;0100" class="post-date">Thu, Dec 7, 2023</time>
  <p>I was wondering if adding jumps to stochastic volatility, as is done in the SVCJ model of Duffie, Singleton and Pan <em>&ldquo;Transform Analysis and Asset Pricing for Affine Jump-Diffusion&rdquo;</em> also in Broadie and Kaya <em>&ldquo;Exact simulation of stochastic volatility and other affine jump diffusion processes&rdquo;</em>, would lead to rougher paths, or if it would mislead the roughness estimators.</p>
<p>The answer to the first question can almost be answered visually:
<figure><img src="/post/svcj_variance_path.png"/>
</figure>
</p>
<p>The parameters used are the one from Broadie and Kaya: v0=0.007569, kappa=3.46, theta=0.008, rho=-0.82, sigma=0.14 (Heston), jump correlation -0.38, jump intensity 0.47, jump vol 0.0001, jump mean 0.05, jump drift -0.1.</p>
<p>The Rough Heston with H=0.1 is much &ldquo;noisier&rdquo;. There is not apparent difference between SVCJ and Heston in the path of the variance.</p>
<p>The estimator of Cont and Das (on a subsampled path) leads to a Hurst exponent H=0.503, in line with a standard Brownian motion.
<figure><img src="/post/svcj_estimate.png"/><figcaption>
            <h4>Cont-Das estimate H=0.503.</h4>
        </figcaption>
</figure>
</p>
<p>The estimator from Rosenbaum and Gatheral leads to a Hurst exponent (slope of the regression) H=0.520 with well behaved regressions:
<figure><img src="/post/svcj_estimate_gatheral_1.png"/><figcaption>
            <h4>Regressions for each q.</h4>
        </figcaption>
</figure>

<figure><img src="/post/svcj_estimate_gatheral_2.png"/><figcaption>
            <h4>Regression over all qs which leads to the estimate of H.</h4>
        </figcaption>
</figure>
</p>
<p>On this example, there are relatively few jumps during the 1 year duration. If we multiply the jump intensity by 1000 and reduce the jump mean accordingly, the conclusions are the same. Jumps and roughness are fundamentally different.</p>
<p>Of course this does not mean that the short term realized volatility does not look rough as evidenced in Cont and Das paper:
<figure><img src="/post/svcj_rv.png"/><figcaption>
            <h4>The 1 hour realized volatility looks rough.</h4>
        </figcaption>
</figure>

I computed the realized volatility on a subsampled path, using disjoint windows of 1h of length.</p>
<p>It is not really rough, estimators will have a tough time leading to stable estimates on it.
<figure><img src="/post/svcj_rv_estimate.png"/><figcaption>
            <h4>Hurst exponent estimation based on the 1h realized variance. The mean H=0.054 but is clearly not reliable.</h4>
        </figcaption>
</figure>
</p>
<p>This is very visible with the Rosenbaum-Gatheral way of estimating H, we see that the observations do not fall on a line at all but flatten:
<figure><img src="/post/svcj_rv_estimate_gatheral_1.png"/><figcaption>
            <h4>Regressions for each q based on the 1h realized variance.</h4>
        </figcaption>
</figure>
</p>
<p>The pure Heston model leads to similar observations.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/measuring_roughness_with_julia/">Measuring Roughness with Julia</a>
  </h1>
  <time datetime="2023-11-07T20:56:42&#43;0100" class="post-date">Tue, Nov 7, 2023</time>
  <p>I received a few e-mails asking me for the code I used to measure roughness in my preprint on the <a href="/post/implied_volatility_roughness">roughness of the implied volatility</a>. Unfortunately, the code I wrote for this paper is not in a <em>good</em> state, it&rsquo;s all in one long file line by line, not necessarily in order of execution, with comments that are only meaningful to myself.</p>
<p>In this post I will present the code relevant to measuring the oxford man institute roughness with Julia. I won&rsquo;t go into generating Heston or rough volatility model implied volatilities, and focus only on the measure on roughness based on some CSV like input. I downloaded the oxfordmanrealizedvolatilityindices.csv from the Oxford Man Institute website (unfortunately now discontinued, data bought by Refinitiv but still available in some github repos) to my home directory</p>
<p><div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-julia" data-lang="julia"><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">using</span> DataFrames, CSV, Statistics, Plots, StatsPlots, Dates, TimeZones 
</span></span><span style="display:flex;"><span>df <span style="color:#666">=</span> DataFrame(CSV<span style="color:#666">.</span>File(<span style="color:#4070a0">&#34;/home/fabien/Downloads/oxfordmanrealizedvolatilityindices.csv&#34;</span>))
</span></span><span style="display:flex;"><span>df1 <span style="color:#666">=</span>  df[df<span style="color:#666">.</span><span style="color:#902000">Symbol</span> <span style="color:#666">.==</span> <span style="color:#4070a0">&#34;.SPX&#34;</span>,<span style="color:#666">:</span>]
</span></span><span style="display:flex;"><span>dsize <span style="color:#666">=</span>  trunc(<span style="color:#902000">Int</span>,length(df1<span style="color:#666">.</span>close_time)<span style="color:#666">/</span><span style="color:#40a070">1.0</span>)
</span></span><span style="display:flex;"><span>tm <span style="color:#666">=</span> [abs((Date(ZonedDateTime(<span style="color:#902000">String</span>(d),<span style="color:#4070a0">&#34;y-m-d H:M:S+z&#34;</span>))<span style="color:#666">-</span>Date(ZonedDateTime(<span style="color:#902000">String</span>(dfv<span style="color:#666">.</span>Column1[<span style="color:#40a070">1</span>]),<span style="color:#4070a0">&#34;y-m-d H:M:S+z&#34;</span>)))<span style="color:#666">.</span>value) <span style="color:#007020;font-weight:bold">for</span> d <span style="color:#007020;font-weight:bold">in</span> dfv<span style="color:#666">.</span>Column1[<span style="color:#666">:</span>]];
</span></span><span style="display:flex;"><span>ivm <span style="color:#666">=</span> dfv<span style="color:#666">.</span>rv5[<span style="color:#666">:</span>]
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">using</span> Roots, Statistics
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">function</span> wStatA(ts, vs, K0,L,step,p)
</span></span><span style="display:flex;"><span>    bvs <span style="color:#666">=</span> vs <span style="color:#60a0b0;font-style:italic"># big.(vs)</span>
</span></span><span style="display:flex;"><span>    bts <span style="color:#666">=</span> ts <span style="color:#60a0b0;font-style:italic"># big.(ts)</span>
</span></span><span style="display:flex;"><span>    value <span style="color:#666">=</span> sum( abs(log(bvs[k<span style="color:#666">+</span>K0])<span style="color:#666">-</span>log(bvs[k]))<span style="color:#666">^</span>p <span style="color:#666">/</span> sum(abs(log(bvs[l<span style="color:#666">+</span>step])<span style="color:#666">-</span>log(bvs[l]))<span style="color:#666">^</span>p <span style="color:#007020;font-weight:bold">for</span> l <span style="color:#007020;font-weight:bold">in</span> k<span style="color:#666">:</span>step<span style="color:#517918">:k</span><span style="color:#666">+</span>K0<span style="color:#666">-</span>step) <span style="color:#666">*</span> abs((bts[k<span style="color:#666">+</span>K0]<span style="color:#666">-</span>bts[k])) <span style="color:#007020;font-weight:bold">for</span> k<span style="color:#666">=</span><span style="color:#40a070">1</span><span style="color:#666">:</span>K0<span style="color:#666">:</span>L<span style="color:#666">-</span>K0<span style="color:#666">+</span><span style="color:#40a070">1</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">return</span> value
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">end</span>
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">function</span> meanRoughness(tm, ivm, K0, L)
</span></span><span style="display:flex;"><span>    cm <span style="color:#666">=</span> zeros(length(tm)<span style="color:#666">-</span>L);
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">for</span> i <span style="color:#666">=</span> <span style="color:#40a070">1</span><span style="color:#666">:</span>length(cm)
</span></span><span style="display:flex;"><span>        <span style="color:#007020;font-weight:bold">local</span> ivi <span style="color:#666">=</span> ivm[i<span style="color:#666">:</span>i<span style="color:#666">+</span>L]
</span></span><span style="display:flex;"><span>        <span style="color:#007020;font-weight:bold">local</span> ti <span style="color:#666">=</span> tm[i<span style="color:#666">:</span>i<span style="color:#666">+</span>L]
</span></span><span style="display:flex;"><span>        T <span style="color:#666">=</span> abs((ti[<span style="color:#007020;font-weight:bold">end</span>]<span style="color:#666">-</span>ti[<span style="color:#40a070">1</span>]))
</span></span><span style="display:flex;"><span>        <span style="color:#007020;font-weight:bold">try</span>
</span></span><span style="display:flex;"><span>            cm[i] <span style="color:#666">=</span> <span style="color:#40a070">1.0</span> <span style="color:#666">/</span>  find_zero(p <span style="color:#666">-&gt;</span> wStatA(ti, ivi, K0, L, <span style="color:#40a070">1</span>,p)<span style="color:#666">-</span>T,(<span style="color:#40a070">1.0</span>,<span style="color:#40a070">100.0</span>))
</span></span><span style="display:flex;"><span>        <span style="color:#007020;font-weight:bold">catch</span> e
</span></span><span style="display:flex;"><span>            <span style="color:#007020;font-weight:bold">if</span> <span style="color:#007020;font-weight:bold">isa</span>(e, <span style="color:#902000">ArgumentError</span>)
</span></span><span style="display:flex;"><span>                cm[i] <span style="color:#666">=</span> <span style="color:#40a070">0.0</span>
</span></span><span style="display:flex;"><span>            <span style="color:#007020;font-weight:bold">else</span>
</span></span><span style="display:flex;"><span>                throw(e)
</span></span><span style="display:flex;"><span>            <span style="color:#007020;font-weight:bold">end</span>
</span></span><span style="display:flex;"><span>        <span style="color:#007020;font-weight:bold">end</span>
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">end</span>
</span></span><span style="display:flex;"><span>    meanValue <span style="color:#666">=</span> mean(filter( <span style="color:#007020;font-weight:bold">function</span>(x) x <span style="color:#666">&gt;</span> <span style="color:#40a070">0</span> <span style="color:#007020;font-weight:bold">end</span>,cm))
</span></span><span style="display:flex;"><span>    stdValue <span style="color:#666">=</span> std(filter( <span style="color:#007020;font-weight:bold">function</span>(x) x <span style="color:#666">&gt;</span> <span style="color:#40a070">0</span> <span style="color:#007020;font-weight:bold">end</span>,cm))
</span></span><span style="display:flex;"><span>    <span style="color:#007020;font-weight:bold">return</span> meanValue, stdValue, cm
</span></span><span style="display:flex;"><span><span style="color:#007020;font-weight:bold">end</span>
</span></span><span style="display:flex;"><span>meanValue, stdValue, cm <span style="color:#666">=</span> meanRoughness(tm, ivm, K0,K0<span style="color:#666">^</span><span style="color:#40a070">2</span>)
</span></span><span style="display:flex;"><span>density(cm,label<span style="color:#666">=</span><span style="color:#4070a0">&#34;H&#34;</span>,ylabel<span style="color:#666">=</span><span style="color:#4070a0">&#34;Density&#34;</span>)</span></span></code></pre></div>
The last plot should look like
<figure><img src="/post/oxford_spx500_roughness.png"/>
</figure>

It may be slightly different, depending on the date (and thus the number of observations) of the CSV file (the ones I found on github are not as recent as the one I used initially in the paper).</p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/3/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
