<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>Chase the Devil &middot; Chase the Devil</title>

  
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/poole.css">
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/poole-overrides.css">
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/hyde-overrides.css">
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/hyde-x.css">
  <link rel="stylesheet" href="http://chasethedevil.github.io/css/highlight/sunburst.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://chasethedevil.github.io/touch-icon-144-precomposed.png">
  <link href="http://chasethedevil.github.io/favicon.png" rel="icon">

  
  
  
  <link href="http://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil &middot; Chase the Devil" />

  <meta name="description" content="">
  <meta name="keywords" content="">
  <link rel="author" href="http://plus.google.com/105444622997415581493">
<script type="text/javascript"
  src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<link href=' http://fonts.googleapis.com/css?family=UnifrakturMaguntia' rel='stylesheet' type='text/css'>
</head>
<body class="theme-base-00">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      
      <h1>Chase the Devil</h1>
      <p class="lead">Technical blog for Fabien.</p>
    </div>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item"><a href="http://chasethedevil.github.io/">Blog</a></li>
      
      <li class="sidebar-nav-item"><a href="http://chasethedevil.github.io/about/">About</a></li>
      
      <li class="sidebar-nav-item"><a href="http://chasethedevil.github.io/post/">Posts</a></li>
      
    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>  
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="http://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>

    

    
  </div>
</div>


<div class="content container">
  <div class="posts">
    
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/filon_for_heston/">Adaptive Filon quadrature for stochastic volatility models</a>
      </h1>
      <span class="post-date">May 12, 2016 &middot; 1 minute read
      
      <br/>
      <a class="label" href="http://chasethedevil.github.io/categories/quant">quant</a>
      </span>
      
      <p>A while ago, I have applied a relatively simple adaptive Filon quadrature to the problem of <a href="http://papers.ssrn.com/abstract=2620166">volatility swap pricing</a>. The <a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0ahUKEwi18brlh9XMAhWIVhoKHdnGA2UQFggdMAA&amp;url=http%3A%2F%2Fwww.ams.org%2Fmcom%2F1968-22-101%2FS0025-5718-1968-0225485-5%2FS0025-5718-1968-0225485-5.pdf&amp;usg=AFQjCNEQvSMm6vOaXIX2MqAJ-GQt79QRiA&amp;sig2=HLHd-rc74qnCuo5yp1Q13A">Filon quadrature</a> is an old quadrature from 1928 that allows to integrate oscillatory integrand like \(f(x)\cos(k x) \) or \(f(x)\sin(k x) \). It turns out that combined with an adaptive Simpson like method, it has many advantages over more generic adaptive quadrature methods like Gauss-Lobatto, which is often used on similar problems.</p>

<p><a href="http://dl.acm.org/citation.cfm?id=321029">Flinn</a> derived a similar quadrature, but taking into account the derivative values, which are likely to be easily available on those problems. This produces a even more robust quadrature and of higher convergence order.</p>

<p>I was curious how practical this would be on Heston or other stochastic volatility models with a known characteristic function. It turns out it produces a very competitive performance-wise and very stable option pricing method: it can be up to <strong>five times faster</strong> than an adaptive Gauss-Lobatto within a calibration. I found it faster and more robust than the Cos method (especially as it is quite tricky to guess properly the truncation of the Cos method).</p>

<p>If you are interested, you will find much more details in <a href="/lefloch_heston_filon.pdf">this document</a>.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/rational_fit/">Least Squares Rational Function</a>
      </h1>
      <span class="post-date">Apr 21, 2016 &middot; 4 minute read
      
      <br/>
      <a class="label" href="http://chasethedevil.github.io/categories/quant">quant</a>
      </span>
      
      <p>In my paper <a href="http://ssrn.com/abstract=2420757">&ldquo;Fast and Accurate Analytic Basis Point Volatility&rdquo;</a>,
I use a table of Chebyshev polynomials to provide an accurate representation of some function. This is
an idea I first saw in the <a href="http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package">Faddeeva package</a> to
represent the cumulative normal distribution with high accuracy, and high performance. It is also
simple to find out the Chebyshev polynomials, and which intervals are the most appropriate for those, which
makes this technique quite appealing.</p>

<p>Still, it would have been nice to have also a more visually appealing rational function representation, as <a href="http://www.kcl.ac.uk/nms/depts/mathematics/research/finmath/publications/2007Shaw.pdf">W. Shaw</a>
did for the cumulative normal distribution (again). Popular algorithms to find the best rational function representation
seem to be minimax or Remez. But I could not find an open-source library where those were implemented. There is an
interesting implementation in <a href="http://www.chebfun.org">chebfun</a> but this depends on Matlab.</p>

<p>The Numerical recipe book provides a simple algorithm in <a href="http://www.aip.de/groups/soe/local/numres/bookcpdf/c5-13.pdf">chapter 5.13</a>, not looking for the best possible rational function, but just for one
that would be &ldquo;good enough&rdquo;. Interestingly, the first part of the algorithm merely computes a least squares solution
on some chebyshev like nodes. I however quickly noticed funny behaviors with the code: it could produce a worse fit
for a higher order numerator or denominator. Then I tried some <a href="http://www.scientificpython.net/pyblog/rational-least-squares-approximation">least squares python code</a> which ended up being just buggy:
I am not sure what the code actually does with all the numpy and scipy magic, it gives solutions with poles in the data, and clearly not the least squares solution. I can&rsquo;t fully understand why one would propose such a code.</p>

<p>It turns out that I had an alternative very basic least squares polynomial fit implementation, which is based on <a href="http://math.stackexchange.com/questions/924482/least-squares-regression-matrix-for-rational-functions">this matrix representation</a>.
I wondered if it would be as prone to errors as Numerical recipe code (where they use SVD internally to solve).</p>

<p>The answer is: it depends. It depends on the solver used. If I use a QR solver, then the implementation looks robust on my test data,
much more than Numerical recipe code. If I use LU, it just fails in some cases. If I use SVD, it is sometimes better sometimes worse than Numerical recipe.</p>

<p>Interestingly, I thought, that, maybe, a <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a> could allow to regain the lost accuracy with SVD, using
as starting point, the SVD guess. However, it does not work, it just converges more or less to the same (bad) solution.</p>

<p>Another interesting point, is that the using QR decomposition (instead of SVD)
in the Numerical recipe implementation resulted in a much better solution, better even than my basic least squares fit,
which looked robust, but was actually not so much.</p>

<p>Armed with this improved Numerical recipe code (labeled NRI), here is a comparison of fit of my naive code (with QR) against the improved numerical recipe
(NRI) for a polynomial of degree 20. We can visually see the difference (when we zoom)!</p>


<figure >
    
        <img src="/post/rational_fit_value.png" />
    
    
    <figcaption>
        <h4>Least squares polynomial fit of degree 20 zoomed.</h4>
        
    </figcaption>
    
</figure>


<p>The RMSE difference on the full interval [0,1] on 1000 equidistant points is huge:</p>

<table>
<thead>
<tr>
<th>Method</th>
<th>RMSE</th>
</tr>
</thead>

<tbody>
<tr>
<td>Polynomial Naive</td>
<td>0.234</td>
</tr>

<tr>
<td>Polynomial NRI</td>
<td>0.039</td>
</tr>

<tr>
<td>Rational   NRI</td>
<td>0.001</td>
</tr>
</tbody>
</table>

<p>A <sup>10</sup>&frasl;<sub>10</sub> rational function (numerator of degree 10, denominator of degree 10) gives a much better fit than a polynomial of degree 20. It is interesting to look at the error
visually to understand how large is the difference:</p>


<figure >
    
        <img src="/post/rational_fit_error.png" />
    
    
    <figcaption>
        <h4>Least squares polynomial fit error of degree 20.</h4>
        
    </figcaption>
    
</figure>


<p>I still can draw a conclusion to my quest for a rational function approximation: I won&rsquo;t find a good one with the
change of variables I am using in my paper, as I would imagine the least squares solution to be at worst around one order of magnitudes
 away from the minimax. The errors I got suggest I would need a few rational functions, maybe 3 or more, and then it does not look
all that appealing compared to the table of Chebyshev polynomials.</p>

<p>I thought this was a good example of how relatively simple numerical tasks can be challenging in practice.</p>

<p><strong>Update</strong> - The paper now contains a solution for the normal volatility inversion problem with only 3 rational functions.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/least_squares_spline/">Least Squares Spline for Volatility Interpolation</a>
      </h1>
      <span class="post-date">Feb 19, 2016 &middot; 1 minute read
      
      <br/>
      <a class="label" href="http://chasethedevil.github.io/categories/quant">quant</a>
      </span>
      
      <p>I am experimenting a bit with least squares splines. Existing algorithms (for example from the NSWC Fortran library) usually work
with B-splines, a relatively simple explanation of how it works is given in <a href="http://www.geometrictools.com/Documentation/BSplineCurveLeastSquaresFit.pdf">this paper</a> (I think this is how De Boor coded it in the NSWC library).
Interestingly there is <a href="http://educ.jmu.edu/~lucassk/Papers/Spline3.pdf">an equivalent formulation in terms of standard cubic splines</a>, although it seems that the
pseudo code on that paper has errors.</p>

<p>Least squares splines give a very good fit for option implied volatilities with only a few parameters.
In theory, the number of parameters is N+2 where N is the number of interpolation points.
I tried on some of my AAPL 1 month option chain, with only 3 points (so 2 splines or 5 free parameters).</p>


<figure >
    
        <img src="/post/least_squares_spline.png" />
    
    
    <figcaption>
        <h4>least squares spline on 1m AAPL options.</h4>
        
    </figcaption>
    
</figure>


<p>It would be interesting to add the natural constraints so that it can be linearly extrapolated. Maybe for next time.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/mystic_parabola/">The Mystic Parabola</a>
      </h1>
      <span class="post-date">Feb 16, 2016 &middot; 2 minute read
      
      <br/>
      <a class="label" href="http://chasethedevil.github.io/categories/quant">quant</a>
      </span>
      
      <p>I recently had some fun trying to work directly with the option chain from the <a href="http://www.nasdaq.com/symbol/aapl/option-chain">Nasdaq website</a>.
The data there is quite noisy, but a simple parabola can still give an amazing fit. I will consider the options of maturity two years as illustration.
I also relied on a simple implied volatility algorithm that can be summarized in the following steps:</p>

<ul>
<li>Compute a rough guess for the forward price by using interest, borrow curves and by extrapolating the dividends.</li>
<li>Imply the forward from the European Put-Call parity relationship on the mid prices of the two strikes closes to the rough forward guess. A simple linear interpolation between the two strikes can be used to compute the forward.</li>
<li>Compute the Black implied volatilities as if the option were European using P. Jaeckel algorithm.</li>
<li>Calibrate the proportional dividend amount or the growth rate by minimizing, for example with a Levenberg-Marquardt minimizer, the difference between model and mid-option prices corresponding to the three strikes closest to the forward. The parameters in this case are effectively the dividend amount and the volatilities for Put and Call options (the same volatility is used for both options). The initial guess stems directly from the two previous steps. American option prices are computed by the finite difference method.</li>
<li>Solve numerically the volatilities one by one with the TOMS748 algorithm so that the model prices match the market mid out-of-the-money option prices.</li>
</ul>

<p>Then I just fit a least squares parabola in variance on log-moneyness, using options trading volumes as weights and obtain the following figure:</p>


<figure >
    
        <img src="/post/mystic_parabola.png" />
    
    
    <figcaption>
        <h4>least squares parabola on 2y AAPL options.</h4>
        
    </figcaption>
    
</figure>


<p>Isn&rsquo;t the fit just amazing?
Even if I found it surprising, it&rsquo;s probably not so surprising. The curve has to be smooth, somewhat monotone, and will be therefore like a parabola near the money. While there is no guarantee it will fit that well far away, it&rsquo;s actually a matter of scale. Short maturities will lead to not so great fit in the wings, while long maturities will correspond to a narrower range of scaled strikes and match better a parabola.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/yahoo_finance_implied_volatility/">Yahoo Finance Implied Volatility</a>
      </h1>
      <span class="post-date">Feb 3, 2016 &middot; 1 minute read
      
      <br/>
      
      </span>
      
      <p>The <a href="https://finance.yahoo.com/q/op?s=GOOG&amp;date=1457049600">option chain</a> on Yahoo finance shows an implied volatility number for each call or put option in the last column.
I was wondering a bit how they computed that number. I did not exactly find out their methodology, especially since we don&rsquo;t even know the daycount convention used, but
I did find that it was likely just garbage.</p>

<p>A red-herring is for example the large discrepancy between put vols and call vols. For example strike 670, call vol=50%, put vol=32%.
This suggests that the two are completely decoupled, and they use some wrong forward (spot price?) to obtain those numbers. If I compute
the implied volatilities using put-call parity close to the money to find out the implied forward price, I end up with ask vols of 37% and 34% or call and put mid vols of 33%.
By considering the put-call parity, I assume European option prices, which is not correct in this case. It turns out however, that with the low interest rates we live in, there is nearly zero additional value due to the American early exercise.</p>

<p>I am not sure what use people can have of Yahoo implied volatilities.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/is_tufte_overrated/">Is Tufte overrated?</a>
      </h1>
      <span class="post-date">Feb 3, 2016 &middot; 2 minute read
      
      <br/>
      
      </span>
      
      <p><a href="http://www.edwardtufte.com/tufte/">Tufte</a> proposes interesting guidelines to present data, or even to design written semi-scientific papers or books. Some advices
are particularly relevant like the careful use of colors (don&rsquo;t use all the colors of the rainbow just because you can), and
in general don&rsquo;t add lines in a graph or designs that are not directly relevant to the message that needs to be conveyed. There is also a parallel
with Feynman message against (Nasa) <a href="http://www.zdnet.com/article/death-by-powerpoint/">Powerpoint presentations</a>. But other inspirations, are somewhat doubtful.
He seems to have a fetish for <a href="http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0000hB">old texts</a>. They might be considered pretty, or interesting in some ways, but
I don&rsquo;t find them particularly easy to read. They look more like esoteric books rather than practical books. If you want to write
the new Bible for your new cult, it&rsquo;s probably great, not so sure it&rsquo;s so great for a more simple subject.
Also somewhat surprisingly, his own website is not very well designed, it looks like a maze and very end of 90s.</p>

<p>I experimented a bit with the <a href="https://tufte-latex.github.io/tufte-latex/">Tufte latex template</a>. It produced <a href="http://papers.ssrn.com/sol3/Delivery.cfm/SSRN_ID2712316_code1514784.pdf?abstractid=2711720&amp;mirid=1">that document</a> for example. But someone rightfully pointed out to me
that the reference style was not really elegant, that it did not look like your typical nice science paper references. Furthermore,
 using superscript so much could be annoying for someone used to read math and consider superscript numbers as math symbols.
In general, there seems to be a conflict between the use of Latex and many Tufte guidelines:
Latex does not encourage you to lay out one by one each piece,
something the good old desktop publishing software allow you to do quite well.</p>

<p>I was also wondering a bit on what design to use for a book. And I realised that the best layout to consider is simply the layout
of a book I enjoyed to read. For example, I like the recent SIAM book design, I find that it gives enough space to read the text
and the maths without having the impression of deciphering some codex, and without headache. It turns out there is even a <a href="http://www.siam.org/books/authors/p_handbook8.php">latex template</a> available.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/linear_flat_forward_interpolation/">Linear and Flat forward interpolation with cash dividends</a>
      </h1>
      <span class="post-date">Jan 19, 2016 &middot; 2 minute read
      
      <br/>
      
      </span>
      
      <p>When the dividend curve is built from discrete cash dividends, the dividend yield is discontinuous at the dividend time as the asset price jumps from the dividend amount.
This can be particularly problematic for numerical schemes like finite difference methods. In deed, a finite difference grid
will make use of the forward yield (eventually adjusted to the discretisation scheme), which explodes then.
Typically, if one is not careful about this, then increasing the number of time steps does not increase accuracy anymore, as
the spike just becomes bigger on a smaller time interval. A simple work-around is to limit the resolution to one day.
This means that intraday, we interpolate the dividend yield.</p>

<p>If we simply interpolate the yields linearly intraday, then the yield becomes continuous again, and numerical schemes will work much better.
But if we take a look at the actual curve of &ldquo;forward&rdquo; yields, it becomes sawtooth shaped!

<figure >
    
        <img src="/post/linear_flat_forward.png" />
    
    
    <figcaption>
        <h4>effective forward drift used in the finite difference grid with 4 time-steps per day</h4>
        
    </figcaption>
    
</figure>

On the above figure, we can see the Dirac like forward yield if we work with the direct formulas, while interpolating intraday allows to smooth out the initial Dirac overall the interval corresponding to 1-day.</p>

<p>In reality, one should use flat forward interpolation instead, where the forward yield is maintained constant during the day. The forward rate is defined as
<div>$$f(t_0,t_1)= \frac{r(t_1) t_1 -r(t_0) t_0}{t_1-t_0}$$</div>
where the continuously compounded rate \(r\) is defined so that \(Z(0,t)= e^{-r(t)t}\).
In the case of the Black-Scholes drift, the drift rate is defined so that the forward price (not to confuse with the forward rate) \(F(0,t)= e^{-q(t)t}\).</p>

<p>The flat forward interpolation is equivalent to a linear interpolation on the logarithm of discount factors.
In ACT/365, let \(t_0=\max\left(0,\frac{365}{\left\lceil 365 t \right\rceil-1}\right), t_1 = \frac{365}{\left\lceil 365 t \right\rceil}\), the interpolated yield is:
<div>$$\bar{q}(0,t)t = q(t_0)t_0\frac{t_1-t}{t_1-t_0} + q(t_1)t_1\frac{t-t_0}{t_1-t_0}\text{.}$$</div>
Another work-around would be to model this via proportional dividends instead of a &ldquo;continuous&rdquo; yield curve.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/moved-to-hugo/">Moved to hugo</a>
      </h1>
      <span class="post-date">Dec 20, 2015 &middot; 3 minute read
      
      <br/>
      
      </span>
      
      <p>I moved my blog from blogger to <a href="https://gohugo.io/">Hugo</a>. Blogger really did not evolve since Google take-over in 2003. Wordpress is today much nicer and prettier. It&rsquo;s clear that Google did not invest at all, possibly because blogs are passé. Compared to mid 2000, there are very few blogs today. Even programming blogs are scarce. It could be interesting to quantify this. My theory is that it is the direct consequence of the popularity of social networks, and especially facebook (possibly also stackoverflow for programmers): people don&rsquo;t have time anymore to write as their extra-time is used on social networks. Similarly I noticed that almost nobody comments anymore to the point that even Disqus is very rarely used, and again I attribute that to the popularity of sites like reddit. This is why I did not bother with a comment section on my blog, just email me or tweet about it instead.</p>

<p>I was always attracted by the static web sites concept, because there is actually very little things that ought to be truely dynamic from a individual point of view. Dynamic hosting also tends to be problematic in the long-run, for example I never found the time to upgrade my chord search engine to the newer Google appengine and now it&rsquo;s just off. I used to freeze my personal website (created with a dynamic templating tool Velocity, django, etc.) with a python script. So a static blog was the next logical step, and these days, it&rsquo;s quite popular. Static blogs put the author fully in control of the content and its presentation. <a href="http://jekyllrb.com">Jekyll</a> started the trend along with github allowing good old personal websites. It offers a modern looking blog, with very little configuration steps. I tried Hugo instead because it&rsquo;s written in <a href="http://golang.com">the Go language</a>. It&rsquo;s much faster, but I don&rsquo;t really care about that for something of the size of my blog. I was curious however how good was the Go language on real world projects, and I knew I could always customize it if I ever needed to. Interestingly, I did stumble on a few panics (core dump equivalent where the program just crashes, in this case the hugo local server), something that does not happen with Java based tools or even with Ruby or Python based tools. Even though I like the Go language more and more (I am doing some pet project with it - I believe in the focus on fast compilation and simple language), I found this a bit alarming. This is clearly a result of the errors versus exceptions choice, as it&rsquo;s up to the programmer to handle the errors properly and not panic unnecessarily (I even wonder if it makes any sense to panic for a server).</p>

<p>Anyway I think it looks better now, maybe a bit too minimalist. I&rsquo;ll add details when I have more time.</p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/controlling-the-sabr-wings-with-hagan-pde/">Controlling the SABR wings with Hagan PDE </a>
      </h1>
      <span class="post-date">Dec 15, 2015 &middot; 2 minute read
      </span>
      
      <p>On the <a href="http://www.wilmott.com/messageview.cfm?catid=4&amp;threadid=78001&amp;FTVAR_MSGDBTABLE=&amp;STARTPAGE=4">Wilmott forum</a>, Pat Hagan has recently suggested to cap the equivalent local volatility in order to control the wings and better match CMS prices. It also helps making the SABR approximation better behaved as the expansion is only valid when<br /><div>$$ 1 + 2\frac{\rho\nu}{\alpha}y(K)+\frac{\nu^2}{\alpha^2}y^2(K) $$</div><div>is close to 1.&nbsp;</div><div><br /><div>In the PDE approach (especially the non transformed one), it is very simple, one just needs to update the equivalent local vol as&nbsp;</div></div><div>$$\alpha K^\beta \min\left(M, \sqrt{1 + 2\frac{\rho\nu}{\alpha}y(K)+\frac{\nu^2}{\alpha^2}y^2(K)}\right)$$</div><div><br /></div><div>While it is straightforward to include in the PDE, it is more difficult to derive a good approximation. The zero-th order behaves as expected, but the first order formula has a unnatural kink, likely because of the non differentiability due to the min function.&nbsp;</div><div><br /></div><div>The following graphs presents the non capped PDE, the capped PDE with M=4*nu (PDEC4) and M=6*nu (PDEC6) as well as the approximation (Andersen Ratcliffe / Gatheral first order) where I have only taken care of the right wing. The SABR parameters are alpha = 0.0630, beta = 0.7, rho = -0.363, nu = 0.421, T = 10, f = 0.0439.</div><div><br /></div><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-7N928DyGhHY/Vm_ibyWlVcI/AAAAAAAAIP0/YF7Mfcpm4w4/s1600/Screenshot%2Bfrom%2B2015-12-15%2B10-25-16.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="400" src="http://1.bp.blogspot.com/-7N928DyGhHY/Vm_ibyWlVcI/AAAAAAAAIP0/YF7Mfcpm4w4/s400/Screenshot%2Bfrom%2B2015-12-15%2B10-25-16.png" width="385" /></a></div><div><br /></div><div>We can see that the higher the cap is, the closer we are to the standard SABR PDE, and the lower the cap is, the flatter are the wings.<br /><br />The approximation matches well ATM (it is then equivalent to standard SABR PDE) but then has a discontinuous derivative for the K that reaches the threshold M. Far away, it matches very well again.</div></p>

      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="http://chasethedevil.github.io/post/broken-internet/">Broken Internet?</a>
      </h1>
      <span class="post-date">Nov 9, 2015 &middot; 1 minute read
      </span>
      
      <p>There is something funny going on with upcoming generic top level domains (gTLDs), they seem to be looked up in a strange manner (at least on latest Linux). For example:<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">ping chrome&nbsp;</span><br /><br />or<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">ping nexus&nbsp;</span><br /><br />returns 127.0.53.53.<br /><br />While existing <a href="https://www.name.com/new-gtld">official gTLD</a>s don&rsquo;t (<span style="font-family: &quot;Courier New&quot;,Courier,monospace;">ping dental</span> returns &ldquo;unknown host&rdquo; as expected). I first thought it was a network misconfiguration, but as <a href="https://groups.google.com/forum/#!msg/public-dns-discuss/bzhTQnFqE6I/E9F46xhka98J">I am not the only one to notice this</a>, it&rsquo;s likely a genuine internet issue.<br /><br />Strange times.</p>

      
    </div>
    
    
    
    <ul class="pagination">
        
        <li>
            <a href="/" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
        </li>
        
        <li
        >
        <a href="/" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
        </li>
        
        <li
        ><a href="/">1</a></li>
        
        <li
        class="active"><a href="/page/2/">2</a></li>
        
        <li
        ><a href="/page/3/">3</a></li>
        
        <li
        ><a href="/page/4/">4</a></li>
        
        <li
        ><a href="/page/5/">5</a></li>
        
        <li
        ><a href="/page/6/">6</a></li>
        
        <li
        ><a href="/page/7/">7</a></li>
        
        <li
        ><a href="/page/8/">8</a></li>
        
        <li
        ><a href="/page/9/">9</a></li>
        
        <li
        ><a href="/page/10/">10</a></li>
        
        <li
        ><a href="/page/11/">11</a></li>
        
        <li
        ><a href="/page/12/">12</a></li>
        
        <li
        ><a href="/page/13/">13</a></li>
        
        <li
        ><a href="/page/14/">14</a></li>
        
        <li
        ><a href="/page/15/">15</a></li>
        
        <li
        ><a href="/page/16/">16</a></li>
        
        <li
        ><a href="/page/17/">17</a></li>
        
        <li
        ><a href="/page/18/">18</a></li>
        
        <li
        ><a href="/page/19/">19</a></li>
        
        <li
        ><a href="/page/20/">20</a></li>
        
        <li
        ><a href="/page/21/">21</a></li>
        
        <li
        ><a href="/page/22/">22</a></li>
        
        <li
        ><a href="/page/23/">23</a></li>
        
        <li
        ><a href="/page/24/">24</a></li>
        
        <li
        ><a href="/page/25/">25</a></li>
        
        <li
        ><a href="/page/26/">26</a></li>
        
        <li
        ><a href="/page/27/">27</a></li>
        
        <li
        ><a href="/page/28/">28</a></li>
        
        <li
        ><a href="/page/29/">29</a></li>
        
        <li
        ><a href="/page/30/">30</a></li>
        
        <li
        ><a href="/page/31/">31</a></li>
        
        <li
        ><a href="/page/32/">32</a></li>
        
        <li
        ><a href="/page/33/">33</a></li>
        
        <li
        ><a href="/page/34/">34</a></li>
        
        <li
        ><a href="/page/35/">35</a></li>
        
        <li
        >
        <a href="/page/3/" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
        </li>
        
        <li>
            <a href="/page/35/" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
        </li>
        
    </ul>
    
  </div>
</div>


<div class="content container" style="padding-top: 0rem;">
<a href="//www.reddit.com/submit" onclick="window.location = '//www.reddit.com/submit?url=' + encodeURIComponent(window.location); return false"> <img src="//www.redditstatic.com/spreddit7.gif" alt="submit to reddit" border="0" /> </a>
<a href="https://twitter.com/share" class="twitter-share-button"{count} data-hashtags="chasethedevil">Tweet</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</div>
<script src="http://chasethedevil.github.io/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<script>
  var _gaq=[['_setAccount','UA-365717-1'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>

</body>
</html>

