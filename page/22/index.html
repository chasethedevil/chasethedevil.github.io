<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.93.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2022. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/gnome-3-unity-crap/">Gnome 3, Unity, Crap</a>
  </h1>
  <time datetime="2011-08-17T19:09:00Z" class="post-date">Wed, Aug 17, 2011</time>
  <p>After the upgrate to Ubuntu 11.04 I was directly on Unity. Having a dock on the left side is nice, but unfortunately, it has various bugs which makes it sometimes annoying. Also the menu on top like Mac Os X is not a bad idea, but it breaks many apps (for example Picasa). Then there is the scrollbar insanity, it&rsquo;s almost impossible to click on to scroll, and again breaks in some apps (for example Eclipse). Fortunately one can disable all of that and go back to the old Gnome 2.</p>
<p>Then I decided to try Gnome 3 the past 2 weeks. I was curious that Linus found it so bad. At first it looked nice, probably because it&rsquo;s different. But very quickly, it becomes annoying: the full screen changed just to switch window with the mouse is a stupid idea. And then installing Gnome 3 in Ubuntu is not such a great idea: Rhythmbox does not work well anymore with it, all my &ldquo;file open&rdquo; mapping was gone and replaced with garbage: why does it propose GIMP to open a PDF is beyond me.</p>
<p>Gnome 2 + a nice standard dock + widgets (or the nice unity indicators) + an indexer would have been perfect. Now I am back to KDE 4 which provides all of that, even if it feels a bit bloated.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/carmack--gpgpu-programming/">Carmack &amp; GPGPU programming</a>
  </h1>
  <time datetime="2011-08-14T10:20:00Z" class="post-date">Sun, Aug 14, 2011</time>
  <p>Finally someone who shares the same opinion on the current state of GPGPU programming.</p>
<p>John Carmack: On the other hand, we have converted all of our offline processing stuff to ray tracing. For years, the back-end MegaTexture generation for Rage was done with&hellip; we had a GPGPU cluster with NVIDIA cards and it was such a huge pain to keep. It was an amazing pain where one system would be having heat problems and would be behaving weird even though we thought they had identical drivers. Something would always be wrong with render farm 12, and whenever we wanted to put in new features it was like “Okay, writing new fragment programs to go into this.” Now, granted I did this just when CUDA was in its infancy. If I did re-implement it with OpenCL or CUDA we wouldn’t have some of these problems, but when I converted all these over to ray tracing there was a number of things that got a lot better. Things that we deal with, for example shadows and reflections that have to be approximated, and were so used to doing with rasterization&hellip; we sometimes forget how big of hacks these are. To be able to say I really just want that ray, and tell me what it hit; not do a projection with feathering shadowed edges and whatever the heck else we’re doing there, so much of the code got so much easier. If it’s a choice of&hellip; now that we have these awesome multi-core x86 CPUs where we can get 24 threads in commodity boxes&hellip; it’s true that one GPU card can do more ray tracing than one 24 thread x86 box, but it’s not multiples more and if it’s just a matter of buying more $2000 boxes, it makes the development, maintenance, and upkeep much better. While everyone in high performance computing is all “rah-rah” GPUs right now, I’ve come full circle back around to saying the fact that we can get massive amounts of x86 cores and threads&hellip; it wont win on FLOPS/watt or FLOPS/volume, but in terms of results per developer hour it is much, much better.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/expylogx-much-faster-than-math.powxy/">exp(y*log(x)) Much Faster than Math.pow(x,y)</a>
  </h1>
  <time datetime="2011-04-08T23:03:00Z" class="post-date">Fri, Apr 8, 2011</time>
  <p>Today I found out that replacing <em>Math.pow(x,y)</em> by <em>Math.exp(y</em>Math.log(x))* made me gain 50% performance in my program. Of course, both x and y are double in my case. I find this quite surprising, I expected better from Math.pow.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/best-headphones-ever/">Best Headphones Ever?</a>
  </h1>
  <time datetime="2011-04-06T13:08:00Z" class="post-date">Wed, Apr 6, 2011</time>
  <p>I just miraculously received some AKG Q701 headphones. I never tried really high end headphones before those. I was used to my Sennheiser HD555, which I thought were quite good already.</p>
<p>I always thought there was not much difference between let&rsquo;s say a $100 headphone and a $500 one, and that only real audiophiles would be able notice the difference: people like Jake who can distinguish artefacts in 320kbps mp3, but not people like me. I was wrong. Those headphones make a real difference, everything seems very clear, and it doesn&rsquo;t feel like listening music through headphones at all, it is much closer to a good hi-fi stereo sound on the ear.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/simd-and-mersenne-twister/">SIMD and Mersenne-Twister</a>
  </h1>
  <time datetime="2011-02-05T13:18:00Z" class="post-date">Sat, Feb 5, 2011</time>
  <p>Since 2007, there is a new kind of Mersenne-Twister (MT) that exploits SIMD architecture, the <a href="http://www.math.sci.hiroshima-u.ac.jp/%7Em-mat/MT/SFMT/">SFMT</a>. The Mersenne-Twister has set quite a standard in random number generation for Monte-Carlo simulations, even though it has flaws.</p>
<p>I was wondering if SFMT improved the performance over MT for a Java implementation. There is actually on the same page a decent Java port of the original algorithm. When I ran it, it ended up slower by more than 20% than the classical Mersenne-Twister (32-bit) on a 64-bit JDK 1.6.0.23 for Windows.</p>
<p>This kind of result is not too unsurprising as the code is more complex. But this shows that Java is unable to use SIMD instructions properly, which is still a bit disappointing.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/xorwow-lecuyer-testu01-results/">XORWOW L&#39;ecuyer TestU01 Results</a>
  </h1>
  <time datetime="2011-01-12T20:26:00Z" class="post-date">Wed, Jan 12, 2011</time>
   

<div style="font-family: &quot;Courier New&quot;,Courier,monospace;"><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;">Nvidia uses XorWow random number generator in its CURAND library. It is a simple and fast random number generator with a reasonably long period. It can also be parallelized relatively easily. Nvidia suggests it passes L'Ecuyer TestU01, but is not very explicit about it. So I've decided to see how it performed on TestU01.</div><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;"><br /></div><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;">I found very simple to test a new random number generator on TestU01, the documentation is great and the examples helpful. Basically there is just a simple C file to create &amp; compile &amp; run.</div><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;"><br /></div><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;">XORWOW passes the SmallCrush battery, and according to Marsaglia, it also passes DieHard. But it fails on 1 test in Crush and 3 in BigCrush. By comparison, Mersenne-Twister fails on 2 tests in Crush and 2 in BigCrush. Here are the results of the failures:</div><div style="font-family: Georgia,&quot;Times New Roman&quot;,serif;"><br /></div><span style="font-family: inherit;"><br /></span></div><div style="font-family: &quot;Courier New&quot;,Courier,monospace;">========= Summary results of Crush =========<br /><br />&nbsp;Version:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TestU01 1.2.3<br />&nbsp;Generator:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Xorwow<br />&nbsp;Number of statistics:&nbsp; 144<br />&nbsp;Total CPU time:&nbsp;&nbsp; 00:50:15.92<br />&nbsp;The following tests gave p-values outside [0.001, 0.9990]:<br />&nbsp;(eps&nbsp; means a value &lt; 1.0e-300):<br />&nbsp;(eps1 means a value &lt; 1.0e-15):<br /><br />&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Test&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p-value<br />&nbsp;----------------------------------------------<br />&nbsp;72&nbsp; LinearComp, r = 29&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - eps1<br />&nbsp;----------------------------------------------<br />&nbsp;All other tests were passed</div><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">========= Summary results of BigCrush =========</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;Version:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; TestU01 1.2.3</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;Generator:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Xorwow</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;Number of statistics:&nbsp; 160</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;Total CPU time:&nbsp;&nbsp; 06:12:38.79</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;The following tests gave p-values outside [0.001, 0.9990]:</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;(eps&nbsp; means a value &lt; 1.0e-300):</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;(eps1 means a value &lt; 1.0e-15):</span><br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Test&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; p-value</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;----------------------------------------------</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp; 7&nbsp; CollisionOver, t = 7&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.6e-6</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;27&nbsp; SimpPoker, r = 27&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; eps&nbsp; </span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;81&nbsp; LinearComp, r = 29&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1 - eps1</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;----------------------------------------------</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">&nbsp;All other tests were passed</span>



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/the-cuda-performance-myth/">The CUDA Performance Myth</a>
  </h1>
  <time datetime="2011-01-03T16:07:00Z" class="post-date">Mon, Jan 3, 2011</time>
  <p>There is an <a href="http://arxiv.org/PS_cache/arxiv/pdf/0901/0901.0638v4.pdf">interesting</a> article on how to generate efficiently the inverse of the normal cumulative distribution on the GPU. This is useful for Monte-Carlo simulations based on normally distributed variables.</p>
<p>Another result of the paper is a method (breakless algorithm) to compute it apparently faster than the very good <a href="http://www.mth.kcl.ac.uk/~shaww/web_page/papers/Wichura.pdf">Wichura&rsquo;s AS241</a> algorithm on the CPU as well keeping a similar precision. The key is to avoid branches (if-then) at the cost of not avoiding log() calls. As the algorithm is very simple, I decided to give it a try in Java.</p>
<p>Unfortunately I found out that in Java, on 64 bit machines, the breakless algorithm is actually around twice slower. Intrigued, I tried in Visual C++ the same, and found this time it was 1.5x slower. Then I tried in gcc and found that it was 10% faster&hellip; But the total time was significant, because I had not applied any optimization in the gcc compilation. With -O3 flag AS241 became much faster and we were back at the Java result: the breakless algorithm was twice slower. The first result is that the JITed java code is as fast as optimized C++ compiled code. The second result is that the authors did not think of compiling with optimization flag.</p>
<p>Then I decided to benchmark the CUDA float performance of similar algorithms. The CUDA program was 7x faster than the double precision multithreaded CPU program. This is comparing Nvidia GT330m vs Core i5 520m and float precision vs double precision on a naturally parallel problem. This is very far from the usually announced x80 speedup. Of course if one compares the algorithms with GCC single threaded no optimization, we might attain x50, but this is not a realistic comparison at all. I have heard that double precision is 8x slower on the GPU when compared to float precision: the difference then becomes quite small. Apparently Fermi cards are much faster, unfortunately I don&rsquo;t have any. And still I would not expect much better than 10x speedup. This is good but very far from the usually advertised speedup.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/firefox-4-is-great/">Firefox 4 Is Great</a>
  </h1>
  <time datetime="2010-12-22T16:03:00Z" class="post-date">Wed, Dec 22, 2010</time>
  <p>I temporarily <a href="/posts/bye-bye-firefox">abandonned Firefox</a> for Chrome/Chromium. I am now back at using Firefox as Firefox 4 is as fast or faster than Chrome and seems more stable, especially under linux. Also it does not send anything to Google and there is bookmark sync independently of Google.</p>
<p>I am impressed that Mozilla managed to improve Firefox that much.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/another-look-at-java-matrix-libraries/">Another Look at Java Matrix Libraries</a>
  </h1>
  <time datetime="2010-11-29T12:45:00Z" class="post-date">Mon, Nov 29, 2010</time>
  <p>A while ago, <a href="/post/the-pain-of-java-matrix-libraries">I was already looking</a> for a good Java Matrix library, complaining that there does not seem any real good one where development is still active: the 2 best ones are in my opinion <a href="http://math.nist.gov/javanumerics/jama/">Jama</a> and <a href="http://dsd.lbl.gov/~hoschek/colt/">Colt</a>.</p>
<p>Recently I tried to <a href="http://www.wilmott.com/detail.cfm?articleID=345">price options via RBF</a> (radial basis functions) based on <a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1648878">TR-BDF2</a> time stepping.
This is a problem where one needs to do a few matrix multiplications and inverses (or better, LU solve) in a loop. The size of the matrix is typically 50x50 to 100x100, and one can loop between 10 and 1000 times.<!-- raw HTML omitted --><!-- raw HTML omitted -->Out of curiosity I decided to give <a href="http://ojalgo.org/">ojalgo</a> and <a href="http://code.google.com/p/matrix-toolkits-java">MTJ</a> a chance. I had read benchmarks (<a href="http://blog.mikiobraun.de/2009/04/some-benchmark-numbers-for-jblas.html">here about jblas</a> and <a href="http://code.google.com/p/java-matrix-benchmark/wiki/Runtime_2xXeon_2010_08">here the java matrix benchmark</a>) where those libraries performed really well.<!-- raw HTML omitted --><!-- raw HTML omitted -->On my core i5 laptop under the latest 64bit JVM (Windows 7), I found out that for the 100x100 case, <em>Jama was actually 30% faster than MTJ</em>, and ojalgo was more than 50% slower. I also found out that I did not like ojalgo API at all. I was quite disappointed by those results.</p>
<p>So I tried the same test on a 6-core Phenom II (ubuntu 64bit), Jama was faster than MTJ by 0-10%. Ojalgo and ParallelColt were slower than Jama by more than 50% and 30%.</p>
<p>This does not mean that ojalgo and ParallelColt are so bad, maybe they behave much better than the simple Jama on large matrices. They also have more features, including sparse matrices. But Jama is quite a good choice for a default library, MTJ can also be a good choice, it can be faster and use less memory because most methods take the output matrix/vector as a parameter. Furthermore MTJ can use the native lapack and blas libraries for improved performance. The bigger the matrices, the most difference it will make.</p>
 

<table border=1><tbody><tr><td>Run</td><td>Jama</td><td>MTJ</td><td>MTJ native</td></tr><tr><td>1</td><td>0.160</td><td>0.240</td><td>0.140</td></tr><tr><td>2</td><td>0.086</td><td>0.200</td><td>0.220</td></tr><tr><td>10</td><td>0.083</td><td>0.089</td><td>0.056</td></tr></tbody></table>


<p>(On a Phenom II under Ubuntu 10.10 64-bit)</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/java-enum-is-evil/">Java enum Is Evil</a>
  </h1>
  <time datetime="2010-08-12T17:32:00Z" class="post-date">Thu, Aug 12, 2010</time>
   

Before Java 1.5, I never really complained about the lack of <b><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;"><span class="Apple-style-span" style="color: purple;">enum</span></span></b> keyword. Sure the <a href="http://java.sun.com/developer/Books/shiftintojava/page1.html">old enum via class pattern</a> was a bit verbose at first (N.B.: Java 1.5 enums can also be verbose once you start adding methods to them). But more importantly, you would often use the table lookup pattern in combination.<br /><br />The problem with Java 1.5 <b><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;"><span class="Apple-style-span" style="color: purple;">enum</span></span></b> is that it is not Object-Oriented. You <u>can't extend</u> an <b><span class="Apple-style-span" style="color: purple;"><span class="Apple-style-span" style="font-family: 'Courier New', Courier, monospace;">enum</span></span></b>, you can't add an element in an existing enum. Many will say "but that's what enum is for, a static list of things". In my experience, &nbsp;the list of things often changes with time, or needs to be extended at one point. Furthermore, most people (including me when I am very lazy) end up writing switch statements on enum values. Enum promotes bad programming practices.<br /><br />Think twice about using enum, this is often not what you want.



  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/21/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/23/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
