<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.147.8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2025. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/war-of-the-random-number-generators/">The war of the random number generators</a>
  </h1>
  <time datetime="2020-09-17T20:56:42&#43;0100" class="post-date">Thu, Sep 17, 2020</time>
  <p>These days, there seems to be some sort of small war to define what is a modern good random number generators to advise for simulations.
Historically, the Mersenne-Twister (MT thereafter) won this war. It is used by default in many scientific libraries and software, even if there has been a few issues with it:</p>
<ol>
<li>A bad initial seed may make it generate a sequence of low quality for at least as many as 700K numbers.</li>
<li>It is slow to jump-ahead, making parallelization not so practical.</li>
<li>It fails some TestU01 Bigcrush tests, mostly related to the F2 linear algebra, the algebra of the Mersenne-Twister.</li>
</ol>
<p>It turns out, that before MT (1997), a lot of the alternatives were much worse, except, perhaps, <a href="https://arxiv.org/abs/hep-lat/9309020">RANLUX</a> (1993), which is quite slow due to the need of skipping many points of the generated sequence.</p>
<p>The first issue has been mostly solved by more modern variants such as  <a href="https://dl.acm.org/doi/pdf/10.1145/369534.369540">MT-64</a> or <a href="https://www-labs.iro.umontreal.ca/~lecuyer/myftp/papers/wellrng.pdf">Well19937a</a> or <a href="https://arxiv.org/pdf/1505.06582">Memt19997</a>. The warm-up needed has been thus significantly shortened, and a better seed initialization is present in those algorithms. It is not clear however that it has been fully solved, as there are very few studies analyzing the quality with many different seeds, I found only a summary of one test, p.7 of <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjAo7TpuvLrAhXQCuwKHbgACJoQFjABegQIBRAB&amp;url=https%3A%2F%2Fwww6.inrae.fr%2Frecord%2FMedia%2Ffichiers%2FJ2_DHill&amp;usg=AOvVaw253AN7COqhMPQT3_KPuPe6">this presentation</a>.</p>
<p>The second issue may be partly solved by considering a shorter period, such as in the Well1024 generator.</p>
<p>The third issue may or may not be a real issue in practice. Those tests can be seen as taylored to make MT (and F2 algebra based generators) fail and not be all that practical. However, Vigna exposes the problem on some more concrete examples in <a href="https://arxiv.org/pdf/1910.06437.pdf">his recent paper</a>. The title of this paper has the provocative title <em>It Is High Time We Let Go Of The Mersenne Twister</em>. Before that paper, Vigna and her arch-enemy O&rsquo;Neil, regularly advised to let go of the Mersenne-Twister and use a generator they created instead. For Vigna, the generator is some variant of xorshift, the most recent being xoroshiro256**, and for O&rsquo;Neil, it is one of her numerous PCG algorithms. In both cases, a flurry of generators is proposed, and it seems that a lot of them are poor (Vigna criticizes strongly PCG on <a href="http://pcg.di.unimi.it/pcg.php">his personal page</a>; O&rsquo;Neil does something similar against xorshift variants <a href="https://lemire.me/blog/2017/09/08/the-xorshift128-random-number-generator-fails-bigcrush/">sometimes with the help of Lemire</a>). The recommended choice for each has evolved over the years. For a reader or a user, it looks then that both are risky/unproven. The authors of MT recently also added their own opinion on a specific xorshift variant (xorshift128+), with their papers <em><a href="https://arxiv.org/abs/1908.10020">Again, random numbers fall mainly in the planes: xorshift128+ generators</a></em> and <em><a href="https://arxiv.org/abs/1907.03251">Pseudo random number generators: attention for a newly proposed generator</a></em>. An important insight of that latter paper, is to insist that it is not enough to pass a good test suite like BigCrush for a generator to be good.</p>
<p>So what is recommended then?</p>
<p>A good read on the subject is another paper, with the title <a href="http://www.jucs.org/doi?doi=10.3217/jucs-012-06-0672">Pseudorandom Number Generation: Impossibility and Compromise</a>, also from the MT authors, explaining the challenge of defining what is a good random number generator. It ignores however the MRG family of generator studied by L&rsquo;Ecuyer, whose MRG32k3a is also relatively widely used, and has been there for a while now without any obvious defect against it being proven (good track record). This generator has a relatively fast jump-ahead, which is one of the reasons why it regained popularity with the advent of GPUs and does not fail TestU01 BigCrush. It is a bit slower than MT, but not much, especially with <a href="https://github.com/vigna/MRG32k3a">this implementation</a> from Vigna (3x faster than original double based implementation).</p>
<p>There are not many studies on block based crypto-generator such as AES or Chacha for simulation, which become a bit more trendy (thanks to <a href="http://www.thesalmons.org/john/random123/papers/random123sc11.pdf">Salmons paper</a>) as they are trivial use in a parallel Monte-Carlo simulation (jump-ahead as fast as generation of one number). In theory the uniformity should be good, since otherwise that would be a channel of attack.</p>
<p>The conclusion of the presentation referenced earlier in this post, is also very relevant:</p>
<ul>
<li>use the best sequential generators (i.e. MT, MRG32k3a or some Well),</li>
<li>test the stochastic variability by changing generator,</li>
<li>do not parallelize by inputing different seeds (prefer a jump-ahead or a tested substream approach).</li>
</ul>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/sobol-64-bits/">Sobol with 64-bits integers</a>
  </h1>
  <time datetime="2020-09-09T20:56:42&#43;0100" class="post-date">Wed, Sep 9, 2020</time>
  <p>A while ago, I wondered how to make some implementation of Sobol support 64-bits integers (long) and double floating points. <a href="https://en.wikipedia.org/wiki/Sobol_sequence">Sobol</a> is the most used
quasi random number generator (QRNG) for (quasi) Monte-Carlo simulations.</p>
<p>The standard Sobol algorithms are all coded with 32-bits integers and lead to double floating point numbers which can not be smaller than
\( 2^{-31} \). I was recently looking back at the internals at Sobol generators, and noticed that generating with 64-bits integers would not help much.</p>
<p>A key to understanding why is to analyze exactly what is the output that Sobol generates. If one asks for a sequence of N numbers (in dimension D),
the output will be a multiple of \( 2^{-L} \) where L is the log-2 of N. The 32 bits only become useful when \( N &gt; 2^{31} \).
An implementation with 64-bit integers becomes only useful if we query an extremely long sequence (longer than 1 billion numbers), which is not all that practical in reality. Furthermore, the direction numbers would then require double the amount of memory (which may be relatively large for 20K dimensions).</p>
<p>Another interesting detail I learnt recently was to avoid skipping the first point, when scrambling (or randomization) is applied, as per <a href="https://arxiv.org/abs/2008.08051">this article</a> from Owen (2020). In a non-randomized or non-scrambled setting, we skip the first point typically because it is 0, and 0 is often problematic, for example if we need to take the inverse cumulative distribution function, to simulate a specific distribution. What I find slightly surprising is that there is no symmetry between 1 and 0: the point (1, 1) is never generated by a two-dimensional Sobol generator, but (0, 0) is the first number. If we apply some sort of inverse cumulative distribution (and no scrambling), it looks like then the result would be skewed towards negative infinity.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/intel-failures-and-the-future/">Intel failure and the future of computing</a>
  </h1>
  <time datetime="2020-07-24T20:56:42&#43;0100" class="post-date">Fri, Jul 24, 2020</time>
  <p>What has been happening to the INTC stock today may be revealing of the future. The stock dropped more than 16%, mainly because they announced that their 7nm process does not work (well) and they may rely on an external foundry for their processors. Initially, in 2015, they thought they would have 8nm process by 2017, and 7nm by 2018. They are more than 3 years late.</p>
<p>Intel used to be a leader in the manufacturing process for microprocessor. While the company has its share of internal problems, it may also be that we are starting to hit the barrier, where it becomes very difficult to improve on the existing. The end of Mooreâ€™s law has been announced many times, it was already a subject 20 years ago. Today, it may be real, if there is only a single company capable of manufacturing processor using a 5nm process (TSMC).</p>
<p>It will be interesting to see what this means for software in general. I suppose clever optimizations and good parallelization may start playing a much more important role. Perhaps we will see also more enthusiasm towards specialized processors, somewhat similar to GPUs and neural network processors.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/mar9-cac40-crash/">March 9, 2020 crash - where will CAC40 go?</a>
  </h1>
  <time datetime="2020-03-09T21:56:42&#43;0100" class="post-date">Mon, Mar 9, 2020</time>
  <p>The stock market crashed by more than 7% on March 9, 2020. It is one of the most important drop since September 2001.
I looked at BNP warrant prices on the CAC40 French index, with a maturity of March 20, 2020 , to see what they would tell about the market direction on the day of the crash. This is really a not-so-scientific experiment.</p>
<p>The quotes I got were quite noisy. I applied a few different techniques to imply the probability density from the option prices:</p>
<ul>
<li>The one-step Andreasen-Huge with some regularization. The regularization is a bit too mild, although, in terms of implied vol, this was the worst fit  among the techniques.</li>
<li>The stochastic collocation towards a septic polynomial, no regularization needed here. The error in implied volatilities is similar to Andreasen-Huge, even though the implied density is much smoother. I however discovered a small issue with the default optimal monotonic fit, and had to tweak a little bit the optimal polynomial, more on this later in this post.</li>
<li>Some RBF collocation of the implied vols. Best fit, with regularization, and very smooth density, which however becomes negative in the high strikes.</li>
<li>Some experimental Andreasen-Huge like technique, with a minimal grid and good regularization.</li>
</ul>
<figure><img src="/post/cac40_mar9_dens.png"><figcaption>
      <h4>Implied probability density for March 20, CAC40 warrants.</h4>
    </figcaption>
</figure>

<p>The implied forward price was around 4745.5. Interestingly, this corresponds to the first small peak visible on the blue and red plots. The strongest peak
is located a little beyond 5000, which could mean that the market believes that the index will go back up above 5000. This does not mean that you should buy however, as there are other, more complex explanations. For example, the peak could be a latent phenomenon related to the previous days.</p>
<p>Now here is how the plot is with the &ldquo;raw&rdquo; optimal collocation polynomial:
<figure><img src="/post/cac40_mar9_dens_0.png"><figcaption>
      <h4>Implied probability density for March 20, CAC40 warrants with raw optimal polynomial collocation.</h4>
    </figcaption>
</figure>
</p>
<p>Notice the spurious peak. In terms of implied volatility, this corresponds to a strange, unnatural angle in the smile. The reason for this unnatural peak lies in the details of the stochastic collocation technique: the polynomial we fit is monotonic, but ends up with slope close to zero at some point in order to better fit the data. If the slope is exactly zero, there is a discontinuity in the density. Here the very low slope tranlates to the peak. The fix is simply to impose a floor on the slope (although it may not be obvious in advance to know how much this floor should be).</p>
<figure><img src="/post/cac40_mar9_collo.png"><figcaption>
      <h4>Collocation polynomials for March 20, CAC40 warrants.</h4>
    </figcaption>
</figure>

<p>And for the curious, here are the implied vol plots:</p>
<figure><img src="/post/cac40_mar9_vol.png"><figcaption>
      <h4>Implied vol fits for March 20, CAC40 warrants.</h4>
    </figcaption>
</figure>


  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/42/">42</a>
  </h1>
  <time datetime="2019-12-05T20:56:42&#43;0100" class="post-date">Thu, Dec 5, 2019</time>
  <p>Today my 6-years old son came with a math homework. The stated goal was to learn the different ways to make 10 out of smaller numbers. I was impressed. Immediately, I wondered</p>
<blockquote>
<p>how many ways are there to make 10 out of smaller numbers?</p></blockquote>
<p>This is one of the beauties of maths: a very simple problem, which a 6-years old can understand, may actually be quite fundamental. If you want to solve this in the general case, for any number instead of 10, you end up with the <a href="https://en.wikipedia.org/wiki/Partition_(number_theory)#Restricted_part_size_or_number_of_parts">partition function</a>. And in order to find this, you will probably learn recurrence relations. So what is the answer for 10?</p>
<blockquote>
<p>42</p></blockquote>
<p>Then I looked at one exercise they did in class, which was simply to find different ways to pay 10 euros with bills of 10, 5 and coins of 2 and 1 euro(s).</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/python-numba-overrated/">Numba, Pypy Overrated?</a>
  </h1>
  <time datetime="2019-02-12T20:56:42&#43;0100" class="post-date">Tue, Feb 12, 2019</time>
  <p>Many benchmarks show impressive performance gains with the use
of <a href="https://numba.pydata.org/">Numba</a> or <a href="https://www.pypy.org/">Pypy</a>. Numba allows to compile just-in-time some specific methods, while Pypy takes
the approach of compiling/optimizing the full python program: you use it just like the standard
python runtime. From those benchmarks, I imagined that those  tools would improve my 2D Heston PDE solver
performance easily. The initialization part of my program contains embedded for loops over several 10Ks elements.
To my surprise, numba did not improve anything (and I had to isolate the code, as it would
not work on 2D numpy arrays manipulations that are vectorized). I surmise it does not play well
with scipy sparse matrices.
Pypy did not behave better, the solver became actually slower than with the standard python
interpreter, up to twice as slow, for example, in the case of the main solver loop which only does matrix multiplications and LU solves sparse systems. I did not necessarily expect any performance improvement in this specific loop, since it only consists in a few calls to expensive scipy calculations. But I did not expect a 2x performance drop either.</p>
<p>While I am sure that there are good use cases, especially for numba, I was a bit disappointed
that it likely would require to significantly change my code to have any effect (possibly to not do the initialization with scipy sparse matrices, but then, it is not so clear how). Also, I
find <a href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/A_Comparison_Of_C_Julia_Python_Numba_Cython_Scipy_and_BLAS_on_LU_Factorization?lang=en">most</a> <a href="https://modelingguru.nasa.gov/docs/DOC-2676">benchmarks</a> dumb. For example, the 2D ODE solver of the latter uses a simple dense 2D numpy array. On real code,
things are not as simple.</p>
<p>Next I should try Julia again out of curiosity. I tried it <a href="/post/modern-programming-language-for-monte-carlo/">several years ago</a> in the context of a simple Monte-Carlo simulation and my experiment at the time
was not all that encouraging, but it may be much better at building PDE solvers and not too much work to port my python code. I am curious to see if it ends up being faster, and whether the accessible LU solvers are any good. If Julia delivers, it&rsquo;s a bit of a shame for python, since it has so many excellent high quality numerical libraries. Also when I look back at my old Monte-Carlo test, I bet it would benefit greatly from numba, somewhat paradoxically, compared to my current experiment.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/about/">About</a>
  </h1>
  <time datetime="2018-12-17T10:05:07&#43;0100" class="post-date">Mon, Dec 17, 2018</time>
  <p>I just moved my blog to a static website, created with <a href="https://gohugo.io/">Hugo</a>, I explain the reasons why <a href="/post/moved-to-hugo/">here</a>.
You can find more about me on my <a href="https://fr.linkedin.com/in/fabien-le-floc-h-8aa306">linkedin profile</a>.
In addition you might find the following interesting:</p>
<ul>
<li><a href="http://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1514784">List of quantitative finance papers</a> I have freely available on SSRN</li>
<li><a href="/lefloch_trbdf2_draft.pdf">TR-BDF2 for Stable American Option Pricing</a>. This is the first draft, not the final version that was published in the Journal of Computational Finance.</li>
<li><a href="/lefloch_sabr_slides.pdf">Presentation on finite difference techniques for arbitrage-free SABR</a> I gave at the conference on models and numerics in financial mathematics at the Lorentz center in 2015.</li>
<li><a href="/lefloch_exact_log.pdf">Exact Forward for Finite-Difference Schemes on the Log-transformed Black-Scholes PDE</a>.</li>
<li><a href="/lefloch_volatility_asymptotics.pdf">Asymptotic bounds of the normal volatility</a>.</li>
</ul>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/quadprog-nans/">Fixing NaNs in Quadprog</a>
  </h1>
  <time datetime="2018-10-07T20:56:42&#43;0100" class="post-date">Sun, Oct 7, 2018</time>
  <p>Out of curiosity, I tried <a href="https://github.com/cran/quadprog">quadprog</a> as <a href="https://quantsrus.github.io/post/state_of_convex_quadratic_programming_solvers/">open-source quadratic programming convex optimizer</a>, as it is looks fast, and the code stays relatively simple. I however stumbled on cases where the algorithm would return NaNs even though my inputs seemed straighforward. Other libraries such as CVXOPT did not have any issues with those inputs.</p>
<p>Searching on the web, I found that I was not the only one to stumble on this kind of issue with quadprog. In particular, in 2014, Benjamen Tyner <a href="http://r.789695.n4.nabble.com/quadprog-solve-QP-sometimes-returns-NaNs-td4697548.html">gave a simple example in R</a>, where solve.QP returns NaNs while the input is very simple: an identity matrix with small perturbations out of the diagonal. Here is a copy of his example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-R" data-lang="R"><span style="display:flex;"><span>    <span style="color:#06287e">library</span>(quadprog)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    n <span style="color:#666">&lt;-</span> <span style="color:#40a070">66L</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#06287e">set.seed</span>(<span style="color:#40a070">6860</span>)
</span></span><span style="display:flex;"><span>    X <span style="color:#666">&lt;-</span> <span style="color:#06287e">matrix</span>(<span style="color:#40a070">1e-20</span>, n, n)
</span></span><span style="display:flex;"><span>    <span style="color:#06287e">diag</span>(X) <span style="color:#666">&lt;-</span> <span style="color:#40a070">1</span>
</span></span><span style="display:flex;"><span>    Dmat <span style="color:#666">&lt;-</span> <span style="color:#06287e">crossprod</span>(X)
</span></span><span style="display:flex;"><span>    y <span style="color:#666">&lt;-</span> <span style="color:#06287e">seq_len</span>(n)
</span></span><span style="display:flex;"><span>    dvec <span style="color:#666">&lt;-</span> <span style="color:#06287e">crossprod</span>(X, y)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Amat <span style="color:#666">&lt;-</span> <span style="color:#06287e">diag</span>(n)
</span></span><span style="display:flex;"><span>    bvec <span style="color:#666">&lt;-</span> y <span style="color:#666">+</span> <span style="color:#06287e">runif</span>(n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    sol <span style="color:#666">&lt;-</span> <span style="color:#06287e">solve.QP</span>(Dmat, dvec, Amat, bvec, meq <span style="color:#666">=</span> n)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#06287e">print</span>(sol<span style="color:#666">$</span>solution) <span style="color:#60a0b0;font-style:italic"># this gives all NaNs</span></span></span></code></pre></div>
<p>Other people stumbled on <a href="https://stats.stackexchange.com/questions/259993/why-would-quadratic-program-in-svm-not-work-for-very-large-or-very-small-lambda">similar</a> issues.</p>
<p>In my specific case, I was able to debug the quadprog algorithm and find the root cause: two variables \(g_c\) and \(g_s\) can become very small, and their square becomes essentially zero, creating a division by zero. If, instead of computing \( \frac{g_s^2}{g_c^2} \) we compute \( \left(\frac{g_s}{g_c}\right)^2 \), then the division by zero is avoided as the two variables are of the same order.</p>
<figure><img src="/post/quadprog_nan_fix.png"><figcaption>
      <h4>Sample code change [on github](https://github.com/cran/quadprog/pull/1/commits/7f51915f7c662c7fac3d4e2ab067cfbc292767f8).</h4>
    </figcaption>
</figure>

<p>While it probably does not catter for all the possible NaN use cases, it did fix all the cases I stumbled upon.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/nflx-stock-crash-probability/">On the Probability of a Netflix Stock Crash</a>
  </h1>
  <time datetime="2018-07-12T20:56:42&#43;0100" class="post-date">Thu, Jul 12, 2018</time>
  <p>This is a follow up to my <a href="/post/tsla-stock-crash-probability">previous post</a> where I explore the probability
of a TSLA stock crash, reproducing the <a href="https://www.linkedin.com/pulse/options-market-thinks-16-chance-tesla-exist-january-2020-klassen/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BaOfn2Xf6RIum6%2F9ddKS9fA%3D%3D">results of Timothy Klassen</a>.</p>
<p>According to the implied cumulative probability density, TSLA has around 15% chance of crashing below $100. Is this really
high compared to other stocks? or is it the interpretation of the data erroneous?</p>
<p>Here I take a look at NFLX (Netflix). Below is the implied volatility according to three different models.</p>
<figure><img src="/post/nflx2020_vol.png"><figcaption>
      <h4>Black volatility implied from Jan 2020 NFLX options.</h4>
    </figcaption>
</figure>

<p>Again, the shape is not too difficult to fit, and all models give nearly the same fit, within the bid-ask spread as long as we
include an inverse relative bid-ask spread weighting in the calibration.</p>
<p>The corresponding implied cumulative density is</p>
<figure><img src="/post/nflx2020_cum.png"><figcaption>
      <h4>Cumulative density implied from Jan 2020 NFLX options.</h4>
    </figcaption>
</figure>

<p>More explicitely, we obtain</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>probability of NFLX &lt; 130</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Collocation</td>
          <td>3.3%</td>
      </tr>
      <tr>
          <td>Lognormal Mixture</td>
          <td>3.6%</td>
      </tr>
      <tr>
          <td>Andreasen-Huge regularized</td>
          <td>3.7%</td>
      </tr>
  </tbody>
</table>
<p>The probability of NFLX moving below $130 is significantly smaller than the probability of TSLA moving below $100 (which is also around the same 1/3 of the spot price).</p>
<p>For completeness, the implied probability density is</p>
<figure><img src="/post/nflx2020_rnd.png"><figcaption>
      <h4>Probability density implied from Jan 2020 NFLX options.</h4>
    </figcaption>
</figure>

<p>I did not particularly optimize the regularization constant here. The lognormal mixture can have a tendency to produce
too large bumps in the density at specific strikes, suggesting that it could also benefit of some regularization.</p>
<p>What about risky stocks like SNAP (Snapchat)? It is more challenging to imply the density for SNAP as there are
much fewer traded options on the NASDAQ: less strikes, and a lower volume. Below is an attempt.</p>
<figure><img src="/post/snap2020_vol.png"><figcaption>
      <h4>Black volatility implied from Jan 2020 SNAP options.</h4>
    </figcaption>
</figure>

<figure><img src="/post/snap2020_cum.png"><figcaption>
      <h4>Cumulative density implied from Jan 2020 SNAP options.</h4>
    </figcaption>
</figure>

<p>We find a probability of moving below 1/3 of the current price lower than with TSLA: around 12% SNAP vs 15% for TSLA.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/tsla-stock-crash-probability/">On the Probability of a TSLA Stock Crash</a>
  </h1>
  <time datetime="2018-07-11T20:56:42&#43;0100" class="post-date">Wed, Jul 11, 2018</time>
  <p>Timothy Klassen had an <a href="https://www.linkedin.com/pulse/options-market-thinks-16-chance-tesla-exist-january-2020-klassen/?lipi=urn%3Ali%3Apage%3Ad_flagship3_feed%3BaOfn2Xf6RIum6%2F9ddKS9fA%3D%3D">interesting post</a> on linkedin recently, with the title &ldquo;the options market thinks there is a 16% chance that Tesla will not exist in January 2020&rdquo;.
As I was also recently looking at the TSLA options, I was a bit intrigued. I looked at the option chain on July 10th,
and implied the European volatility from the American option prices. I then fit a few of my favorite models: Andreasen-Huge with Tikhonov regularization, the lognormal mixture, and a polynomial collocation of degree 7.
This results in the following graph</p>
<figure><img src="/post/tsla2020_vol.png"><figcaption>
      <h4>Black volatility implied from Jan 2020 TSLA options.</h4>
    </figcaption>
</figure>

<p>The shape is not too difficult to fit, and all models give nearly the same fit, within the bid-ask spread as long as we
include an inverse relative bid-ask spread weighting in the calibration. Note that I
removed the quote for strike 30, as the bid-ask spread was, unusually extremely large and would only introduce noise.</p>
<p>Like Timothy Klassen, we can look at the implied cumulative density of each model.</p>
<figure><img src="/post/tsla2020_cum.png"><figcaption>
      <h4>Cumulative density implied from Jan 2020 TSLA options.</h4>
    </figcaption>
</figure>

<p>More explicitely, we obtain</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>probability of TSLA &lt; 100</th>
          <th>probability of TSLA &lt; 15</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Collocation</td>
          <td>15.4%</td>
          <td>7.2%</td>
      </tr>
      <tr>
          <td>Lognormal Mixture</td>
          <td>15.2%</td>
          <td>8.0%</td>
      </tr>
      <tr>
          <td>Andreasen-Huge regularized</td>
          <td>15.0%</td>
          <td>7.7%</td>
      </tr>
  </tbody>
</table>
<p>It would have been great of Timothy Klassen had shown the implied density as well, in the spirit of my <a href="/post/implying-the-probability-density-from-market-option-prices-ii">earlier posts on a similar subject</a>.</p>
<figure><img src="/post/tsla2020_rnd.png"><figcaption>
      <h4>Probability density implied from Jan 2020 TSLA options.</h4>
    </figcaption>
</figure>

<p>In particular, the choice of model has a much stronger impact on the implied density. Even though Andreasen-Huge has more
modes, its fit in terms of volatilities is not better than the lognormal mixture model. We could reduce the number of modes by increasing
the constant of the Tikhonov regularization, at the cost of a slightly worse fit.</p>
<p>The collocation produces a significantly simpler shape. This is not necessarily a drawback, since the fit is quite good
and more importantly, it does not have a tendency to overfit (unlike the two other models considered).</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>weighted root mean square error in vols</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Collocation</td>
          <td>0.00484</td>
      </tr>
      <tr>
          <td>Lognormal Mixture</td>
          <td>0.00432</td>
      </tr>
      <tr>
          <td>Andreasen-Huge regularized</td>
          <td>0.00435</td>
      </tr>
  </tbody>
</table>
<p>The most interesting from T. Klassen plot, is the comparison with the stock price across time.
It is expected that the probability of going under $100 will be larger when the stock price moves down,
which is what happen around March 27, 2018. But it is not so expected that when it comes back up higher
(above $350 in mid June), the probability of going under $100 stays higher than it was before March 27, 2018,
where the stock price was actually lower.
In fact, there seems to be a signal in his time-serie, where the probability of going under $100 increases
significantly, one week before March 27, that is one week before the actual drop,
suggesting that the drop was priced in the options.</p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/4/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/6/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
