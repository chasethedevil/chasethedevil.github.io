<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.91.2" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2022. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/">Heston or Schobel-Zhu issues with short expiries</a>
  </h1>
  <time datetime="2014-07-03T23:28:00Z" class="post-date">Thu, Jul 3, 2014</time>
   

It's relatively well known that Heston does not fit the market for short expiries. Given that there are just 5 parameters to fit a full surface, it's almost logical that one part of the surface of it is not going to fit well the market.<br />I was more surprised to see how bad Heston or Schobel-Zhu were to fit a single short expiry volatility slice. As an example I looked at SP500 options with 1 week expiry. It does not really matter if one forces kappa and rho to constant values (even to 0) the behavior is the same and the error in fit does not change much.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png" height="400" width="390" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Schobel-Zhu fit for a slice of maturity 1 week</td></tr></tbody></table>In this graph, the brown, green and red smiles corresponds to Schobel-Zhu fit using an explicit guess (matching skew &amp; curvature ATM), using Levenberg-Marquardt on this guess, and using plain differential evolution. <br />What happens is that the smiles flattens to quickly in the strike dimension. One consequence is that the implied volatility can not be computed for extreme strikes: the smile being too low, the price becomes extremely small, under machine epsilon and the numerical method (Cos) fails. There is also a bogus angle in the right wing, because of numerical error. I paid attention to ignore too small prices in the calibration by truncating the initial data.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png" height="400" width="391" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Heston fit, with Lord-Kahl (exact wings)</td></tr></tbody></table><br />SABR behaves much better (fixing beta=1 in this case) in comparison (As I use the same truncation as for Schobel-Zhu, the flat left wing part is ignored). <br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png" height="400" width="391" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SABR fit for a slice of maturity 1 week</td></tr></tbody></table>For longer expiries, Heston &amp; Schobel-Zhu, even limited to 3 parameters, actually give a better fit in general than SABR.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/">On the Role of Static Types and Generic Types on Productivity</a>
  </h1>
  <time datetime="2014-06-29T10:40:00Z" class="post-date">Sun, Jun 29, 2014</time>
   

Most developers have strong opinions on dynamic types programming languages vs static types programming languages. The former is often assumed to be good for small projects/prototyping while the later better for bigger projects. But there is a surprisingly small number of studies to back those claims.<br /><br />One such study is "<a href="http://diyhpl.us/~bryan/papers2/paperbot/7a01e5a892a6d7a9f408df01905f9359.pdf" target="_blank">An experiment about static and dynamic type systems: doubts about the positive impact of static type systems on development time</a>" and came to the conclusion that on a small project, static typing did not decrease programming time, and actually increased debugging time. However 4 years later, "<a href="http://users.dcc.uchile.cl/~rrobbes/p/ICPC2014-idetypes.pdf" target="_blank">An empirical comparison of static and dynamic type systems on API usage in the presence of an IDE: Java vs. groovy with eclipse</a>" shows that a developer is 2x more productive with Java than with Groovy using an unknown API. This contrasts a bit (but does not contradict) with their previous study "<a href="http://swp.dcc.uchile.cl/TR/2012/TR_DCC-20120418-005.pdf" target="_blank">Static Type Systems (Sometimes) have a Positive Impact on the Usability of Undocumented Software: An Empirical Evaluation</a>" that showed Groovy to be more productive on small projects. One problem is that all these studies stem from the same person.<br /><br />It's more interesting to look at generic types vs raw types use, where even less studies have been done. "<a href="http://dl.acm.org/citation.cfm?id=2509528" target="_blank">Do developers benefit from generic types?: an empirical comparison of generic and raw types in java</a>" concludes that generic types do not provide any advantages to fix typing errors, hardly surprising in my opinion. Generic types (especially with type erasure as in Java) is the typical idea that sounds good but that in practice does not really help: it makes the code actually more awkward to read and tend to make developers too lazy to create new classes that would often be more appropriate than a generic type (think Map&lt;String,List&lt;Map&lt;String, Date&gt;&gt;&gt;).



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/moore-penrose-inverse-gauss-newton-sabr-minimization/">Moore-Penrose Inverse &amp; Gauss-Newton SABR Minimization</a>
  </h1>
  <time datetime="2014-06-24T15:29:00Z" class="post-date">Tue, Jun 24, 2014</time>
   

I have found a particularly nice initial guess to calibrate SABR. As it is quite close to the true best fit, it is tempting to use a very simple minimizer to go to the best fit. Levenberg-Marquardt works well on this problem, but can we shave off a few iterations?<br /><br />I firstly considered the basic <a href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank">Newton's method</a>, but for least squares minimization, the Hessian (second derivatives) is needed. It's possible to obtain it, even analytically with SABR, but it's quite annoying to derive it and code it without some automatic differentiation tool. It turns out that as I experimented with the numerical Hessian, I noticed that it actually did not help convergence in our problem. <a href="http://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm" target="_blank">Gauss-Newton</a> converges similarly (likely because the initial guess is good), and what's great about it is that you just need the Jacobian (first derivatives). <a href="https://www.math.lsu.edu/system/files/MunozGroup1%20-%20Paper.pdf" target="_blank">Here</a> is a good overview of Newton, Gauss-Newton and Levenberg-Marquardt methods.<br /><br />While Gauss-Newton worked on many input data, I noticed it failed also on some long maturities equity smiles. The full Newton's method did not fare&nbsp; better. I had to take a close look at the matrices involved to understand what was going on. It turns out that sometimes, mostly when the SABR rho parameter is close to -1, the Jacobian would be nearly rank deficient (a row close to 0), but not exactly rank deficient. So everything would appear to work, but it actually misbehaves badly.<br /><br />My first idea was to solve the reduced problem if a row of the Jacobian is too small, by just removing that row, and keep the previous value for the guess corresponding to that row. And this simplistic approach made the process work on all my input data. Here is the difference in RMSE compared to a highly accurate Levenberg-Marquardt minimization for 10 iterations:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png" height="260" width="320" /></a></div><br /><br />Later, while reading some more material related to least square optimization, I noticed the use of the <a href="http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse" target="_blank">Moore-Penrose inverse</a> in cases where a matrix is rank deficient. The Moore-Penrose inverse is defined as:<br />$$ M^\star = V S^\star U^T$$<br />where \( S^\star \) is the diagonal matrix with inverted eigenvalues and 0 if those are deemed numerically close to 0, and \(U, V\) the eigenvectors of the SVD decomposition:<br />$$M=U S V^T$$<br />It turns out to work very well, beside being simpler to code, I expected it to be more or less equivalent to the previous approach (a tiny bit slower but we don't care as we deal with small matrices, and the real slow part is the computation of the objective function and the Hessian, which is why looking at iterations is more important).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png" height="267" width="320" /></a></div><br />It seems to converge a little bit less quickly, likely due to the threshold criteria that I picked (1E-15).<br />Three iterations is actually most of the time (90%) more than enough to achieve a good accuracy (the absolute RMSE is between 1E-4 and 5E-2) as the following graph shows. The few spikes near 1E-3 represent too large errors, the rest is accurate enough compared to the absolute RMSE.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png" height="242" width="320" /></a></div><br />To conclude, we have seen that using the Moore-Penrose inverse in a Gauss-Newton iteration allowed the Gauss-Newton method to work on rank-deficient systems.<br />I am not sure how general that is, in my example, the true minimum either lies inside the region of interest, or on the border, where the system becomes deficient. Of course, this is related to a "physical" constraint, here namely rho &gt; -1.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/one-interview-question-for-job-seekers-in-finance/">One Interview Question for Job Seekers in Finance</a>
  </h1>
  <time datetime="2014-06-19T21:51:00Z" class="post-date">Thu, Jun 19, 2014</time>
  <p>I presented in an <!-- raw HTML omitted -->earlier post<!-- raw HTML omitted --> that I was mostly disillusioned with interview questions, it&rsquo;s better to find out if you can learn something out of a candidate.<!-- raw HTML omitted --><!-- raw HTML omitted -->Well there is maybe one very simple question that could be revealing, for people who pretend to be vaguely familiar with Black-Scholes:<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->What is the price of an at-the-money  binary option under very high volatility? <!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Alternatively it can be asked with just an at-the-money european option under very high volatility.<!-- raw HTML omitted --><!-- raw HTML omitted -->What makes think of it is that some &ldquo;product manager&rdquo; recently tested risk with volatilities at 300% and was wondering why they did not see any vega (based on a 1% additive shift), and opened bugs, generated noise&hellip;</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/">On the importance of accuracy for bpvol solvers</a>
  </h1>
  <time datetime="2014-06-12T17:31:00Z" class="post-date">Thu, Jun 12, 2014</time>
  <p>While I was playing around calibrating the arbitrage free SABR model from Hagan (using the PDE on probability density approach), I noticed a misbehavior for some short maturity smiles. I thought it was due to the PDE implementation. Actually some of it was, but the remaining large error was due to the bpvol solver.<!-- raw HTML omitted --><!-- raw HTML omitted -->I initially took the same approach as Choi et al. in <!-- raw HTML omitted -->my solver<!-- raw HTML omitted -->, that is to work with in-the-money prices (they work with straddles) because it&rsquo;s nice and convenient. I thought it was no big deal if prices lower than 1E-16 were not solved. It turns out I was wrong. Choi et al. solver has the same issue.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->In the above figure, CKK denotes the Choi et al algorithm (similar with my old algorithm) and Chebyshev is my updated algorithm that is accurate with far out-of-the-money option. What happens is that even though the market price at the lowest strike is not very low, the price at the lowest strike stemming from the best fit smile is extremely low, and when we want to invert it, CKK produces a large error due to lack of representation of numbers near 1.0 as it uses indirectly the in-the-money price. That&rsquo;s where it introduces a particularly big error in this case.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->I have updated my solver since, to work with out-of-the-money option prices as well, and have near machine accuracy on the whole range. I also reduced the number of Chebyshev polynomials used in the process. All the details are in my updated paper at <!-- raw HTML omitted --><a href="http://papers.ssrn.com/abstract=2420757">http://papers.ssrn.com/abstract=2420757</a><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/throws-exception/">throws Exception</a>
  </h1>
  <time datetime="2014-05-27T10:49:00Z" class="post-date">Tue, May 27, 2014</time>
   

There was a big debate at work around Exception declaration in a Java API. I was quite surprised that such an apparently simple subject could end up being so controversial. The controversy was around the choice of declaring in the interfaces:<br /><br /><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">void myMethod() throws Exception</span></span><br /><br />instead of<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() throws MyAPIException</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() throws MyAPIRuntimeException</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() </span></span><br /><br />where MyAPI represents either a generic API related exception or a specific exception related to the method in question.<br /><br />The choice of "throws Exception" did not even occur to me as a possibility, but after some digging, I found that some relatively famous libraries actually followed that principle at one point, for example Apache Struts 1.x or Spring MVC. <br /><br />More modern libraries, like Google Guava, commons-math 3.x, Struts 2.x generally favor MyAPIRuntimeException where MyAPI is actually context-specific. Some old popular libraries declare a checked Exception, for example the HibernateException in Hibernate.<br /><br />This seems to be a recurring subject on Stackoverflow:<br /><a href="http://stackoverflow.com/questions/20530221/java-interface-throws-exception-best-practice" target="_blank">Stackoverflow - Java interface throws Exception best practice</a><br /><a href="http://stackoverflow.com/questions/4283634/what-to-put-in-the-throws-clause-of-an-interface-method" target="_blank">Stackoverflow - What to put in the throws clause of an interface method</a><br /><br />But those are quite poor in terms of explanations. The best comments on this subjects are from:<br /><a href="http://www.artima.com/intv/handcuffs.html"><span class="ts">Anders         Hejlsberg (C#, Delphi, Turbo Pascal creator) - The Trouble with         Checked Exceptions</span></a><br />    <a href="http://www.artima.com/intv/solid2.html"><span class="ts">James         Gosling (Java creator) - Failure and Exceptions</span></a><br /><br />    <br />This comment from Anders is particularly acute:<br />    "<b>To work around this requirement, people do ridiculous things.       For example, they decorate every method with, "</b><b><code>throws         Exception</code></b><b>." That just completely defeats the       feature, and you just made the programmer write more gobbledy       gunk. That doesn't help anybody.</b>    "<br />    <br /><br />         <br />Today I believe the API in question declares "throws Exception"... <br />         <br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/kde-xfce-gnome-shell-in-2014/">KDE, XFCE, Gnome-Shell in 2014</a>
  </h1>
  <time datetime="2014-05-25T09:26:00Z" class="post-date">Sun, May 25, 2014</time>
   

Many people (and notoriously, Linus Torvald) complained about Gnome-shell, especially the early iterations. Similarly KDE 4 was a nightmare of instability and inflexibility when it came out. And XFCE has always sounded a bit too basic. the moves of Gnome and KDE were particularly shocking as the earlier iteration: Gnome 2 and KDE 3 were well appreciated, productive environments.<br /><br /><span style="font-size: large;">Gnome Shell 3.10</span><br /><br />It took me a bit of time to get used to it, and in the early stages I went to KDE 4 for a while, only to come back to it later.<br /><br /><ul><li><i>Positive aspects:</i> lots of space on the desktop, things don't get in the way, looks good,very good desktop overview (fast and well presented), a dock by default, great external monitor support (plug and play, remembers settings automatically), best OSD (volume) of all.</li><li><i>Negative aspects:</i> the notifications bar looks awkward and badly integrated (better with an extension), still unstable and big memory leaks (on Fedora 20, where the integration should be the best, it regularly crashes, starts with 300Mb and goes up to 1Gb in a couple of days), fallback-session completely useless as one can not customize it at all. But the killer for my work was&nbsp; inability to share the desktop with Webex, while XFCE could.</li></ul><br /><span style="font-size: large;">KDE</span> <br /><br />I gave it a long try especially in 2012, it has not changed much in 2014. My opinion of it fell when I tried it a very short time after months of Gnome Shell, and even more so after seeing the trouble my parents had with it, compared to Gnome 2.<br /><br /><ul><li><i>Positive aspects:</i> desktop search (needs to be configured in order to scan only the relevant folders, used to be slow and resource intensive, not so much in 2014)<i>&nbsp;</i></li><li><i>Negative aspects:</i> resource hog, awful start menu, too many shiny effects by default that only distract the user from his task, silly concepts like activities, every aspect of the desktop seems to require tweaking in non obvious ways for it to be more usable, looks ok but not great.</li></ul><br /><span style="font-size: large;">XFCE</span><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-nw0i5RiPfz0/U4RIv9aY2pI/AAAAAAAAHTQ/xF9ZwyRtXZk/s1600/Screenshot+-+05272014+-+10:06:57+AM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-nw0i5RiPfz0/U4RIv9aY2pI/AAAAAAAAHTQ/xF9ZwyRtXZk/s1600/Screenshot+-+05272014+-+10:06:57+AM.png" height="223" width="400" /></a></div><br />On Fedora, the default XFCE is very very basic, so much that I could hardly see a difference with one from 10 years ago. On Xubuntu, it's much much better. When I came to it from Gnome-Shell, I was surprised at how good was the "old" desktop paradigm for productivity. I also surprisingly found multiple desktops more natural to use than on Gnome Shell/KDE.<br />On Fedora the way to make it like Xubuntu is to install elementary icons, the whisker menu and choose the greybird/bluebird themes.<br /><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><br /></span></span><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">yum groups install "Xfce Desktop"<br />yum install xfce4-mixer.x86_64 xfce4-whiskermenu-plugin.x86_64 xfce4-cpugraph-plugin.x86_64 xfce4-mount-plugin.x86_64 xfce4-icon-theme.noarch google-droid* elementary-xfce-icon-theme.noarch xfce4-volumed.x86_64 pavucontrol.x86_64</span></span><br /><br /><ul><li><i>Positive aspects:</i> fast and lean, great start menu. </li><li><i>Negative aspects:</i> external monitor support could be more automatic like Gnome-Shell, no nice overview of all windows, default installation can be a bit too bare, sometimes not sexy (volume applet is ugly, xubuntu provides the unity indicators in xfce as a remedy), primitive OSD.</li></ul><span style="font-size: large;"><br /></span><span style="font-size: large;">Cinnamon, Unity, Conclusion</span><br /><br />I gave a short try to cinnamon as well, in hopes that it was more stable than gnome shell. In short, it was not. It's certainly less of a memory hog, but I had some strange behavior with an additional phantom panel sometimes appearing at the bottom at the screen. And overall it looks a lot less polished.<br /><br />Unity is more interesting, but it's too Ubuntu centric, I don't like the start button equivalent (slow, badly presented, don't care about HUD), the windows overview is not as useful as Gnome shell, the dock, something I usually like, is strangely annoying.<br /><br />This is a very subjective review, my feeling is that in 2014, people should not waste their time with KDE or Cinnamon. Gnome shell could be worth a try if you don't care so much about memory leaks and slight instability but value a distraction free desktop. Otherwise go for XFCE or Unity on (X)ubuntu.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/two-sabr-for-the-same-smile/">Two SABR for the same smile</a>
  </h1>
  <time datetime="2014-05-20T12:08:00Z" class="post-date">Tue, May 20, 2014</time>
  <p>While playing around with <!-- raw HTML omitted -->differential evolution<!-- raw HTML omitted --> &amp; SABR calibration, I noticed that sometimes, several set of parameters can lead to a very similar smile, usually the good one is for relatively low vol of vol and the bad one is for relatively high vol of vol. I first looked for errors in my implementation, but it&rsquo;s a real phenomenon.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->I used the normal implied volatility formula with beta=1, then converted it to lognormal (Black) volatility. While it might not be a great idea to rely on the normal formula with beta=1, I noticed the same phenomenon with the <!-- raw HTML omitted -->arbitrage free PDE density approach<!-- raw HTML omitted -->, especially for long maturities. Interestingly, I did not notice such behavior before with other stochastic volatility models like Heston or Schobel-Zhu: I suspect it has to do with the approximations rather than with the true behavior of SABR.<!-- raw HTML omitted --><!-- raw HTML omitted -->Differential evolution is surprisingly good at finding the global minimum without much initial knowledge, however when there are close fits like this it can be more problematic, usually this requires pushing the population size up. I find that differential evolution is a neat way to test the robustness (as well as performance) of different SABR algorithms as it will try many crazy sets.<!-- raw HTML omitted --><!-- raw HTML omitted -->In practice, for real world calibration, there is not much use of differential evolution to calibrate SABR as it is relatively simple to find a good initial guess.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston-vs-sabr-slice-by-slice-fit/">Heston vs SABR slice by slice fit</a>
  </h1>
  <time datetime="2014-05-15T22:06:00Z" class="post-date">Thu, May 15, 2014</time>
  <p>Some people use <!-- raw HTML omitted -->Heston to fit one slice <!-- raw HTML omitted -->of a volatility surface. In this case, some parameters are clearly redundant. Still, I was wondering how it fared against SABR, which is always used to fit a slice. And what about Schobel-Zhu?<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Aggregated error in fit per slice on 10 surfaces<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->With Heston, the calibration is actually slightly better with kappa=0, that is, without mean reversion, because the global optimization is easier and the mean reversion is fully redundant. It&rsquo;s still quite remarkable that 3 parameters result in a fit as good as 5 parameters.<!-- raw HTML omitted -->This is however not the case for Schobel-Zhu, where each &ldquo;redundant parameter&rdquo; seem to make a slight difference in the quality of calibration. kappa = 0 deteriorate a little bit the fit (the mean error is clearly higher), and theta near 0 (so calibrating 4 parameters) is also a little worse (although better than kappa = 0). Also interestingly, the five parameters Schobel-Zhu fit is slightly better than Heston, but not so when one reduce the number of free parameters.<!-- raw HTML omitted --><!-- raw HTML omitted -->So what about Heston vs SABR. It is interesting to consider the case of general Beta and Beta=1: it turns out that as confirmed for equities, beta=1 is actually a better choice.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Aggregated error in fit per slice on 10 surfaces<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Overall on my 10 surfaces composed each of around 10 slices, an admittedly small sample, Heston (without mean-reversion) fit is a little bit better than SABR. Also the <!-- raw HTML omitted -->SVI-SABR<!-- raw HTML omitted --> idea from Gatheral is not great: the fit is clearly worse than SABR with Beta=1 and even worse than a simple quadratic.<!-- raw HTML omitted -->Of course the best overall fit is achieved with the classic SVI, because it has 6 parameters while the others have only 3.<!-- raw HTML omitted --><!-- raw HTML omitted -->All the calibrations so far were done slice by slice independently, using levenberg marquardt on an initial guess found by differential evolution. Some people advocate for speed or stability of parameters reasons the idea of calibrating each slice using the previous slice as initial guess with a local optimizer like levenberg marquardt, in a bootstrapping fashion.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->The results can be quite different, especially for SVI, which then becomes the worst, even worse than SVI-SABR, which is actually a subset of SVI with fewer parameters. How can this be? <!-- raw HTML omitted --><!-- raw HTML omitted -->This is because as the number of parameters increases, the first slices optimizations have a disproportionate influence, and finding the real minimum is much more difficult, even with differential evolution for the first slice. It&rsquo;s easy to picture that you&rsquo;ll have much more chances to get stuck in some local minimum.<!-- raw HTML omitted -->It&rsquo;s interesting to note that the real stochastic volatility models are actually better behaved in this regard, but I am not so sure that this kind of calibration is such a great idea in general.<!-- raw HTML omitted --><!-- raw HTML omitted -->In practice, the SVI parameters fitted independently evolve in a given surface on each slice in a smooth manner, mostly monotonically. It&rsquo;s just that to go from one set on one slice to the other on the next slice, you might have to do something more than a local optimization.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/quadratic-spline-with-knots-at-mid-points/">Quadratic Spline with Knots at Mid-Points</a>
  </h1>
  <time datetime="2014-05-14T14:12:00Z" class="post-date">Wed, May 14, 2014</time>
  <p>Two months ago, I looked at <!-- raw HTML omitted -->arbitrage free interpolation using piecewise-constant density<!-- raw HTML omitted -->. This is equivalent to a piecewise quadratic polynomial in call prices where each piece is centered around each call strike.<!-- raw HTML omitted --><!-- raw HTML omitted -->I wondered at the time what a quadratic spline would look like on this problem, as it should be very close in theory, except that we can ensure that it is C1, a condition for a good looking implied volatility.<!-- raw HTML omitted --><!-- raw HTML omitted -->For a while, I did not find any references around splines where knots are in between two interpolation points and derived my own formula. And then I lost the paper, but out of curiosity, I looked at the excellent De Boor book &ldquo;<!-- raw HTML omitted -->A Practical Guide to Splines<!-- raw HTML omitted -->&rdquo; and found that there was actually a chapter around this: quadratic splines with knots at mid-points. Interestingly, it turns out that a quadratic spline on standard knots is not always well defined, which is why, if one does quadratic splines, the knots need to be moved.<!-- raw HTML omitted --><!-- raw HTML omitted -->The papers from this era are quite rudimentary in their presentation (the book is much better). I found the paper from Demko 1977 &ldquo;<!-- raw HTML omitted -->Interpolation by Quadratic Splines<!-- raw HTML omitted -->&rdquo; quite usable for coding. I adjusted the boundaries to make the first and last quadratic fit the first two/last two strikes (adding a first strike at 0 and a large last strike if necessary) and spend countless time worrying about indices. The result on a simple classic example is interesting.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->On the non monotonic discrete density data of my earlier blog entry, this gives:<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->QSpline is the quadratic spline<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Unfortunately, interpolating small prices with such a spline results in a highly oscillating interpolation: this is the <!-- raw HTML omitted -->Gibbs phenomenon for splines<!-- raw HTML omitted -->. We need to loose strict C1 continuity for practical applications, and use a first derivative approximation instead, very much like the Harmonic cubic spline.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->On Jaeckel data, the quadratic spline on prices is highly oscillating<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/10/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/12/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
