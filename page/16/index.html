<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.83.1" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2021. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/">Exact Forward in Monte-Carlo</a>
  </h1>
  <time datetime="2013-05-13T17:58:00Z" class="post-date">Mon, May 13, 2013</time>
  <p>Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.<!-- raw HTML omitted --><!-- raw HTML omitted -->In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe &ldquo;The Equity Option Volatility Smile: a Finite Difference Approach&rdquo; which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices.<!-- raw HTML omitted --><!-- raw HTML omitted -->Code has been updated and roll-backed, people have complained around it. But nobody really made the effort to simply write clearly what&rsquo;s going on, or even write a unit test around it. So it was just FUD, until <!-- raw HTML omitted -->this paper<!-- raw HTML omitted -->.<!-- raw HTML omitted --><!-- raw HTML omitted -->In short, for log-Euler, one can use the intuitive forward drift rate: r1<em>t1-r0</em>t0 (ratio of discount factors), but for Euler, one need to use a less intuitive forward drift rate to reproduce a nearly exact forward price.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/">Quasi Monte Carlo in Finance</a>
  </h1>
  <time datetime="2013-05-13T13:16:00Z" class="post-date">Mon, May 13, 2013</time>
  <p>I have been wondering if there was any better alternative than the standard Sobol (+ Brownian Bridge) quasi random sequence generator for the Monte Carlo simulations of finance derivatives.<!-- raw HTML omitted --><!-- raw HTML omitted -->Here is what I found:<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Scrambled Sobol. The idea is to rerandomize the quasi random numbers slightly. It can provide better uniformity properties and allows for a real estimate of the standard error. There are many ways to do that. The simple Cranley Patterson rotation consisting in adding a pseudo random number modulo 1, Owen scrambling (permutations of the digits) and simplifications of it to achieve a reasonable speed. This is all very well described in <!-- raw HTML omitted -->Owen Quasi Monte Carlo document<!-- raw HTML omitted --> <!-- raw HTML omitted --><!-- raw HTML omitted -->Lattice rules. It is another form of quasi random sequences, which so far was not very well adapted to finance problems. A <!-- raw HTML omitted -->presentation from Giles &amp; Kuo<!-- raw HTML omitted --> look like it&rsquo;s changing.<!-- raw HTML omitted --><!-- raw HTML omitted -->Fast PCA. An alternative to Brownian Bridge is the standard PCA. The problem with PCA is the performance in O(n^2). A possible speedup is possible in the case of a equidistant time steps. <!-- raw HTML omitted -->This paper<!-- raw HTML omitted --> shows it can be generalized. But the data in it shows it is only advantageous for more than 1024 steps - not so interesting in Finance.<!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/time-estimates-in-software-development/">Time Estimates in Software Development</a>
  </h1>
  <time datetime="2013-05-07T21:01:00Z" class="post-date">Tue, May 7, 2013</time>
  <p>Recently, that I completed a project that I had initially estimated to around 2 months,  in nearly 4 hours. This morning I fixed the few remaining bugs. I looked at the clock, surprised it was still so early and I still had so many hours left in the day.<!-- raw HTML omitted --><!-- raw HTML omitted -->Now I have more time to polish the details and go beyond the initial goal (I think this scares my manager a bit), but I could (and I believe some people do this often) stop now and all the management would be satisfied.<!-- raw HTML omitted --><!-- raw HTML omitted -->What&rsquo;s interesting is that everybody bought the 2 months estimate without questions (I almost even believed it myself). This reminded me of my <!-- raw HTML omitted -->productivity zero post<!-- raw HTML omitted -->.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/">Upper Bounds in American Monte-Carlo</a>
  </h1>
  <time datetime="2013-04-30T17:05:00Z" class="post-date">Tue, Apr 30, 2013</time>
  <p><!-- raw HTML omitted -->Glasserman and Yu<!-- raw HTML omitted --> (GY) give a relatively simple algorithm to compute lower and upper bounds of a the price of a Bermudan Option through Monte-Carlo.<!-- raw HTML omitted --><!-- raw HTML omitted -->I always thought it was very computer intensive to produce an upper bound, and that the standard <!-- raw HTML omitted -->Longstaff Schwartz algorithm<!-- raw HTML omitted --> was quite precise already. GY algorithm is not much slower than the Longstaff-Schwartz algorithm, but what&rsquo;s a bit tricky is the choice of basis functions: they have to be Martingales. This is the detail I overlooked at first and I, then, could not understand why my results were so bad. I looked for a coding mistake for several hours before I figured out that my basis functions were not Martingales. Still it is possible to find good Martingales for the simple Bermudan Put option case and GY actually propose some <!-- raw HTML omitted -->in another paper<!-- raw HTML omitted -->.<!-- raw HTML omitted --><!-- raw HTML omitted -->Here are some preliminary results where I compare the random number generator influence and the different methods. I include results for GY using In-the-money paths only for the regression (-In suffix) or all (no suffix). <!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->value using 16k training path and Sobol - GY-Low-In is very close to LS.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->error using 16k training path - a high number of simulations not that useful<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->error using 1m simulation paths - GY basis functions require less training than LS<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->One can see the the upper bound is not that precise compared to the lower bound estimate, and that using only in the money paths makes a big difference. GY regression is good with only 1k paths, LS requires 10x more.<!-- raw HTML omitted --><!-- raw HTML omitted -->Surprisingly, I noticed that the Brownian bridge variance reduction applied to Sobol was  increasing the GY low estimate, so as to make it sometimes slightly higher than Longstaff-Schwartz price.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/the-wonderful-un/">The Wonderful UN</a>
  </h1>
  <time datetime="2013-04-24T21:35:00Z" class="post-date">Wed, Apr 24, 2013</time>
  <p>Already the name United Nations should be suspicious, but now <!-- raw HTML omitted -->they are shown to have spread Cholera<!-- raw HTML omitted --> to Haiti, as if the country did not have enough suffering. They have a nice building in New-York, and used to have a popular representative, but unfortunately, for poor countries, they never really achieved much. In Haiti, there were many stories of rapes and corruption by U.N. members more than 10 years ago. The movie<!-- raw HTML omitted --> <!-- raw HTML omitted -->The Whistleblower<!-- raw HTML omitted --> <!-- raw HTML omitted -->suggests it was the same in the Balkans. I am sure it did not change much since.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/moved-from-dropbox-to-ubuntu-one/">Moved From Dropbox to Ubuntu One</a>
  </h1>
  <time datetime="2013-04-23T20:44:00Z" class="post-date">Tue, Apr 23, 2013</time>
  <p>Dropbox worked well, but the company decided to blacklist it. I suppose some people abused it. While looking for an alternative, I found <!-- raw HTML omitted -->Ubuntu One<!-- raw HTML omitted -->. It&rsquo;s funny I never tried it before even though I use Ubuntu. I did not think it was a dropbox replacement, but it is. And you get 5GB instead of Dropbox 2GB limit, which is enough for me (I was a bit above the 2GB limit). It works well under Linux but as well on Android and there is an iOS app I have not yet tried. It also works on Windows and Mac.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-longstaff-schwartz-american-option-price/">Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</a>
  </h1>
  <time datetime="2013-04-22T18:00:00Z" class="post-date">Mon, Apr 22, 2013</time>
  <p>In the book <!-- raw HTML omitted --><!-- raw HTML omitted -->Monte Carlo Methods in Financial Engineering<!-- raw HTML omitted --><!-- raw HTML omitted -->, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.<!-- raw HTML omitted --><!-- raw HTML omitted -->However Longstaff and Schwartz seem to just reuse the paths in <!-- raw HTML omitted -->their paper<!-- raw HTML omitted -->, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.<!-- raw HTML omitted --><!-- raw HTML omitted -->How large is this bias? What is the correct methodology?<!-- raw HTML omitted --><!-- raw HTML omitted -->I have tried with Sobol quasi random numbers to evaluate that bias on a simple Bermudan put option of maturity 180 days, exercisable at 30 days, 60 days, 120 days and 180 days using a Black Scholes volatility of 20% and a dividend yield of 6%. As a reference I use <!-- raw HTML omitted -->a finite difference solver based on TR-BDF2<!-- raw HTML omitted -->.<!-- raw HTML omitted --><!-- raw HTML omitted -->I found it particularly difficult to evaluate it: should we use the same number of paths for the 2 methods or should we use the same number of paths for the monte carlo mean computation only? Should we use the same number of paths for regression and for monte carlo mean computation or should the monte carlo mean computation use much more paths?<!-- raw HTML omitted --><!-- raw HTML omitted -->I have tried those combinations and was able to clearly see the bias only in one case: a large number of paths for the Monte-Carlo mean computation compared to the number of paths used for the regression using a fixed total number of paths of 256<em>1024+1, and 32</em>1024+1 paths for the regression.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->FDM price=2.83858387194312<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Longstaff discarded paths price=2.8385854695510426 <!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Longstaff reused paths price=2.8386108892756847<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Those numbers are too good to be a real. If one reduces too much the total number of paths or the number of paths for the regression, the result is not precise enough to see the bias. For example, using 4K paths for the regression leads to 2.83770 vs 2.83767. Using 4K paths for regression and only 16K paths in total leads to 2.8383 vs 2.8387. Using 32K paths for regressions and increasing to 1M paths in total leads to 2.838539 vs 2.838546.<!-- raw HTML omitted --><!-- raw HTML omitted -->For this example the Longstaff-Schwartz price is biased low, the slight increase due to path reuse is not very visible and most of the time does not deteriorate the overall accuracy. But as a result of reusing the paths, the Longstaff-Schwartz price might be higher than the real value.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/a-fast-exponential-function-in-java/">A Fast Exponential Function in Java</a>
  </h1>
  <time datetime="2013-04-19T16:48:00Z" class="post-date">Fri, Apr 19, 2013</time>
   

In finance, because one often dicretize the log process instead of the direct process for Monte-Carlo simulation, the Math.exp function can be called a lot (millions of times for a simulation) and can be a bottleneck. I have noticed that the simpler Euler discretization was for local volatility Monte-Carlo around 30% faster, because it avoids the use of Math.exp.<br /><br />Can we improve the speed of exp over the JDK one? At first it would seem that the JDK would just call either the processor exp using an <a href="http://bad-concurrency.blogspot.co.uk/2012/08/arithmetic-overflow-and-intrinsics.html">intrinsic function call</a> and that should be difficult to beat. However what if one is ok for a bit lower accuracy? Could a simple <a href="http://www.siam.org/books/ot99/OT99SampleChapter.pdf">Chebyshev polynomial expansion</a> be faster?<br /><br />Out of curiosity, I tried a Chebyshev polynomial expansion with 10 coefficients stored in a final double array. I computed the coefficient using a precise quadrature (Newton-Cotes) and end up with 1E-9, 1E-10 absolute and relative accuracy on [-1,1].<br /><br />Here are the results of a simple sum of 10M random numbers:<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">0.75s for Math.exp sum=1.7182816693332244E7<br />0.48s for ChebyshevExp sum=1.718281669341388E7<br />0.40s for FastMath.exp sum=1.7182816693332244E7</span><br /><br />So while this simple implementation is actually faster than Math.exp (but only works within [-1,1]), FastMath from Apache commons maths, that relies on a table lookup algorithm is just faster (in addition to being more precise and not limited to [-1,1]).<br /><br />Of course if I use only 5 coefficients, the speed is better, but the relative error becomes around 1e-4 which is unlikely to be satisfying for a finance application.<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">0.78s for Math.exp sum=1.7182816693332244E7<br />0.27s for ChebyshevExp sum=1.718193001875838E7<br />0.40s for FastMath.exp sum=1.7182816693332244E7</span>



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii/">Root finding in Lord Kahl Method to Compute Heston Call Price (Part III)</a>
  </h1>
  <time datetime="2013-04-12T13:41:00Z" class="post-date">Fri, Apr 12, 2013</time>
  <p>I forgot two important points in my <!-- raw HTML omitted -->previous post<!-- raw HTML omitted --> about <!-- raw HTML omitted -->Lord-Kahl method<!-- raw HTML omitted --> to compute the Heston call price:<!-- raw HTML omitted --><!-- raw HTML omitted -->- Scaling: scaling the call price appropriately allows to increase the maximum precision significantly, because the <!-- raw HTML omitted -->Carr-Madan<!-- raw HTML omitted --> formula operates on log(Forward) and log(Strike) directly, but not the ratio, and alpha is multiplied by the log(Forward). I simply scale by the spot, the call price is (S_0*max(S/S_0-K/S0)). Here are the results for <!-- raw HTML omitted -->Lord-Kahl<!-- raw HTML omitted -->, <!-- raw HTML omitted -->Kahl-Jaeckel<!-- raw HTML omitted --> (the more usual way limited to machine epsilon accuracy), <!-- raw HTML omitted -->Forde-Jacquier-Lee<!-- raw HTML omitted --> ATM implied volatility without scaling for a maturity of 1 day:<!-- raw HTML omitted --><!-- raw HTML omitted -->strike 62.5=2.919316809400033E-34 8.405720564041985E-12 0.0<!-- raw HTML omitted -->strike 68.75=-8.923683388191852E-28 1.000266536266281E-11 0.0<!-- raw HTML omitted -->strike 75.0=-3.2319611910032E-22 2.454925152051146E-12 0.0<!-- raw HTML omitted -->strike 81.25=1.9401743410877718E-16 2.104982854689297E-12 0.0 <!-- raw HTML omitted -->strike 87.5=-Infinity -1.6480150577535824E-11 0.0<!-- raw HTML omitted -->strike 93.75=Infinity 1.8277663826893331E-9 1.948392142070432E-9<!-- raw HTML omitted -->strike 100.0=0.4174318393886519 0.41743183938679845 0.4174314959743768<!-- raw HTML omitted -->strike 106.25=1.326968012594355E-11 7.575717830832218E-11 1.1186618909114702E-11<!-- raw HTML omitted -->strike 112.5=-5.205783145942609E-21 2.5307755890935368E-11 6.719872683111381E-45<!-- raw HTML omitted -->strike 118.75=4.537094156599318E-25 1.8911094912255066E-11 3.615356241778357E-114<!-- raw HTML omitted -->strike 125.0=1.006555799739525E-27 3.2365221613872563E-12 2.3126009701775733E-240<!-- raw HTML omitted -->strike  131.25=4.4339539263484925E-31 2.4794388764348696E-11 0.0<!-- raw HTML omitted --><!-- raw HTML omitted -->One can see negative prices and meaningless prices outside ATM. With scaling it changes to: <!-- raw HTML omitted -->strike 62.5=2.6668642552659466E-182 8.405720564041985E-12 0.0<!-- raw HTML omitted -->strike 68.75=7.156278101597845E-132 1.000266536266281E-11 0.0<!-- raw HTML omitted -->strike 81.25=7.863105641534119E-55 2.104982854689297E-12 0.0<!-- raw HTML omitted -->strike 87.5=7.073641308465115E-28 -1.6480150577535824E-11 0.0<!-- raw HTML omitted -->strike 93.75=1.8375145950924849E-9 1.8277663826893331E-9 1.948392142070432E-9<!-- raw HTML omitted -->strike 100.0=0.41743183938755385 0.41743183938679845 0.4174314959743768<!-- raw HTML omitted -->strike 106.25=1.3269785342953315E-11 7.575717830832218E-11 1.1186618909114702E-11<!-- raw HTML omitted -->strike 112.5=8.803247187972696E-42 2.5307755890935368E-11 6.719872683111381E-45<!-- raw HTML omitted -->strike 118.75=5.594342441346233E-90 1.8911094912255066E-11 3.615356241778357E-114<!-- raw HTML omitted -->strike 125.0=7.6539757567179276E-149 3.2365221613872563E-12 2.3126009701775733E-240<!-- raw HTML omitted -->strike 131.25=0.0 2.4794388764348696E-11 0.0<!-- raw HTML omitted --><!-- raw HTML omitted -->One can now now see that the Jacquier-Lee approximation is quickly not very good.<!-- raw HTML omitted --><!-- raw HTML omitted -->- Put: the put option price can be computed using the exact same <!-- raw HTML omitted -->Carr-Madan <!-- raw HTML omitted -->formula, but using a negative alpha instead of a positive alpha. When I derived this result (by just reproducing the Carr-Madan steps with the put payoff instead of the call payoff), I was surprised, but it works.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii/">Root finding in Lord Kahl Method to Compute Heston Call Price (Part II)</a>
  </h1>
  <time datetime="2013-04-11T16:29:00Z" class="post-date">Thu, Apr 11, 2013</time>
  <p>In my <!-- raw HTML omitted -->previous post<!-- raw HTML omitted -->, I explored the <!-- raw HTML omitted -->Lord-Kahl method<!-- raw HTML omitted --> to compute the call option prices under the Heston model. One of the advantages of this method is to go beyond machine epsilon accuracy and be able to compute very far out of the money prices or very short maturities. The standard methods to compute the Heston price are based on a sum/difference where both sides are far from 0 and will therefore be limited to less than machine epsilon accuracy even if the integration is very precise.<!-- raw HTML omitted --><!-- raw HTML omitted -->However the big trick in it is to find the optimal alpha used in the integration. A suboptimal alpha will often lead to high inaccuracy, because of some strong oscillations that will appear in the integration. So the method is robust only if the root finding (for the optimal alpha) is robust.<!-- raw HTML omitted --><!-- raw HTML omitted -->The original paper looks the Ricatti equation for B where B is the following term in the characteristic function:
$$\phi(u) = e^{iuf+A(u,t)+B(u,t)\sigma_0}$$</p>
<p>The solution defines the \(\alpha_{max}\) where the characteristic function explodes. While the Ricatti equation is complex but not complicated:
$$ dB/dt = \hat{\alpha}(u)-\beta(u) B+\gamma B^2 $$</p>
<p>I initially did not understand its role (to compute \(\alpha_{max}\)), so that, later, one can compute alpha_optimal with a good bracketing. The bracketing is particularly important to use a decent solver, like the Brent solver. Otherwise, one is left with, mostly, Newton&rsquo;s method. It turns out that I explored a reduced function, which is quite simpler than the Ricatti and seems to work in all the cases I have found/tried: solve $$1/B = 0$$<!-- raw HTML omitted --> If B explodes, \(\phi\) will explode. The trick, like when solving the Ricatti equation, is to have either a good starting point (for Newton) or, better, a bracketing. It turns out that Lord and Kahl give a bracketing for (1/B), even if they don&rsquo;t present it like this: their \(\tau_{D+}\) on page 10 for the lower bracket, and \(\tau_+\) for the upper bracket. \(\tau_+\) will make \(1/B\) explode, exactly. One could also find the next periods by adding \(4\pi/t\) instead of \(2\pi/t\) like they do to move from \(\tau_{D+}\) to \(\tau_+\). But this does not have much interest as we don&rsquo;t want to go past the first explosion.<!-- raw HTML omitted --><!-- raw HTML omitted -->It&rsquo;s quite interesting to see that my simple approach is actually closely related to the more involved Ricatti approach. The starting point could be the same. Although it is much more robust to just use Brent solver on the bracketed max. I actually believe that the Ricatti equation explodes at the same points, except, maybe for some rare combination of Heston parameters.<!-- raw HTML omitted --><!-- raw HTML omitted -->From a coding perspective, I found that <!-- raw HTML omitted -->Apache commons maths<!-- raw HTML omitted --> was a decent library to do complex calculus or solve/minimize functions. The complex part was better than some in-house implementation: for example the square root was more precise in commons maths, and the solvers are robust. It even made me think that it is often a mistake to reinvent to wheel. It&rsquo;s good to choose the best implementations/algorithms as possible. But reinventing a Brent solver??? a linear interpolator??? Also the commons maths library imposes a good structure. In house stuff tends to be messy (not real interfaces, or many different ones). I believe the right approach is to use and embrace/extends Apache commons maths. If some algorithms are badly coded/not performing well, then write your own using the same kind of interfaces as commons maths (or some other good maths library).<!-- raw HTML omitted --><!-- raw HTML omitted -->The next part of this series on Lord-Kahl method is <!-- raw HTML omitted -->here<!-- raw HTML omitted -->.</p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/15/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/17/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
