<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.142.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2025. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/brownian-bridge-or-not-with-heston-quadratic-exponential-qmc/">Brownian Bridge or Not with Heston Quadratic Exponential QMC</a>
  </h1>
  <time datetime="2014-01-24T19:35:00Z" class="post-date">Fri, Jan 24, 2014</time>
   

At first I did not make use of the Brownian Bridge technique in Heston QMC, because the variance process is not simulated like a Brownian Motion under the <a href="http://www.google.com/url?q=http://www.javaquant.net/papers/LeifAndersenHeston.pdf&amp;sa=U&amp;ei=p9jjUtPJK8uUhQfb_YGgAw&amp;ved=0CB4QFjAA&amp;sig2=EjKPr39tR0ni1vq5gK9mkA&amp;usg=AFQjCNGQyqSaDu2kl6_KpP-s-KwMvJ6hPg">Quadratic Exponential algorithm from Andersen</a>.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-kWmb0WWAYzA/UuPX4UMySRI/AAAAAAAAG-4/cBcjRgfTc5s/s1600/Screenshot+from+2014-01-25+16:26:05.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-kWmb0WWAYzA/UuPX4UMySRI/AAAAAAAAG-4/cBcjRgfTc5s/s1600/Screenshot+from+2014-01-25+16:26:05.png" /></a></div><br />It is, however, perfectly possible to use the Brownian Bridge on the asset process. Does it make a difference? In my small test, it does not seem to make a difference. An additional question would be, is it better to take first N for the asset and next N for the variance or vice versa or intertwined? Intertwined would seem the most natural (this is what I used without Brownian Bridge, but for simplicity I did Brownian bridge on first N).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-WN5frRWHix4/UuKyR69BtbI/AAAAAAAAG-g/BTXm5K2XOQo/s1600/heston_bnp_sobol2.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-WN5frRWHix4/UuKyR69BtbI/AAAAAAAAG-g/BTXm5K2XOQo/s1600/heston_bnp_sobol2.png" height="222" width="320" /></a></div><br /><br />By contrast, Schobel-Zhu QE scheme can make full use of the Brownian Bridge technique, in the asset process as well as in the variance process. Here is a summary of the volatility process under the QE scheme from <a href="http://www.google.com/url?q=http://arno.uvt.nl/show.cgi%3Ffid%3D99534&amp;sa=U&amp;ei=itjjUo2jD6Wa0AXqzYHgCQ&amp;ved=0CB4QFjAA&amp;sig2=KdYdo3sAFjV6fwPS4Gak0A&amp;usg=AFQjCNEZ8DRGCf4EXOxCXs3oPc_98628hA">van Haastrecht</a>:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-kYpBO8PfEkw/UuPX_40DnbI/AAAAAAAAG_A/v7QAN_lX9oM/s1600/Screenshot+from+2014-01-25+16:26:23.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-kYpBO8PfEkw/UuPX_40DnbI/AAAAAAAAG_A/v7QAN_lX9oM/s1600/Screenshot+from+2014-01-25+16:26:23.png" /></a></div><br />Another nice property of Schobel-Zhu is that the QE simulation is as fast as Euler, and therefore 2.5x faster than the Heston QE.<br /><br />I calibrated the model to the same surface, and the QMC price of a ATM call option seems to have a similar accuracy as Heston QMC. But we can see that the Brownian Bridge does increase accuracy in this case. I was surprised that accuracy was not much better than Heston, but maybe it is because I did yet not implement the Martingale correction, while I did so in the Heston case.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/--mK4u1K8nOk/UuKyR2y6FHI/AAAAAAAAG-k/QSOjDwosqLo/s1600/sz_bnp_sobol3.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/--mK4u1K8nOk/UuKyR2y6FHI/AAAAAAAAG-k/QSOjDwosqLo/s1600/sz_bnp_sobol3.png" height="222" width="320" /></a></div><br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/adjoint-algorithmic-differentiation-for-black-scholes/">Adjoint Algorithmic Differentiation for Black-Scholes</a>
  </h1>
  <time datetime="2014-01-21T13:03:00Z" class="post-date">Tue, Jan 21, 2014</time>
   

<a href="http://en.wikipedia.org/wiki/Automatic_differentiation">Adjoint algorithmic differentiation</a> is particularly interesting in finance as we often encounter the case of a function that takes many input (the market data) and returns one output (the price) and we would like to also compute sensitivities (greeks) to each input.<br /><br />As I am just starting around it, to get a better grasp, I first tried to apply the idea to the analytic knock out barrier option formula, by hand, only to find out I was making way too many errors by hand to verify anything. So I tried the simpler vanilla Black-Scholes formula. I also made various errors, but managed to fix all of them relatively easily.<br /><br />I decided to compare how much time it took to compute price, delta, vega, theta, rho, rho2 between single sided finite difference and the adjoint approach. Here are the results for 1 million options:<br /><br /><blockquote class="tr_bq">FD time=2.13s<br />Adjoint time=0.63s</blockquote><br /><b>It works well</b>, but doing it by hand is crazy and too error prone. It might be simpler for Monte-Carlo payoffs however.<br /><br />There are not many Java tools that can do reverse automatic differentiation, I found <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=5&amp;cad=rja&amp;ved=0CFAQFjAE&amp;url=http%3A%2F%2Fcluster.grid.pub.ro%2Fwiki%2Findex.php%2FADiJaC_-_Automatic_Differentiation_of_Java_Classfiles&amp;ei=SmDeUtDLCYrG0QWrlIHQDw&amp;usg=AFQjCNHLMwobO6u5jSoUifCdHvE2yPZ6oQ&amp;sig2=wFbGRYjgLenCNGRbrQ1gKQ&amp;bvm=bv.59568121,d.d2k">some thesis</a> on it, with an interesting byte code oriented approach (one difficulty is that you need to reverse loops, while statements).



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/placing-the-strike-on-the-grid-and-payoff-smoothing-in-finite-difference-methods-for-vanilla-options/">Placing the Strike on the Grid and Payoff Smoothing in Finite Difference Methods for Vanilla Options</a>
  </h1>
  <time datetime="2014-01-12T16:27:00Z" class="post-date">Sun, Jan 12, 2014</time>
   

Pooley et al., in <a href="https://cs.uwaterloo.ca/~paforsyt/report.pdf">Convergence Remedies for non-smooth payoffs in option pricing</a> suggest that placing the strike on the grid for a Vanilla option is good enough:<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-CIS-yTdMn7k/UtKy6HtMIhI/AAAAAAAAG-I/Hef_w7gTcJ0/s1600/pooley_vanilla_smooth.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-CIS-yTdMn7k/UtKy6HtMIhI/AAAAAAAAG-I/Hef_w7gTcJ0/s1600/pooley_vanilla_smooth.png" height="48" width="640" /></a></div><br />At the same time, Tavella and Randall show in their book that numerically, placing the strike in the middle of two nodes leads to a more accurate result. My own numerical experiments confirm Tavella and Randall suggestion.<br /><br />In reality, what Pooley et al. really mean, is that quadratic convergence is maintained if the strike is on the grid for vanilla payoffs, contrary to the case of discontinuous payoffs (like digital options) where the convergence decreases to order 1. So it's ok to place the strike on the grid for a vanilla payoff, but it's not optimal, it's still better to place it in the middle of two nodes. Here are absolute errors in a put option price:<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">on grid, no smoothing&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.04473021824995271</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">on grid, Simpson smoothing&nbsp;&nbsp;&nbsp; 0.003942854282069419<br />on grid, projection smoothing 0.044730218065351934<br />middle, no smoothing&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.004040359609906119</span><br /><br />As expected (and mentioned in Pooley et al.), the projection does not do anything here. When the grid size is doubled, the convergence ratio of all methods is the same (order 2), but placing the strike in the middle still increases accuracy significantly.<br /><br />Here is are the same results, but for a digital put option:<br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">on grid, no smoothing &nbsp; &nbsp; &nbsp; &nbsp; 0.03781319921461046<br />on grid, Simpson smoothing&nbsp;&nbsp;&nbsp; 8.289052335705427E-4<br />on grid, projection smoothing 1.9698293587372406E-4<br />middle, no smoothing&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3.5122153011418744E-4</span><br /><br />Here only the 3 last methods are of order-2 convergence, and projection is in deed the most accurate method, but placing the strike in the middle is really not that much worse, and much simpler.<br /><br />A disadvantage of Simpson smoothing (or smoothing by averaging), is that it breaks put-call parity (see the paper "<a href="http://papers.ssrn.com/abstract=2362969">Exact Forward and Put Call Parity with TR-BDF2</a>") <br /><br />I think the emphasis in their paper on "no smoothing is required" for vanilla payoffs can be misleading. I hope I have clarified it in this post.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/coordinate-transform-of-the-andreasen-huge-sabr-pde--spline-interpolation/">Coordinate Transform of the Andreasen Huge SABR PDE &amp; Spline Interpolation</a>
  </h1>
  <time datetime="2014-01-08T18:51:00Z" class="post-date">Wed, Jan 8, 2014</time>
  <p>Recently, I noticed <a href="/post/arbitrage-free-sabr---another-view-on-hagan-approach">how close</a> are the two PDE based approaches from Andreasen-Huge and Hagan for an arbitrage free SABR. Hagan gives a local volatility very close to the one Andreasen-Huge use in the forward PDE in call prices. A multistep Andreasen-Huge (instead of their one step PDE method) gives back prices and densities nearly equal to Hagan density based approach.</p>
<p>Hagan proposed in some unpublished paper a coordinate transformation for two reasons: the ideal range of strikes for the PDE can be very large, and concentrating the points where it matters should improve stability and accuracy. The transform itself can be found in the <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fwww.andersen-piterbarg-book.com%2F&amp;ei=dYzNUrG6Eo7n7Aamp4GwAQ&amp;usg=AFQjCNE3sdrH2B8EDg40Gocp8FB-QEtnew&amp;sig2=aoDaRX5-zTolem9mUrEumw&amp;bvm=bv.58187178,d.ZGU">Andersen-Piterbarg book</a> &ldquo;Interest Rate Modeling&rdquo;, and is similar to the famous log transform, but for a general local volatility function (phi in the book notation).</p>
<figure><img src="/post/piterbarg_lv_transform1.png">
</figure>

<p>There are two ways to transform Andreasen Huge PDE:</p>
<ul>
<li>through a non-uniform grid: the input strikes are directly transformed based on a uniform grid in the inverse transformed grid (paying attention to still put the strike in the middle of two points). This is detailed in the Andersen-Piterbarg book.
<figure><img src="/post/piterbarg_lv_transform2.png">
</figure>
</li>
<li>through a variable transform in the PDE: this gives a slightly different PDE to solve. One still needs to convert then a given strike, to the new PDE variable. This kind of transform is detailed in the <a href="http://eu.wiley.com/WileyCDA/WileyTitle/productCd-0471197602.html">Tavella-Randall book</a> &ldquo;Pricing Financial Instruments: the Finite Difference Method&rdquo;, for example.
<figure><img src="/post/tavella_lv_transform3.png">
</figure>
</li>
</ul>
<p>Both are more or less equivalent. I would expect the later to be slightly more precise but I tried the former as it is simpler to test if you have non uniform parabolic PDE solvers.</p>
<p>It works very well, but I found an interesting issue when computing the density (second derivative of the call price): if one relies on a Hermite kind of spline (Bessel/Parabolic or Harmonic), the density wiggles around. The C2 cubic spline solves this problem as it is C2. Initially I thought those wiggles could be produced because the interpolation did not respect monotonicity and I tried a Hyman monotonic cubic spline out of curiosity, it did not change anything (in an earlier version of this post I had a bug in my Hyman filter) as it preserves monotonicity but not convexity. The wiggles are only an effect of the approximation of the derivatives value.</p>
<figure><img src="/post/ah_dens3.png">
</figure>

<p>Initially, I did not notice this on the uniform discretization mostly because I used a large number of strikes in the PDE (here I use only 50 strikes) but also because the effect is somewhat less pronounced in this case.</p>
<p>I also discovered a bug in my non uniform implementation of Hagan Density PDE, I forgot to take into account an additional dF/dz factor when the density is integrated. As a result, the density was garbage when computed by a numerical difference.</p>
<p><figure><img src="/post/hagan_lv_spline_density2.png"><figcaption>
      <h4>HaganDensity denotes the transformed PDE on density approach. Notice the non-sensical spikes</h4>
    </figcaption>
</figure>

<figure><img src="/post/density_transform_bad.png"><figcaption>
      <h4>Bad Call prices around the forward with Hagan Density PDE. Notice the jumps.</h4>
    </figcaption>
</figure>

<figure><img src="/post/density_transform_good.png"><figcaption>
      <h4>No jumps anymore after the dF/dZ fit </h4>
    </figcaption>
</figure>
</p>
<p><strong>Update March 2014</strong> - I have now a paper with Matlab code <a href="http://ssrn.com/abstract=2402001">Finite Difference Techniques for Arbitrage Free SABR</a></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/random-hardware-issues/">Random Hardware Issues</a>
  </h1>
  <time datetime="2014-01-06T18:44:00Z" class="post-date">Mon, Jan 6, 2014</time>
  <p>Today, after wondering why my desktop computer became so unstable (frequent crashes under Fedora), I found out that the micro usb port of my cell phone has some kind of short circuit. My phone behaves strangely in 2014, it lasted nearly 1 week on battery (I lost it for half of the week), and seems to shutdown for no particular reason once in a while.</p>
<p>On the positive side, I also discovered, after owning my monitor for around 5 years, that it has SD card slots on the side, as well as USB ports. I always used the USB ports of my desktop and never really looked at the side of my monitor&hellip;</p>
<p>I also managed to seriously boost the speed of my home network with a cheap TP-Link wifi router. The one included in the ISP box-modem only supported 802.11g and had really crap coverage, so crap that it was seriously limiting the internet traffic. In the end it was just a matter of disabling wifi and DHCP on the box, adding the new router in the DMZ, and adding a static WAN IP for the box in the router configuration. I did not realize how much of a difference this could make, even on simple websites. I was also surprised that, for some strange reason, routers are cheaper than access points these days.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/levenberg-marquardt--constraints-by-domain-transformation/">Levenberg Marquardt &amp; Constraints by Domain Transformation</a>
  </h1>
  <time datetime="2013-12-17T15:27:00Z" class="post-date">Tue, Dec 17, 2013</time>
   

The Fortran <a href="http://www.netlib.org/minpack/">minpack</a> library has a good <a href="http://en.wikipedia.org/wiki/Levenberg%E2%80%93Marquardt_algorithm">Levenberg-Marquardt</a> minimizer, so good, that it has been ported to many programming languages. Unfortunately it does not support contraints, even simple bounds.<br /><br />One way to achieve this is to transform the domain via a bijective function. For example, \(a+\frac{b-a}{1+e^{-\alpha t}}\) will transform \(]-\infty, +\infty[\) to ]a,b[. Then how should one choose \(\alpha\)?<br /><br />A large \(\alpha\) will make tiny changes in \(t\) appear large. A simple rule is to ensure that \(t\) does not create large changes in the original range ]a,b[, for example we can make \(\alpha t \leq 1\), that is \( \alpha t= \frac{t-a}{b-a} \).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s1600/Screenshot+from+2013-12-17+13:02:22.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="293" src="http://4.bp.blogspot.com/-TJiFg4K2fS4/UrBNcxg9wLI/AAAAAAAAG6Q/8BD0OHbxGhg/s400/Screenshot+from+2013-12-17+13:02:22.png" width="400" /></a></div><br /><br />In practice, for example in the calibration of the<a href="http://en.wikipedia.org/wiki/Heston_model"> Double-Heston</a> model on real data, a naive \( \alpha=1 \) will converge to a RMSE of 0.79%, while our choice will converge to a RMSE of 0.50%. Both will however converge to the same solution if the initial guess is close enough to the real solution. Without any transform, the RMSE is also 0.50%. The difference in error might not seem large but this results in vastly different calibrated parameters. Introducing the transform can significantly change the calibration result, if not done carefully.<br /><br />Another simpler way would be to just impose a cap/floor on the inputs, thus ensuring that nothing is evaluated where it does not make sense. In practice, it however will not always converge as well as the unconstrained problem: the gradient is not defined at the boundary. On the same data, the Schobel-Zhu, unconstrained converges with RMSE 1.08% while the transform converges to 1.22% and the cap/floor to 1.26%. The Schobel-Zhu example is more surprising since the initial guess, as well as the results are not so far:<br />             <style>  <!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:"Liberation Sans"; font-size:x-small }    --> </style>     <br /><table border="0" cellspacing="0" cols="2"> <colgroup span="2" width="85"></colgroup> <tbody><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Initial volatility (v0)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">18.1961174789</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Long run volatility (theta)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Speed of mean reversion (kappa)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">101.2291161766</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Vol of vol (sigma)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">35.2221829015</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Correlation (rho)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">-73.7995231799</span></td> </tr><tr>  <td align="LEFT" height="17" style="border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">ERROR MEASURE</span></td>  <td align="RIGHT" style="border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1.0614889526</span></td> </tr></tbody></table><br />             <style>  <!--    BODY,DIV,TABLE,THEAD,TBODY,TFOOT,TR,TH,TD,P { font-family:"Liberation Sans"; font-size:x-small }    --> </style>     <br /><table border="0" cellspacing="0" cols="2"> <colgroup span="2" width="85"></colgroup> <tbody><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Initial volatility (v0)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">17.1295934569</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Long run volatility (theta)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Speed of mean reversion (kappa)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">67.9818356373</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Vol of vol (sigma)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">30.8491256097</span></td> </tr><tr>  <td align="LEFT" height="16" style="border-bottom: 1px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">Correlation (rho)</span></td>  <td align="RIGHT" style="border-bottom: 1px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">-74.614636128</span></td> </tr><tr>  <td align="LEFT" height="17" style="border-bottom: 3px solid #000000; border-left: 3px solid #000000; border-right: 1px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">ERROR MEASURE</span></td>  <td align="RIGHT" style="border-bottom: 3px solid #000000; border-left: 1px solid #000000; border-right: 3px solid #000000; border-top: 1px solid #000000;" valign="TOP"><span style="font-family: Liberation Serif;">1.2256421987</span></td> </tr></tbody></table><br />The initial guess is kappa=61% theta=11% sigma=26% v0=18% rho=-70%. Only the kappa is different in the two results, and the range on the kappa is (0,2000) (it is expressed in %), much larger than the result. In reality, theta is the issue (in (0,1000)). Forbidding a negative theta has an impact on how kappa is picked. The only way to be closer<br /><br />Finally, a third way is to rely on a simple penalty: returning an arbitrary large number away from the boundary. In our examples this was no better than the transform or the cap/floor.<br /><br />Trying out the various ways, it seemed that allowing meaningless parameters, as long as they work mathematically produced the best results with Levenberg-Marquardt, particularly, allowing for a negative theta in Schobel-Zhu made a difference.


  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/arbitrage-free-sabr---another-view-on-hagan-approach/">Arbitrage Free SABR - Another View on Hagan Approach</a>
  </h1>
  <time datetime="2013-12-14T00:56:00Z" class="post-date">Sat, Dec 14, 2013</time>
  <p>Several months ago, I took a look at <a href="/post/sabr-with-the-new-hagan-pde-approach">two interesting recent ways</a> to price under SABR with no arbitrage:</p>
<ul>
<li>One way is due to <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;ei=F4yrUoL7Kq2M7AasuIFg&amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;sig2=-fFTrJR1wY1elyXBC1EC0A&amp;bvm=bv.57967247,d.ZGU">Andreasen and Huge</a>, where they find an equivalent local volatility expansion, and then use a one-step finite difference technique to price.</li>
<li>The other way is due to Hagan himself, where he numerically solves an approximate PDE in the probability density, and then price with options by integrating on this density.</li>
</ul>
<p>It turns out that the two ways are much closer than I first thought. Hagan PDE in the probability density is actually just the <a href="http://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation">Fokker-Planck</a> (forward) equation.
<figure><img src="/post/Screenshot%20from%202013-12-13%2023%2026%2043.png">
</figure>

The \(\alpha D(F)\) is just the equivalent local volatility. Andreasen and Huge use nearly the same local volatility formula but without the exponential part (that is often negligible except for long maturities), directly in Dupire forward PDE:
<figure><img src="/post/Screenshot%20from%202013-12-13%2023%2031%2018.png">
</figure>

A common derivation (for example in <a href="http://www.amazon.com/The-Volatility-Surface-Practitioners-Finance/dp/0471792519/ref=sr_1_1?ie=UTF8&amp;qid=1386974691&amp;sr=8-1&amp;keywords=Gatheral+the+volatility">Gatheral book</a> of the Dupire forward PDE is to actually use the Fokker-Planck equation in the probability density integral formula. Out of curiosity, I tried to price direcly with Dupire forward PDE and the Hagan local volatility formula, using just linear boundary conditions. Here are the results on Hagan own example:
<figure><img src="/post/hagan_dens1.png">
</figure>

<figure><img src="/post/hagan_iv.png">
</figure>

The Local Vol direct approach overlaps the Density approach nearly exactly, except at the high strike boundary, when it comes to probability density measure or to implied volatility smile. On Andreasen and Huge data, it gives the following:
<figure><img src="/post/ah_dens1.png">
</figure>

<figure><img src="/post/ah_iv1.png">
</figure>
</p>
<p>One can see that the one step method approximation gives the overall same shape of smile, but shifted, while the PDE, in local vol or density matches the Hagan formula at the money.</p>
<p>Hagan managed to derive a slightly more precise local volatility by going through the probability density route, and his paper formalizes his model in a clearer way: the probability density accumulates at the boundaries. But in practice, this formalism does not seem to matter. The forward Dupire way is more direct and slightly faster. This later way also allows to use alternative boundaries, like Andreasen-Huge did.</p>
<p><strong>Update March 2014</strong> - I have now a paper around this &ldquo;<a href="http://ssrn.com/abstract=2402001">Finite Difference Techniques for Arbitrage Free SABR</a>&rdquo;</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/american-option-on-forwardfutures/">American Option on Forward/Futures</a>
  </h1>
  <time datetime="2013-11-21T11:17:00Z" class="post-date">Thu, Nov 21, 2013</time>
  <p>Prices of a Future contract and a Forward contract are the same under the Black-Scholes assumptions (deterministic rates) but the price of options on Futures or options on Forwards might still differ. I did not find this obvious at first.</p>
<p>For example, when the underlying contract expiration date (Futures, Forward) is different from the option expiration date. For a Future Option, the Black-76 formula can be used, the discounting is done from the option expiry date, because one receives the cash on expiration due to the margin account. For a Forward Option, the discounting need to be done from the Forward contract expiry date.</p>
<p>European options prices will be the same when the underlying contract expiration date is the same as the option expiration date. However, this is not true for American options: the immediate exercise will need to be discounted to the Forward expiration date for a Forward underlying, not for a Future.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/spikes-in-hestonschobel-zhu-local-volatility/">Spikes in Heston/Schobel-Zhu Local Volatility</a>
  </h1>
  <time datetime="2013-11-20T13:33:00Z" class="post-date">Wed, Nov 20, 2013</time>
  <p>Using precise vanilla option pricing engine for Heston or Schobel-Zhu, like the Cos method with enough points and a large enough truncation can still lead to spikes in the Dupire local volatility (using the variance based formula).</p>
<p><figure><img src="/post/Screenshot%20from%202013-11-20%2012%2053%2032.png"><figcaption>
      <h4>Local volatility</h4>
    </figcaption>
</figure>
 <figure><img src="/post/Screenshot%20from%202013-11-20%2012%2054%2039.png"><figcaption>
       <h4>Implied volatility</h4>
     </figcaption>
 </figure>
</p>
<p>The large spikes in the local volatility 3d surface are due to constant extrapolation, but there are spikes even way before the extrapolation takes place at longer maturities. Even if the Cos method is precise, it seems to be not enough, especially for large strikes so that the second derivative over the strike combined with the first derivative over time can strongly oscillate.</p>
<p>After wondering about possible solutions (using a spline on the implied volatilities), the root of the error was much simpler: I used a too small difference to compute the numerical derivatives (1E-6). Moving to 1E-4 was enough to restore a smooth local volatility surface.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/local-stochastic-volatility-with-monte-carlo/">Local Stochastic Volatility with Monte-Carlo</a>
  </h1>
  <time datetime="2013-10-16T16:14:00Z" class="post-date">Wed, Oct 16, 2013</time>
   

I always imagined local stochastic volatility to be complicated, and thought it would be very slow to calibrate.<br /><br />After reading a bit about it, I noticed that the calibration phase could just consist in calibrating independently a Dupire local volatility model and a stochastic volatility model the usual way.<br /><br />One can then choose to compute on the fly the local volatility component (not equal the Dupire one, but including the stochastic adjustment) in the Monte-Carlo simulation to price a product. <br /><br />There are two relatively simple algorithms that are remarkably similar, one by Guyon and Henry-Labord√®re in "<a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1885032">The Smile Calibration Problem Solved</a>":<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s1600/Screenshot+from+2013-10-16+15:51:46.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="157" src="http://4.bp.blogspot.com/-uV792mNz4Xo/Ul6baEs2ktI/AAAAAAAAG1Q/RpvCpcEygXc/s400/Screenshot+from+2013-10-16+15:51:46.png" width="400" /></a></div><br />And one from Van der Stoep, Grzelak &amp; Oosterlee "<a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDIQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fabstract%3D2278122&amp;ei=255eUqaEDMaxhAfdqoBI&amp;usg=AFQjCNF2KqSTT2ouvAyiA2J77foOFTzMKw&amp;sig2=fzb4vlDPp49Hp1oT5Wja4A&amp;bvm=bv.54176721,d.ZG4">The Heston Stochastic-Local Volatility Model: Efficient Monte Carlo Simulation</a>":<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s1600/Screenshot+from+2013-10-16+16:01:01.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="63" src="http://1.bp.blogspot.com/-6-0MMofCHVo/Ul6cWo9JtkI/AAAAAAAAG1Y/xBQnqY_J8tM/s400/Screenshot+from+2013-10-16+16:01:01.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s1600/Screenshot+from+2013-10-16+15:59:08.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="256" src="http://3.bp.blogspot.com/-UkLwvQNeeak/Ul6cWrspGxI/AAAAAAAAG1c/S_X907BJur4/s400/Screenshot+from+2013-10-16+15:59:08.png" width="400" /></a></div><br />In the particle method, the delta function is a regularizing kernel approximating the Dirac function. If we use the notation of the second paper, we have a = psi.<br /><br />The methods are extremely similar, the evaluation of the expectation is slightly different, but even that is not very different. The disadvantage is that all paths are needed at each time step. As a payoff is evaluated over one full path, this is quite memory intensive.<br /><br /><br />



  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/15/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/17/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
