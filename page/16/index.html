<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.80.0" />
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>Chase the Devil &middot; Chase the Devil</title>

  
  <link href="https://fonts.googleapis.com/css?family=UnifrakturMaguntia" rel="stylesheet">  
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/poole-overrides.css">
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/hyde-overrides.css">
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/hyde-x.css">
  <link rel="stylesheet" href="https://chasethedevil.github.io/css/highlight/sunburst.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">
  

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="https://chasethedevil.github.io/touch-icon-144-precomposed.png">
  <link href="https://chasethedevil.github.io/favicon.png" rel="icon">

  
  
  
  <link href="%7balternate%20%7bRSS%20application/rss&#43;xml%20%20index%20alternate%20%20false%20false%20true%20false%20false%200%7d%20/index.xml%20https://chasethedevil.github.io/index.xml%7d" rel="alternate" type="application/rss+xml" title="Chase the Devil &middot; Chase the Devil" />

  <meta name="description" content="">
  <meta name="keywords" content="">
  
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML'></script>
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-365717-1', 'auto');
    ga('send', 'pageview');
  </script>
  
</head>
<body class="theme-base-00">
<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      
      <h1>Chase the Devil</h1>
      <p class="lead">out of tech&hellip;</p>
    </div>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item"><a href="https://chasethedevil.github.io/">Blog</a></li>
      
      <li class="sidebar-nav-item"><a href="https://chasethedevil.github.io/about/">About</a></li>
      
      <li class="sidebar-nav-item"><a href="https://chasethedevil.github.io/post/">Posts</a></li>
      
    </ul>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>  
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>

    

    
  </div>
</div>


<div class="content container">
  <div class="posts">
     
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/">SABR with the new Hagan PDE Approach</a>
      </h1>
      <span class="post-date">May 28, 2013 &middot; 3 minute read &middot; <a href="https://chasethedevil.github.io/post/sabr-with-the-new-hagan-pde-approach/#disqus_thread">Comments</a>
      </span>
      
      

At a presentation of the Thalesians, Hagan has presented a new PDE based approach to compute arbitrage free prices under SABR. This is similar in spirit as Andreasen-Huge, but the PDE is directly on the density, not on the prices, and there is no one-step procedure: it's just like a regular PDE with proper boundary conditions.<br /><br />I was wondering how it compared to Andreasen Huge results.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s1600/snapshot14.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-yg9JIC5LoI0/UaSx8aHdOpI/AAAAAAAAGbo/8LI0NAKhI_A/s640/snapshot14.png" height="304" width="640" /></a></div><br /><br />My first implementation was quite slow. I postulated it was likely the Math.pow function calls. It turns out they could be reduced a great deal. As a result, it's now quite fast. But it would still be much slower than Andreasen Huge. Typically, one might use 40 time steps, while Andreasen Huge is 1, so it could be around a 40 to 1 ratio. In practice it's likely to be less than 10x slower, but still.<br /><br />While looking at the implied volatilities I found something intriguing with Andreasen Huge: the implied volatilities from the refined solution using the corrected forward volatility look further away from the Hagan implied volatilitilies than without adjustment, and it's quite pronounced at the money.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s1600/snapshot15.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-SgFohuLcgdE/UaSzjuSlcAI/AAAAAAAAGb4/AwXdfuDDQ7o/s640/snapshot15.png" height="304" width="640" /></a></div>Interestingly, the authors don't plot that graph in their paper. They  plot a similar graph of their own closed form analytic formula, that is  in reality used to compute the forward volatility. I suppose that  because they calibrate and price through their method, they don't really  care so much that the ATM prices don't match Hagan original formula.<br /><br />We can see something else on that graph: Hagan PDE boundary is not as nice as Andreasen Huge boundary for high strikes (they use a Hagan like approx at the boundaries, this is why it crosses the Hagan implied volatilities there). <br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s1600/snapshot16.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://3.bp.blogspot.com/-xttZ_oNvVCk/UaS0j2wO29I/AAAAAAAAGcI/6oaerlRt0Ps/s640/snapshot16.png" height="304" width="640" /></a></div><br /><br />If we use a simple option gamma = 0 boundary in Andreasen Huge, this results in a very similar shape as the Hagan PDE. This is because the option price is effectively 0 at the boundary.<br />Hagan chose a specifically taylored Crank-Nicolson scheme. I was wondering how it fared when I reduced the number of time-steps. <br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s1600/snapshot17.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-691gk88PTt8/UaS2smPqGnI/AAAAAAAAGcY/_VYXXFTjVVM/s400/snapshot17.png" height="190" width="400" /></a></div><br />The answer is: not good. This is the typical Crank-Nicolson issue. It could be interesting to adapt the method to use Lawson-Morris-Goubet or TR-BDF2, or a simple Euler Richardson extrapolation. This would allow to use less time steps, as in practice, the accuracy is not so bad with 10 time steps only.<br /><br />What I like about the Hagan PDE approach is that the implied vols and the probability density converge well to the standard Hagan formula, when there is no negative density problem, for example for shorter maturities. This is better than Andreasen Huge, where there seems to be always 1 vol point difference. However their method is quite slow compared to the original simple analytic formula.<br /><br /><b>Update March 2014</b> - I have now a paper around this "<a href="http://ssrn.com/abstract=2402001">Finite Difference Techniques for Arbitrage Free SABR</a>"




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/sabr-with-andreasen-huge/">SABR with Andreasen-Huge</a>
      </h1>
      <span class="post-date">May 24, 2013 &middot; 3 minute read &middot; <a href="https://chasethedevil.github.io/post/sabr-with-andreasen-huge/#disqus_thread">Comments</a>
      </span>
      
      

I am on holiday today. Unfortunately I am still thinking about work-related matters, and out of curiosity, wanted to do a little experiment. I know it is not very good to spend free time on work related stuff: there is no reward for it, and there is so much more to life. Hopefully it will be over after this post.<br /><br />Around 2 years ago, I saw a presentation from <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1980726&amp;ei=-VOfUYncL8HB7AbK0YHgBg&amp;usg=AFQjCNHDopVl4pLOYEqepVK8Odhk9Td3iA&amp;sig2=SChIkU-TBR7ECaLdDm1orA&amp;bvm=bv.47008514,d.ZGU">Andreasen and Huge about how they were able to price/calibrate SABR</a> by a one-step finite difference technique. At that time, I did not understand much their idea. My mind was too focused on more classical finite differences techniques and not enough on the big picture in their idea. Their idea is quite general and can be applied to much more than SABR. <br /><br />Recently there has been some talk and development going on where I work about SABR (a popular way to interpolate the option implied volatility surface for interest rate derivatives), especially regarding the implied volatility wings at low strike, and sometimes on how to price in a negative rates environment. There are actually quite a bit of research papers around this. I am not really working on that part so I just mostly listened. Then a former coworker suggested that the Andreasen Huge method was actually what banks seemed to choose in practice. A few weeks later, the Thalesians (a group for people interested in quantitative finance) announced a presentation by Hagan (one of the inventor of SABR) about a technique that sounded very much like Andreasen-Huge&nbsp; to deal with the initial SABR issues in low rates.<br /><br />As the people working on this did not investigate Andreasen-Huge technique, I somehow felt that I had to and that maybe, this time, I would be able to grasp their idea.<br /><br />It took me just a few hours to have meaningful results. Here is the price of out of the money vanilla options using alpha = 0.0758194, nu = 0.1, beta = 0.5, rho = -0.1, forward = 0.02, and a maturity of 2 years.<br /><div class="separator" style="clear: both; text-align: center;"></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s1600/Screenshot+from+2013-05-24+13:29:24.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="212" src="http://3.bp.blogspot.com/-DFehlDerd_U/UZ9ZC6WjBPI/AAAAAAAAGag/P0xM8hHgNt0/s400/Screenshot+from+2013-05-24+13:29:24.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s1600/Screenshot+from+2013-05-24+13:30:09.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="215" src="http://4.bp.blogspot.com/-6gkuKp8sN9o/UZ9ZC2xvT7I/AAAAAAAAGaY/ELh6t9NyOTY/s400/Screenshot+from+2013-05-24+13:30:09.png" width="400" /></a></div>I did not have in my home library a way to find the implied volatility for a given price. I knew of 2 existing methods, <a href="http://www.pjaeckel.webspace.virginmedia.com/ByImplication.pdf">Jaeckel "By Implication"</a>, and <a href="http://scholar.google.fr/citations?view_op=view_citation&amp;hl=fr&amp;user=3GRhH_IAAAAJ&amp;citation_for_view=3GRhH_IAAAAJ:d1gkVwhDpl0C">Li rational functions</a> approach. I discovered that Li wrote <a href="http://www.tandfonline.com/doi/abs/10.1080/14697680902849361">a new paper</a> on the subject where he uses a SOR method to find the implied volatility and claims it's very accurate, very fast and very robust. Furthermore, the same idea can be applied to normal implied volatility. What attracted me to it is the simplicity of the underlying algorithm. Jaeckel's way is a nice way to do Newton-Raphson, but there seems to be so many things to "prepare" to make it work in most cases, that I felt it would be too much work for my experiment. It took me a few more hours to code Li SOR solvers, but it worked amazingly well for my experiment.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s1600/Screenshot+from+2013-05-24+13%253A31%253A33.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="215" src="http://2.bp.blogspot.com/-dsDHXQXC7FQ/UZ9ZDjmJkQI/AAAAAAAAGas/lmclHTb4Fy0/s400/Screenshot+from+2013-05-24+13%253A31%253A33.png" width="400" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s1600/Screenshot+from+2013-05-24+13%253A37%253A51.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="211" src="http://3.bp.blogspot.com/-2_g6KW3a-Ds/UZ9ZDmAFjiI/AAAAAAAAGa0/g9Ktmp9Dsr8/s400/Screenshot+from+2013-05-24+13%253A37%253A51.png" width="400" /></a></div><br />At first I had an error in my boundary condition and had no so good results especially with a long maturity. The traps with Andreasen-Huge technique are very much the same as with classical finite differences: be careful to place the strike on the grid (eventually smooth it), and have good boundaries.<br /><br /><br />




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/">Large Steps in Schobel-Zhu/Heston the Lazy Way</a>
      </h1>
      <span class="post-date">May 17, 2013 &middot; 2 minute read &middot; <a href="https://chasethedevil.github.io/post/large-steps-in-schobel-zhuheston-the-lazy-way/#disqus_thread">Comments</a>
      </span>
      
      

<a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1485403&amp;ei=ODqVUY-8GY2v7AaSsIHgDw&amp;usg=AFQjCNGxk1TqaYu0mxni-OQib6V6lU-M0g&amp;sig2=xzLPCiO5kdF97KN4Tz474A&amp;bvm=bv.46471029,d.ZGU">Van Haastrecht, Lord and Pelsser</a> present an effective way to price derivatives by Monte-Carlo under the Schobel-Zhu model (as well as under the Schobel-Zhu-Hull-White model). It's quite similar to Andersen QE scheme for Heston in spirit.<br /><br />In their paper they evolve the (log) asset process together with the volatility process, using the same discretization times. A while ago, when looking at&nbsp; <a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;bvm=bv.46471029,d.ZGU">Joshi and Chan</a> large steps for Heston, I noticed that, inspired by Broadie-Kaya exact Heston scheme, they present the idea to evolve the variance process using small steps and the asset process using large steps (depending on the payoff) using the integrated variance value computed by small steps. The asset steps correspond to payoff evaluation dates&nbsp; At that time I had applied this idea to Andersen QE scheme and it worked reasonably well.<br /><br />So I tried to apply the same logic to Schobel Zhu, and my first tests show that it works too. Interestingly, the speed gain is about 2x. Here are the results for a vanilla call option of different strikes.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s1600/Screenshot+from+2013-05-17+12:33:07.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="276" src="http://4.bp.blogspot.com/-mQzBUiL9Sz0/UZYJOvoQApI/AAAAAAAAGaA/1GnmgQQOIfs/s400/Screenshot+from+2013-05-17+12:33:07.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Similar Error between long and short asset steps</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s1600/Screenshot+from+2013-05-17+12:32:50.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="276" src="http://1.bp.blogspot.com/-TtzRz_UvaRw/UZYJh9AHgCI/AAAAAAAAGaI/RBYU33FUdOs/s400/Screenshot+from+2013-05-17+12:32:50.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Long steps take around 1/2 the time to compute</td></tr></tbody></table>I would have expected the difference in performance to increase when the step size is decreasing, but it's not the case on my computer.<br /><br />It's not truly large steps like <a href="http://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCoQFjAA&amp;url=http%3A%2F%2Fwww.risk.net%2Fdigital_assets%2F6415%2Fjcf_chan_web.pdf&amp;ei=MzyVUaHGO6qI7AbxwIDICQ&amp;usg=AFQjCNGKOILWMeH-0GMgfF-xv35Zq0XfLw&amp;sig2=iQ5RuV32i-pokPP5lzal-Q&amp;bvm=bv.46471029,d.ZGU">Joshi and Chan</a> do in their integrated double gamma scheme as the variance is still discretized in relatively small steps in my case, but it seems like a good, relatively simple optimization. A while ago, I did also implement the full Joshi and Chan scheme, but it's really interesting if one is always looking for long steps: it is horribly slow when the step size is small, which might occur for many exotic payoffs, while Andersen QE scheme perform almost as well as log-Euler in terms of computational cost.




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/">Exact Forward in Monte-Carlo</a>
      </h1>
      <span class="post-date">May 13, 2013 &middot; 1 minute read &middot; <a href="https://chasethedevil.github.io/post/exact-forward-in-monte-carlo/#disqus_thread">Comments</a>
      </span>
      
      

Where I work, there used to be quite a bit of a confusion on which rates one should use as input to a Local Volatility Monte-Carlo simulation.<br /><br />In particular there is a paper in the Journal of Computation Finance by Andersen and Ratcliffe "The Equity Option Volatility Smile: a Finite Difference Approach" which explains one should use specially tailored rates for the finite difference scheme in order to reproduce exact Bond price and exact Forward contract prices.<br /><br />Code has been updated and roll-backed, people have complained around it. But nobody really made the effort to simply write clearly what's going on, or even write a unit test around it. So it was just FUD, until <a href="http://ssrn.com/abstract=2264327">this paper</a>.<br /><br />In short, for log-Euler, one can use the intuitive forward drift rate: r1*t1-r0*t0 (ratio of discount factors), but for Euler, one need to use a less intuitive forward drift rate to reproduce a nearly exact forward price.




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/">Quasi Monte Carlo in Finance</a>
      </h1>
      <span class="post-date">May 13, 2013 &middot; 1 minute read &middot; <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-in-finance/#disqus_thread">Comments</a>
      </span>
      
      

I have been wondering if there was any better alternative than the standard Sobol (+ Brownian Bridge) quasi random sequence generator for the Monte Carlo simulations of finance derivatives.<br /><br />Here is what I found:<br /><ol><li>Scrambled Sobol. The idea is to rerandomize the quasi random numbers slightly. It can provide better uniformity properties and allows for a real estimate of the standard error. There are many ways to do that. The simple Cranley Patterson rotation consisting in adding a pseudo random number modulo 1, Owen scrambling (permutations of the digits) and simplifications of it to achieve a reasonable speed. This is all very well described in <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=6&amp;cad=rja&amp;ved=0CFwQFjAF&amp;url=http%3A%2F%2Fwww-stat.stanford.edu%2F~owen%2Fcourses%2F362%2Freadings%2Fsiggraph03.pdf&amp;ei=08CQUea-F4jMhAfx5YDgCA&amp;usg=AFQjCNGLnKapkdJ4_caiSE3Ro_kf21NvkQ&amp;sig2=j2b_JqQuO9JNU0ko7yTeOw&amp;bvm=bv.46340616,d.ZG4">Owen Quasi Monte Carlo document</a> </li><li>Lattice rules. It is another form of quasi random sequences, which so far was not very well adapted to finance problems. A <a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CDYQFjAB&amp;url=https%3A%2F%2Fwww.maths.unsw.edu.au%2Fsites%2Fdefault%2Ffiles%2Famr08_9_0.pdf&amp;ei=ysKQUebXO4axO7ungPAN&amp;usg=AFQjCNErqQvM1IyLlUJH2EX5_mVG3f-ZCw&amp;sig2=gYbfQebTwUP4mtj6bteCcQ&amp;bvm=bv.46340616,d.ZWU">presentation from Giles &amp; Kuo</a> look like it's changing.</li><li>Fast PCA. An alternative to Brownian Bridge is the standard PCA. The problem with PCA is the performance in O(n^2). A possible speedup is possible in the case of a equidistant time steps. <a href="http://www.google.com/url?q=http://citeseerx.ist.psu.edu/viewdoc/download%3Fdoi%3D10.1.1.142.5057%26rep%3Drep1%26type%3Dpdf&amp;sa=U&amp;ei=5MWQUaioA8KXhQfDnYCYDQ&amp;ved=0CB0QFjAC&amp;usg=AFQjCNHUhpr6_Ofiqqw2XeU8SY_amnx0pw">This paper</a> shows it can be generalized. But the data in it shows it is only advantageous for more than 1024 steps - not so interesting in Finance.</li></ol>




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/time-estimates-in-software-development/">Time Estimates in Software Development</a>
      </h1>
      <span class="post-date">May 7, 2013 &middot; 1 minute read &middot; <a href="https://chasethedevil.github.io/post/time-estimates-in-software-development/#disqus_thread">Comments</a>
      </span>
      
      

Recently, that I completed a project that I had initially estimated to around 2 months,  in nearly 4 hours. This morning I fixed the few remaining bugs. I looked at the clock, surprised it was still so early and I still had so many hours left in the day.<br /><br />Now I have more time to polish the details and go beyond the initial goal (I think this scares my manager a bit), but I could (and I believe some people do this often) stop now and all the management would be satisfied.<br /><br />What's interesting is that everybody bought the 2 months estimate without questions (I almost even believed it myself). This reminded me of my <a href="http://chasethedevil.github.io/post/productivity-zero/">productivity zero post</a>.<br /><br /><br />




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/">Upper Bounds in American Monte-Carlo</a>
      </h1>
      <span class="post-date">Apr 30, 2013 &middot; 2 minute read &middot; <a href="https://chasethedevil.github.io/post/upper-bounds-in-american-monte-carlo/#disqus_thread">Comments</a>
      </span>
      
      

<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.2367&amp;rep=rep1&amp;type=pdf">Glasserman and Yu</a> (GY) give a relatively simple algorithm to compute lower and upper bounds of a the price of a Bermudan Option through Monte-Carlo.<br /><br />I always thought it was very computer intensive to produce an upper bound, and that the standard <a href="http://escholarship.org/uc/item/43n1k4jb.pdf">Longstaff Schwartz algorithm</a> was quite precise already. GY algorithm is not much slower than the Longstaff-Schwartz algorithm, but what's a bit tricky is the choice of basis functions: they have to be Martingales. This is the detail I overlooked at first and I, then, could not understand why my results were so bad. I looked for a coding mistake for several hours before I figured out that my basis functions were not Martingales. Still it is possible to find good Martingales for the simple Bermudan Put option case and GY actually propose some <a href="http://arxiv.org/pdf/math.PR/0503556">in another paper</a>.<br /><br />Here are some preliminary results where I compare the random number generator influence and the different methods. I include results for GY using In-the-money paths only for the regression (-In suffix) or all (no suffix). <br /><br /><div class="separator" style="clear: both; text-align: center;"></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-c7Jhn_s05Ak/UYACCpfoRjI/AAAAAAAAGXQ/LoQscl4b4yM/s1600/gy_train16k_sim_value.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="238" src="http://2.bp.blogspot.com/-c7Jhn_s05Ak/UYACCpfoRjI/AAAAAAAAGXQ/LoQscl4b4yM/s400/gy_train16k_sim_value.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">value using 16k training path and Sobol - GY-Low-In is very close to LS.</td><td class="tr-caption" style="text-align: center;"><br /></td></tr></tbody></table><div class="separator" style="clear: both; text-align: center;"></div><div class="separator" style="clear: both; text-align: center;"></div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-Jx0NPC1QvAs/UYACisXbFiI/AAAAAAAAGXY/mvsHmNhENcE/s1600/gy_tr16k_sim.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="300" src="http://4.bp.blogspot.com/-Jx0NPC1QvAs/UYACisXbFiI/AAAAAAAAGXY/mvsHmNhENcE/s640/gy_tr16k_sim.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">error using 16k training path - a high number of simulations not that useful</td></tr></tbody></table><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-w6Mp3BDNp-w/UYADEhUQx4I/AAAAAAAAGXg/ESRJyLUybNk/s1600/gy_sim1m_training.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="282" src="http://2.bp.blogspot.com/-w6Mp3BDNp-w/UYADEhUQx4I/AAAAAAAAGXg/ESRJyLUybNk/s640/gy_sim1m_training.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">error using 1m simulation paths - GY basis functions require less training than LS</td></tr></tbody></table><br />One can see the the upper bound is not that precise compared to the lower bound estimate, and that using only in the money paths makes a big difference. GY regression is good with only 1k paths, LS requires 10x more.<br /><br />Surprisingly, I noticed that the Brownian bridge variance reduction applied to Sobol was&nbsp; increasing the GY low estimate, so as to make it sometimes slightly higher than Longstaff-Schwartz price.




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/the-wonderful-un/">The Wonderful UN</a>
      </h1>
      <span class="post-date">Apr 24, 2013 &middot; 1 minute read &middot; <a href="https://chasethedevil.github.io/post/the-wonderful-un/#disqus_thread">Comments</a>
      </span>
      
      

Already the name United Nations should be suspicious, but now <a href="http://boingboing.net/2013/04/24/more-evidence-that-haitis-ch.html">they are shown to have spread Cholera</a> to Haiti, as if the country did not have enough suffering. They have a nice building in New-York, and used to have a popular representative, but unfortunately, for poor countries, they never really achieved much. In Haiti, there were many stories of rapes and corruption by U.N. members more than 10 years ago. The movie<i> <a href="http://www.imdb.com/title/tt0896872/">The Whistleblower</a> </i>suggests it was the same in the Balkans. I am sure it did not change much since.




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/moved-from-dropbox-to-ubuntu-one/">Moved From Dropbox to Ubuntu One</a>
      </h1>
      <span class="post-date">Apr 23, 2013 &middot; 1 minute read &middot; <a href="https://chasethedevil.github.io/post/moved-from-dropbox-to-ubuntu-one/#disqus_thread">Comments</a>
      </span>
      
      

Dropbox worked well, but the company decided to blacklist it. I suppose some people abused it. While looking for an alternative, I found <a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDMQFjAA&amp;url=https%3A%2F%2Fone.ubuntu.com%2F&amp;ei=QtZ2UY3NO83GPePRgOgF&amp;usg=AFQjCNFS8gIGpHCkBTGRPbT7qLVFzb584g&amp;sig2=z3LX3TTwx8lLoS41AJCgaA&amp;bvm=bv.45580626,d.ZWU">Ubuntu One</a>. It's funny I never tried it before even though I use Ubuntu. I did not think it was a dropbox replacement, but it is. And you get 5GB instead of Dropbox 2GB limit, which is enough for me (I was a bit above the 2GB limit). It works well under Linux but as well on Android and there is an iOS app I have not yet tried. It also works on Windows and Mac.<br /><br /><br />




      
    </div>
    
    <div class="post">
      <h1 class="post-title">
        <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-longstaff-schwartz-american-option-price/">Quasi Monte-Carlo &amp; Longstaff-Schwartz American Option price</a>
      </h1>
      <span class="post-date">Apr 22, 2013 &middot; 3 minute read &middot; <a href="https://chasethedevil.github.io/post/quasi-monte-carlo-longstaff-schwartz-american-option-price/#disqus_thread">Comments</a>
      </span>
      
      

In the book <i><a href="http://books.google.fr/books?id=e9GWUsQkPNMC&amp;lpg=PA461&amp;vq=longstaff&amp;hl=fr&amp;pg=PA459#v=snippet&amp;q=longstaff&amp;f=false">Monte Carlo Methods in Financial Engineering</a></i>, Glasserman explains that if one reuses the paths used in the optimization procedure for the parameters of the exercise boundary (in this case the result of the regression in Longstaff-Schwartz method) to compute the Monte-Carlo mean value, we will introduce a bias: the estimate will be biased high because it will include knowledge about future paths.<br /><br />However Longstaff and Schwartz seem to just reuse the paths in <a href="http://rfs.oxfordjournals.org/content/14/1/113.short">their paper</a>, and Glasserman himself, when presenting Longstaff-Schwartz method later in the book just use the same paths for the regression and to compute the Monte-Carlo mean value.<br /><br />How large is this bias? What is the correct methodology?<br /><br />I have tried with Sobol quasi random numbers to evaluate that bias on a simple Bermudan put option of maturity 180 days, exercisable at 30 days, 60 days, 120 days and 180 days using a Black Scholes volatility of 20% and a dividend yield of 6%. As a reference I use <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDcQFjAA&amp;url=http%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D1648878&amp;ei=9151UZ3pM4LZPZ-ugeAF&amp;usg=AFQjCNFS9fdRJt9RoerSnb87YDIZmLcCtw&amp;sig2=k8lHjhUe14ep4giVM5Mr5Q&amp;bvm=bv.45512109,d.ZWU">a finite difference solver based on TR-BDF2</a>.<br /><br />I found it particularly difficult to evaluate it: should we use the same number of paths for the 2 methods or should we use the same number of paths for the monte carlo mean computation only? Should we use the same number of paths for regression and for monte carlo mean computation or should the monte carlo mean computation use much more paths?<br /><br />I have tried those combinations and was able to clearly see the bias only in one case: a large number of paths for the Monte-Carlo mean computation compared to the number of paths used for the regression using a fixed total number of paths of 256*1024+1, and 32*1024+1 paths for the regression.<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">FDM price=2.83858387194312</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">Longstaff discarded paths price=2.8385854695510426&nbsp;</span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">Longstaff reused paths price=2.8386108892756847</span><br /><br />Those numbers are too good to be a real. If one reduces too much the total number of paths or the number of paths for the regression, the result is not precise enough to see the bias. For example, using 4K paths for the regression leads to 2.83770 vs 2.83767. Using 4K paths for regression and only 16K paths in total leads to 2.8383 vs 2.8387. Using 32K paths for regressions and increasing to 1M paths in total leads to 2.838539 vs 2.838546.<br /><br />For this example the Longstaff-Schwartz price is biased low, the slight increase due to path reuse is not very visible and most of the time does not deteriorate the overall accuracy. But as a result of reusing the paths, the Longstaff-Schwartz price might be higher than the real value.




      
    </div>
    
    
<ul class="pagination">
  <li class="page-item">
    <a href="/" class="page-link" aria-label="First"><span aria-hidden="true">&laquo;&laquo;</span></a>
  </li>
  <li class="page-item">
    <a href="/page/15/" class="page-link" aria-label="Previous"><span aria-hidden="true">&laquo;</span></a>
  </li>
  <li class="page-item">
    <a class="page-link" href="/">1</a>
  </li>
  <li class="page-item">
    <a class="page-link" href="/page/2/">2</a>
  </li>
  <li class="page-item">
    <a class="page-link" href="/page/3/">3</a>
  </li>
  <li class="page-item disabled">
    <span aria-hidden="true">&nbsp;&hellip;&nbsp;</span>
  </li>
  <li class="page-item">
    <a class="page-link" href="/page/15/">15</a>
  </li>
  <li class="page-item active">
    <a class="page-link" href="/page/16/">16</a>
  </li>
  <li class="page-item">
    <a class="page-link" href="/page/17/">17</a>
  </li>
  <li class="page-item disabled">
    <span aria-hidden="true">&nbsp;&hellip;&nbsp;</span>
  </li>
  <li class="page-item">
    <a class="page-link" href="/page/39/">39</a>
  </li>
  <li class="page-item">
    <a href="/page/17/" class="page-link" aria-label="Next"><span aria-hidden="true">&raquo;</span></a>
  </li>
  <li class="page-item">
    <a href="/page/39/" class="page-link" aria-label="Last"><span aria-hidden="true">&raquo;&raquo;</span></a>
  </li>
</ul>


  </div>
</div>


<script type="text/javascript">
var disqus_shortname = "chasethedevil";
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>

<div class="content container" style="padding-top: 0rem;"-->
 <a href="https://twitter.com/share" class="twitter-share-button"{count} data-hashtags="chasethedevil" data-size="large">Tweet</a>
 <a style="font-size:75%;" href="//www.reddit.com/submit" onclick="window.location = '//www.reddit.com/submit?url=' + encodeURIComponent(window.location); return false"><i class="fa fa-reddit fa-2x" aria-hidden="true"></i>Submit to reddit</a> 
<table style="border-collapse: collapse;">
     <tr style="padding: 0px; margin: 0px; border: none;">
     <td style="vertical-align: middle;padding: 0px; margin: 0px; border: none;font-size: 60%;">&copy; 2006-16 <a href="http://chasethedevil.github.io/about/">Fabien</a></td>
     <td style="vertical-align: middle;padding: 0px; margin: 0px; border: none;font-size: 0px;"><a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="padding: 0px; margin: 0px; border: none;" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a></td>
     <td style="vertical-align: middle;padding: 0px; margin: 0px; border: none;font-size: 60%;">This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>.</td></tr></table>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
</div>
<script src="https://chasethedevil.github.io/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
</body>
</html>

