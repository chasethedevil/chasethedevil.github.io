<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.147.8">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2025. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/attari-lord-kahl--cos-methods-comparison-on-heston/">Attari, Lord-Kahl &amp; Cos Methods Comparison on Heston</a>
  </h1>
  <time datetime="2013-08-28T17:54:00Z" class="post-date">Wed, Aug 28, 2013</time>
  <p>I recently wrote about the <a href="/post/the-cos-method-for-heston/">Cos method</a>. While rereading the various papers on Heston semi-analytical pricing, especially the <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDAQFjAA&amp;url=http%3A%2F%2Fpfadintegral.com%2Fdocs%2FSchmelzle2010%2520Fourier%2520Pricing.pdf&amp;ei=qREeUpO9HuPE0QXfn4DADw&amp;usg=AFQjCNHANjSlqO6-o5ZfWR8xLpoVT7d5XA&amp;sig2=xeon4_iLfpw8HpKz-0PQQA&amp;bvm=bv.51156542,d.d2k">nice summary by Schmelzle</a>, it struck me how close were the Attari/Bates methods and the Cos method derivations. I then started wondering if Attari was really much worse than the Cos method or not.</p>
<p>I noticed that Attari method accuracy is directly linked to the underlying Gaussian quadrature method accuracy. I found that the doubly adaptive Newton-Cotes quadrature by Espelid (coteda) was the most accurate/fastest on this problem (compared to Gauss-Laguerre/Legendre/Extrapolated Simpson/Lobatto). If the accuracy of the integration is 1e-6, Attari maximum accuracy will also be 1E-6, this means that very out of the money options will be completely mispriced (might even be negative). In a sense it is similar to what I observed on the Cos method.</p>
<p>&ldquo;Lord-Kahl&rdquo; uses 1e-4 integration accuracy, &ldquo;Attari&rdquo; uses 1E-6, and &ldquo;Cos&rdquo; uses 128 points. The reference is computed using <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-ii">Lord-Kahl</a> with Newton-Cotes and 1E-10 integration accuracy.
Well here are the results in terms of accuracy:
<figure><img src="/post/Screenshot%20from%202013-08-28%2017%2020%2029.png">
</figure>
</p>
<p>As expected, Lord-Kahl absolute accuracy is only 1E-5 (a bit better than 1E-4 integration accuracy), while Attari is a bit better than 1E-6, and Cos is nearly 1E-7 (higher inaccuracy in the high strikes, probably because of the truncation inherent in the Cos method).
<figure><img src="/post/Screenshot%20from%202013-08-28%2017%2021%2022.png">
</figure>
</p>
<p>The relative error tells a different story, Lord-Kahl is 1E-4 accurate here, over the full range of strikes. It is the only method to be accurate for very out of the money options: the <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price">optimal alpha</a> allows to go beyond machine epsilon without problems. The Cos method can only go to absolute accuracy of around 5E-10 and will oscillate around, while the reference prices can be as low as 1E-25. Similarly Attari method will oscillate around 5E-8.</p>
<p>What&rsquo;s interesting is how much time it takes to price 1000 options of various strikes and same maturity. In Attari, the charateristic function is cached.</p>
<table>
  <thead>
      <tr>
          <th>Method</th>
          <th>Time</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Cos</td>
          <td>0.012s</td>
      </tr>
      <tr>
          <td>Lord-Kahl</td>
          <td>0.099s</td>
      </tr>
      <tr>
          <td>Attari</td>
          <td>0.086s</td>
      </tr>
      <tr>
          <td>Reference</td>
          <td>0.682s</td>
      </tr>
  </tbody>
</table>
<p>The Cos method is around 7x faster than Attari, for a higher accuracy. Lord-Kahl is almost 8x slower than Cos, which is still quite impressive given that here, the characteristic function is not cached, plus it can price very OTM options while following a more useful relative accuracy measure. When pricing 10 options only, Lord-Kahl becomes faster than Attari, but Cos is still faster by a factor of 3 to 5.</p>
<p>It&rsquo;s also quite impressive that on my small laptop I can price nearly 100K options per second with Heston.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/giving-fedora-another-chance/">Giving Fedora Another Chance</a>
  </h1>
  <time datetime="2013-08-14T22:15:00Z" class="post-date">Wed, Aug 14, 2013</time>
  <p>I have had some stability issues with the Ubuntu 13.04 on my home computer, not on my laptop. It might be related to hard disk encryption (out of curiosity I encrypted my main hard drive in the installer option, resulting in a usable but quite slow system - it&rsquo;s incredible how much the hard drive is still important for performance). I did not have any particular issue on my work laptop with it.</p>
<p>Anyway I gave Fedora 19 a try, with Gnome Shell, even if I am no particular fan of it. So far so good, it seems more stable than my previous Fedora 17 trial, and I get used to Gnome Shell. It&rsquo;s quite different, so it takes a while to get used to, but it is as productive as any other env (maybe more so even).</p>
<p>It made me notice that Ubuntu One cloud storage is much less open that it seems: it&rsquo;s extremely difficult to make it work under Fedora. Some people manage this, I did not. I moved to owncloud, which fits my needs.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/julia-and-the-cumulative-normal-distribution/">Julia and the Cumulative Normal Distribution</a>
  </h1>
  <time datetime="2013-08-13T15:52:00Z" class="post-date">Tue, Aug 13, 2013</time>
  <p>I just stumbled upon <!-- raw HTML omitted -->Julia<!-- raw HTML omitted -->, a new programming language aimed at numerical computation. It&rsquo;s quite new but it looks very interesting, with the promise of C like performance (thanks to LLVM compilation) with a much nicer syntax and parallelization features.<!-- raw HTML omitted --><!-- raw HTML omitted -->Out of curiosity, I looked at their cumulative normal distribution implementation. I found that the (complimentary) error function (directly related to the cumulative normal distribution) algorithm relies on an algorithm that can be found in the Faddeeva library. I had not heard of this algorithm or this library before, but the author, <!-- raw HTML omitted -->Steven G. Johnson<!-- raw HTML omitted -->, claims it is faster and as precise as Cody &amp; SLATEC implementations. As <!-- raw HTML omitted -->I previously had a look at those algorithms<!-- raw HTML omitted --> and was quite impressed by Cody&rsquo;s implementation.<!-- raw HTML omitted --><!-- raw HTML omitted -->The <!-- raw HTML omitted -->source of Faddeeva<!-- raw HTML omitted --> shows a big list (100) of Chebychev expansions for various ranges of a normalized error function. I slightly modified the Faddeva code to compute directly the cumulative normal distribution, avoiding some exp(-x<em>x)<em>exp(x</em>x) calls on the way.<!-- raw HTML omitted --><!-- raw HTML omitted -->Is it as accurate? I compared against a high precision implementation as in my previous test of cumulative normal distribution algorithms. And after replacing the exp(-x</em>x) with <!-- raw HTML omitted -->Cody&rsquo;s trick<!-- raw HTML omitted --> to compute it with higher accuracy, here is how it looks (referenced as &ldquo;Johnson&rdquo;).<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->I also measured performance on various ranges, and found out that this Johnson algorithm is around 2x faster than Cody (in Scala) and 30% faster than my optimization of Cody (using a table of exponentials for Cody&rsquo;s trick).<!-- raw HTML omitted --><!-- raw HTML omitted --></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/the-cos-method-for-heston/">The COS method for Heston</a>
  </h1>
  <time datetime="2013-08-02T14:19:00Z" class="post-date">Fri, Aug 2, 2013</time>
  <p>Fang, in <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC4QFjAA&amp;url=http%3A%2F%2Frepository.tudelft.nl%2Fassets%2Fuuid%3A9aa17357-af21-4c09-86a2-3904ced4b873%2Fthesis.pdf&amp;ei=Epf7Uam8CYWChQeCroCgCw&amp;usg=AFQjCNGyjjlwi-ylN6cl2xoUp5A32wwePA&amp;sig2=m-qvIkWMgVH-qw4hq_Y5Ow&amp;bvm=bv.50165853,d.ZG4%22">her thesis</a>, has the idea of the COS method and applies it to Heston. There are several published papers around it to price options under various models that have a known characteristic function, as well as to price more exotic options like barriers or bermudans.</p>
<p>The COS method is very close to the more standard Heston quasi analytic formula (use transform of characteristic function for the density and integrates the payoff with the density, exchanging summation), except that the more simple <a href="http://en.wikipedia.org/wiki/Fourier_series">Fourier series</a> are used instead of the standard Fourier transform. As a consequence there are a few more approximations that are done related to the truncation of the domain of integration and the result is already discrete, so no need for a Gaussian quadrature.</p>
<p>In practice, the promise is to be faster. I was wondering how stable it was, especially with regards to short maturities/large strikes.</p>
<p>It&rsquo;s quite easy to code, I made only one mistake initially: I forgot to handle the first element of the sum differently. It is however very unstable for call options prices, because the upper integration boundary is then used in an exponential, which explodes in most cases I have tried, while for put options, the lower boundary is used in an exponential, and the lower boundary is negative.</p>
<figure><img src="/post/snapshot33.png"><figcaption>
      <h4>Price is too low at high strikes</h4>
    </figcaption>
</figure>

<p>So one has to rely on the put-call parity formula to compute call prices. This means that we are limited to something around machine epsilon accuracy and can&rsquo;t compute a very out-of-the-money call price, contrary to the <a href="/post/root-finding-in-lord-kahl-method-to-compute-heston-call-price-part-iii">Lord-Kahl</a> method. However it seemed stable for the various Heston parameters I have tried and accurate as long as the resulting price is not too small as the following graph shows.
<figure><img src="/post/snapshot34.png"><figcaption>
      <h4>Price is way too high at low strikes</h4>
    </figcaption>
</figure>
</p>
<p>I was surprised to see that the more in-the-money put options also have inaccuracy: the price given is actually less than the final payoff. This is related to the domain of truncation. If I double it (L=24 instead of L=12), those disappear, what remains is that OTM puts can&rsquo;t go beyond 1e-12 for the COS method.</p>
<p>In practice the COS method was effectively 2x to 3x faster than my Lord-Kahl implementation. As a side note, on this problem, Java is only 2x faster than Octave.</p>
<p>As long as we don&rsquo;t care about very small option prices, it is an interesting alternative, especially because it is simple.</p>
<p><strong>Update April 2014</strong> - There is more information on the subject in my paper at <a href="http://papers.ssrn.com/abstract=2362968">http://papers.ssrn.com/abstract=2362968</a></p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/octave-vs-scilab-for-pdes-in-finance/">Octave vs Scilab for PDEs in Finance</a>
  </h1>
  <time datetime="2013-07-30T12:10:00Z" class="post-date">Tue, Jul 30, 2013</time>
  <p>I was used to <a href="https://www.scilab.org">Scilab</a> for small experiments involving linear algebra. I also like some of Scilab choices in algorithms: for example it provides PCHIM monotonic spline algorithm, and uses Cody for the cumulative normal distribution.</p>
<p>Matlab like software is particularly well suited to express PDE solvers in a relatively concise manner. To illustrate some of my experiments, I started to write a Scilab script for the <a href="/post/sabr-with-the-new-hagan-pde-approach/">Arbitrage Free SABR problem</a>. It worked nicely and is a bit nicer to read than my equivalent Scala program. But I was a bit surprised by the low performance.</p>
<p>Then I heard about <a href="https://www.gnu.org/software/octave/">Octave</a>, which is even closer to Matlab syntax than Scilab and started wondering if it was better or faster. Here are my results for 1000 points and 10 time-steps:</p>
<ul>
<li>Scilab 4.3s</li>
<li>Octave 4.1s</li>
</ul>
<p>I then added the keyword sparse when I build the tridiagonal matrix and end up with:</p>
<ul>
<li>Scilab 0.04s</li>
<li>Octave 0.02s</li>
<li>Scala 0.034s (first run)</li>
<li>Scala 0.004s (once Hotpot has kicked in)</li>
</ul>
<p>So Octave looks a bit better than Scilab in terms of performance. However I could not figure out from the documentation what algorithm was used for the cumulative normal distribution and if there was a monotonic spline interpolation in Octave.</p>
<p>In general I find it impressive that Octave is faster than the first run of Scala or Java, and impressive as well that the Hotspot makes gain of x10.</p>
<p><strong>Update 2024</strong> I now use <a href="https://www.julialang.org">Julia</a>, which offers good performance with concise code.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/joda-localdatetime-vs-datetime/">Joda LocalDateTime vs DateTime</a>
  </h1>
  <time datetime="2013-07-17T14:11:00Z" class="post-date">Wed, Jul 17, 2013</time>
  <p>Joda has the concept of LocalDate <a href="http://joda-time.sourceforge.net/api-release/org/joda/time/LocalDateTime.html">LocalDateTime</a> and <a href="http://joda-time.sourceforge.net/api-release/org/joda/time/DateTime.html">DateTime</a>. The LocalDate is just a simple date, while DateTime is a date and a time zone.</p>
<p>Where I work we have a similar distinction, although not the same: a simple &ldquo;absolute&rdquo; date object without time vs a relative date (a timestamp) like the JDK Date.</p>
<p>The standard JDK Date class is a date without a time zone, but Sun deprecated in JDK 1.1 all methods allowing to use it like a LocalDate, forcing to use it through a Calendar (i.e. like a DateTime), that is, with a TimeZone.</p>
<p>I have found one explanation for a potential use case of LocalDateTime vs DateTime: when you take an appointment to the doctor for July 22nd at 10am, the future date is a fixed event. Some people say you just don&rsquo;t care about the TimeZone in this case, and therefore use LocalDateTime. I think it is a bit more subtle than that. One could think of using fixed arbitrary TimeZone, it could just easily be set to UTC or to the default Java time zone or even the correct one. While it&rsquo;s not typically what the user want to worry about, it could be a default setting (like in Google Calendar or in your OS). And that is exactly what the LocalDateTime does internally, it uses a fixed, non modifiable TimeZone.</p>
<p>If the future event is in a few years and you want to store it in a database, it can become more problematic because daylight saving might not be well determined yet. The number stored today might not mean the same thing in a few years. I am not sure if it can be a real issue, but I am <a href="http://stackoverflow.com/questions/9047501/how-do-you-keep-timezone-and-calendar-up-to-date-after-a-time-zone-change">not the only one</a> to worry about that. As the LocalDateTime internally relies on UTC, it is not affected by this.</p>
<p>There is another more technical use case for LocalDateTime, if you have a list of dates in a contract, they are all according to the contract TimeZone, you then probably don&rsquo;t want to specify the TimeZone for each date. The question is then more is the DateTime concept a good idea?</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/the-cuda-performance-myth-ii/">The CUDA Performance Myth II</a>
  </h1>
  <time datetime="2013-07-12T15:23:00Z" class="post-date">Fri, Jul 12, 2013</time>
  <p>This is a kind of following to the <a href="http://chasethedevil.github.io/post/the-cuda-performance-myth/">CUDA performance myth</a>. There is a recent news on the java concurrent mailing list about <a href="http://gee.cs.oswego.edu/cgi-bin/viewcvs.cgi/jsr166/src/main/java/util/SplittableRandom.java?revision=1.7&amp;view=markup">SplittableRandom class</a> proposed for JDK8. It is a new parallel random number generator a priori usable for Monte-Carlo simulations.</p>
<p>It seems to rely on some very recent algorithm. There are some a bit older ones: the ancestor, L&rsquo;Ecuyer <a href="http://www.iro.umontreal.ca/~simardr/rng/MRG32k3a.c">MRG32k3a</a> that can be parallelized through relatively costless skipTo methods, a Mersenne Twister variant <a href="http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/MTGP/">MTGP</a>, and even the less rigourous XorWow popularized by NVidia CUDA.</p>
<p>The book <a href="http://my.safaribooksonline.com/book/-/9780123849885/chapter-16dot-parallelization-techniques-for-random-number-generators/232#X2ludGVybmFsX0J2ZGVwRmxhc2hSZWFkZXI/eG1saWQ9OTc4MDEyMzg0OTg4NS8yNDQ=">GPU Computing Gems</a> provides some interesting stats as to GPU vs CPU performance for various generators (L&rsquo;Ecuyer, Sobol, and Mersenne Twister)</p>
<figure><img src="/post/snapshot32.png"><figcaption>
      <h4>Excerpt from the book</h4>
    </figcaption>
</figure>

<p>A Quad core Xeon is only 4 times slower to generate normally distributed random numbers with Sobol. Fermi cards are faster now, but probably so are newer Xeons. I would have expected this kind of task to be the typical not too complex parallelizable task doable by a GPU, and yet the improvements are not very good (except if you look at raw random numbers, which is almost useless in applications). It confirms the idea that many real world algorithms are not so much faster with GPUs than with CPUs. I suppose what&rsquo;s interesting is that the GPU abstractions forces you to be relatively efficient, while the CPU flexibility might make you lazy.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/unity-vs-kde-in-virtualbox/">Unity vs KDE in Virtualbox</a>
  </h1>
  <time datetime="2013-07-10T23:17:00Z" class="post-date">Wed, Jul 10, 2013</time>
  <p>The other day I installed the latest Ubuntu 13.04 under a VirtualBox virtual machine using Windows as host. To my surprise, unity failed to launch properly on the virtual machine reboot, with compiz complaining, something I have sometimes seen on my work laptop. It&rsquo;s more surprising in a VM since it is in a way much more standard (no strange graphic card, no strange driver, the same stuff for every VirtualBox user (maybe I&rsquo;m wrong there?)). I therefore installed KDE as a way to bypass this issue. Not only it worked, but the UI was much faster: there was some very noticeable lag in Unity, slow fade in fade out effects, when it worked before the reboot.</p>
<p>I am no hater of Unity, it looks well polished, nice to the eye and I use it on a home computer. I find KDE looks a tiny bit less nice, although I prefer the standard scrollbars of KDE. I wonder if others have the same dreadful experience with Unity under VirtualBox.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/bessel-and-harmonic-kinks-in-the-forward/">Bessel and Harmonic Kinks in the Forward</a>
  </h1>
  <time datetime="2013-07-02T15:44:00Z" class="post-date">Tue, Jul 2, 2013</time>
   

As Bessel (sometimes called Hermite) spline interpolation is only <a href="http://en.wikipedia.org/wiki/Smooth_function#The_space_of_Ck_functions">C1</a>, like the Harmonic spline from Fritsch-Butland, the forward presents small kinks compared to a standard cubic spline. Hyman filtering also creates a kink where it fixes the monotonicity. Those are especially visible with a log scale in time. Here is how it looks on the Hagan-West difficult curve.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-xsT1uMr-S6c/UdLZE3SjlEI/AAAAAAAAGg0/lESF6PVyLkk/s942/snapshot30.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="369" src="http://4.bp.blogspot.com/-xsT1uMr-S6c/UdLZE3SjlEI/AAAAAAAAGg0/lESF6PVyLkk/s640/snapshot30.png" width="640" /></a></div><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/godaddy-sold-my-domain-name/">Godaddy sold my domain name</a>
  </h1>
  <time datetime="2013-06-24T19:11:00Z" class="post-date">Mon, Jun 24, 2013</time>
  <p>I discovered that suddenly emails sent to me bounced back yesterday. I logged in my godaddy account and to my surprise saw that I did not own any domain name anymore. I looked at my emails to see if I had received a warning as is usually the case when your domain is about to expire. There was none recent, the most recent was from may 2011, the last time I had renewed my domain.</p>
<p>I then tried to buy again the same domain name only to discover it was already taken! The whois record indicated a day old registration through godaddy itself.</p>
<p>It&rsquo;s no coincidence that godaddy sells for 3 times the price the possibility to try to take over a domain as soon as it will expire. I find particularly dishonest that in this case they fail to warn their own customers that their domain is about to expire. As a result of this policy, someone else will take over the domain through them for a much higher price. A conflict of interest.</p>
<p>From now on I will not register a domain through a registrar that offers the service to snatch up a domain.</p>

  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/17/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/19/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
