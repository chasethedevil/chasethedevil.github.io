<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.131.0">

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2025. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/more-svi-initial-guesses/">More SVI Initial Guesses</a>
  </h1>
  <time datetime="2014-07-31T14:54:00Z" class="post-date">Thu, Jul 31, 2014</time>
   

In the previous post, I showed one could extract the SVI parameters from a best fit parabola at-the-money. It seemed to work reasonably well, but I found some real market data where it can be much less satisfying.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-NBr8TEcIAXQ/U9o1x6oA6AI/AAAAAAAAHc8/g6-auObo244/s1600/Screenshot+from+2014-07-31+14:24:59.png" height="588" width="640" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"></div>Sometimes (actually not so rarely) the ATM slope and curvatures can't be matched given rho and b found through the asymptotes. As a result if I force to just match the curvature and set m=0 (when the slope can't be matched), the simple ATM parabolic guess looks shifted. It can be much worse than this specific example.<br /><br />It is then a bit clearer why Vogt looked to match the lowest variance instead of ATM. We can actually also fit a parabola at the lowest variance (MV suffix in the graph) instead of ATM. It seems to fit generally better.<br /><br />I also tried to estimate the asymptotic slopes more precisely (using the slope of the 5-points parabola at each end), but it seems to not always be an improvement.<br /><br />However this does not work when rho is close to -1 or 1 as there is then no minimum. Often, matching ATM also does not work when rho is -1 or 1. This specific case, but quite common as well for longer expiries in equities need more thoughts, usually a constant slice is ok, but this is clearly where Zeliade's quasi explicit method shines.<br /><br />So far it still all looks good, but then looking at medium maturities (1 year), sometimes all initial guesses don't look comforting (although Levenberg-Marquardt minimization still works on those - but one can easily imagine that it can break as well, for example by tweaking slightly the rho/b and look at what happens then).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-ft_Sj8P5LuU/U9pLLDlt1nI/AAAAAAAAHdY/vMsuRvqonHs/s1600/Screenshot+from+2014-07-31+15:56:06.png" height="640" width="640" /></a></div><br /><div class="separator" style="clear: both; text-align: center;"></div>There is lots of data on this 1 year example. One can clearly see the problem when the slope can not be fitted ATM (SimpleParabolicATM-guess), and even if by chance when it can (TripleParabolicATM-guess), it's not so great.<br />Similarly fitting the lowest variance leads only to a good fit of the right wing and a bad fit everywhere else.<br /><br />Still, as if by miracle, everything converges to the best fit on this example (again one can find cases where some guesses don't converge to the best fit). I have added some weights +-20% around the money, to ensure that we capture the ATM behavior accurately (otherwise the best fit is funny).<br /><br />It is interesting to see that if one minimizes the min square sum of variances (what I do in Vogt-LM, it's in theory slightly faster as there is no sqrt function cost) it results in an ugly looking steeper curvature, while if we just minimize the min square sum of volatilities (what I do in SimpleParabolicMV_LM), it looks better.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/another-svi-initial-guess/">Another SVI Initial Guess</a>
  </h1>
  <time datetime="2014-07-29T14:39:00Z" class="post-date">Tue, Jul 29, 2014</time>
   

The SVI formula is:<br />$$w(k) = a + b ( \rho (k-m) + \sqrt{(k-m)^2+ \sigma^2}$$<br />where k is the log-moneyness, w(k) the implied variance at a given moneyness and a,b,rho,m,sigma the 5 SVI parameters.<br /><br />A. Vogt described a particularly simple way to find an initial guess to fit SVI to an implied volatility slice <a href="http://www.nuclearphynance.com/User%20Files/66/GatheralSmile_estimation_for_one_expiry_NP.pdf" target="_blank">a while ago</a>. The idea to compute rho and sigma from the left and right asymptotic slopes. a,m are recovered from the crossing point of the asymptotes and sigma using the minimum variance.<br /><br />Later, <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CB4QFjAA&amp;url=http%3A%2F%2Fwww.zeliade.com%2Fwhitepapers%2Fzwp-0005.pdf&amp;ei=xI3XU9T3OYme7AbzwoHQDw&amp;usg=AFQjCNGsvbseObGCDAZ1QbYtvOL9J2-aRw&amp;sig2=rz4SilY2q1RSuD9XgWKFig&amp;bvm=bv.71778758,d.ZGU" target="_blank">Zeliade has shown</a> a very nice reduction of the problem to 2 variables, while the remaining 3 can be deduced explicitly. The practical side is that constraints are automatically included, the less practical side is the choice of minimizer for the two variables (Nelder-Mead) and of initial guess (a few random points).<br /><br />Instead, a simple alternative is the following: given b and rho from the asymptotic slopes, one could also just fit a parabola at-the-money, in a similar spirit as the <a href="http://papers.ssrn.com/abstract=2467231" target="_blank">explicit SABR calibration</a>, and recover explicitly a, m and sigma.<br /><br />To illustrate I take the data from Zeliade, where the input is already some SVI fit to market data.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://1.bp.blogspot.com/-nX_T_AL-OTs/U9eShNryCVI/AAAAAAAAHcE/hmho6OMqGks/s1600/Screenshot+from+2014-07-29+13:38:09.png" height="385" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">3M expiry - Zeliade data</td></tr></tbody></table><br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-PSrSfZ2DLms/U9eTJXJ0WdI/AAAAAAAAHcM/Mvw0c_Reo-c/s1600/Screenshot+-+290714+-+13:36:05.png" height="391" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">4Y expiry, Zeliade data</td></tr></tbody></table>We clearly see that ATM the fit is better for the parabolic initial guess than for Vogt, but as one goes further away from ATM, Vogt guess seems better.<br /><br />Compared to SABR, the parabola itself fits decently only very close to ATM. If one computes the higher order Taylor expansion of SVI around k=0, powers of (k/sigma) appear, while sigma is often relatively small especially for short expiries: the fourth derivative will quickly make a difference.<br /><br />On implied volatilities stemming from a SABR fit of the SP500, here is how the various methods behave:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-FfrG3BodMg4/U9eUXHyHoWI/AAAAAAAAHcY/AILqs_rf5us/s1600/Screenshot+-+290714+-+11:57:48.png" height="383" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">1M expiry on SABR data</td></tr></tbody></table><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-KtBTJ6WvRew/U9eUsgTvEuI/AAAAAAAAHcg/Re-gFxN3Rcg/s1600/Screenshot+-+290714+-+11:51:15.png" height="383" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">4Y expiry on SABR data</td></tr></tbody></table>As expected, because SABR (and thus the input implied vol) is much closer to a parabola, the parabolic initial guess is much better than Vogt. The initial guess of Vogt is particularly bad on long expiries, although it will still converge quite quickly to the true minimum with Levenberg-Marquardt.<br /><br />In practice, I have found the method of Zeliade to be very robust, even if a bit slower than Vogt, while Vogt can sometimes (rarely) be too sensitive to the estimate of the asymptotes.<br /><br />The parabolic guess method could also be applied to always fit exactly ATM vol, slope and curvature, and calibrate rho, b to gives the best overall fit. It might be an idea for the next blog post.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/new-sabr-formulae/">New SABR Formulae</a>
  </h1>
  <time datetime="2014-07-16T22:35:00Z" class="post-date">Wed, Jul 16, 2014</time>
   

In a talk at the Global Derivatives conference of Amsterdam (2014), Pat Hagan presented some new SABR formulas, supposedly close to the arbitrage free PDE behavior.<br /><br />I tried to code those from the slides, but somehow that did not work out well on his example, I just had something very close to the good old SABR formulas. I am not 100% sure (only 99%) that it is due to a mistake in my code. Here is what I was looking to reproduce:<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://4.bp.blogspot.com/-gSpOYAndMkk/U8bfHfczKEI/AAAAAAAAHZ8/-eBE_igGVgw/s1600/Screenshot+from+2014-07-16+22:21:08.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://4.bp.blogspot.com/-gSpOYAndMkk/U8bfHfczKEI/AAAAAAAAHZ8/-eBE_igGVgw/s1600/Screenshot+from+2014-07-16+22:21:08.png" height="328" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Pat Hagan Global Derivatives example</td></tr></tbody></table><br />Fortunately, I then found in some thesis the idea of using Andersen &amp; Brotherton-Ratcliffe local volatility expansion. In deed, the arbitrage free PDE from Hagan is equivalent to some Dupire local volatility forward PDE (see <a href="http://papers.ssrn.com/abstract=2402001">http://papers.ssrn.com/abstract=2402001</a>), so Hagan just gave us the local volatility expansion to expand on&nbsp; (the thesis uses Doust, which is not so different in this case).<br /><br />And then it produces on this global derivatives example the following:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-xH6i6aZ0O0A/U8bhUO5EQlI/AAAAAAAAHaI/oobJM0YaWQs/s1600/Screenshot+-+07162014+-+10:32:04+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-xH6i6aZ0O0A/U8bhUO5EQlI/AAAAAAAAHaI/oobJM0YaWQs/s1600/Screenshot+-+07162014+-+10:32:04+PM.png" height="267" width="400" /></a></div><br />The AB suffix are the new SABR formula. Even though the formulas are different, that looks very much like Hagan's own illustration (with a better scale)!<br /><br />I have a draft paper around this and more practical ideas to calibrate SABR:<br /><a href="http://papers.ssrn.com/abstract=2467231">http://papers.ssrn.com/abstract=2467231</a><br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/heston-or-schobel-zhu-issues-with-short-expiries/">Heston or Schobel-Zhu issues with short expiries</a>
  </h1>
  <time datetime="2014-07-03T23:28:00Z" class="post-date">Thu, Jul 3, 2014</time>
   

It's relatively well known that Heston does not fit the market for short expiries. Given that there are just 5 parameters to fit a full surface, it's almost logical that one part of the surface of it is not going to fit well the market.<br />I was more surprised to see how bad Heston or Schobel-Zhu were to fit a single short expiry volatility slice. As an example I looked at SP500 options with 1 week expiry. It does not really matter if one forces kappa and rho to constant values (even to 0) the behavior is the same and the error in fit does not change much.<br /><br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://3.bp.blogspot.com/-BBnY4WPXQSY/U7XGBnRN4oI/AAAAAAAAHV8/xhG_WFcg2gI/s1600/Screenshot+-+07032014+-+11%253A02%253A15+PM.png" height="400" width="390" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Schobel-Zhu fit for a slice of maturity 1 week</td></tr></tbody></table>In this graph, the brown, green and red smiles corresponds to Schobel-Zhu fit using an explicit guess (matching skew &amp; curvature ATM), using Levenberg-Marquardt on this guess, and using plain differential evolution. <br />What happens is that the smiles flattens to quickly in the strike dimension. One consequence is that the implied volatility can not be computed for extreme strikes: the smile being too low, the price becomes extremely small, under machine epsilon and the numerical method (Cos) fails. There is also a bogus angle in the right wing, because of numerical error. I paid attention to ignore too small prices in the calibration by truncating the initial data.<br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-2ezn28O1sEw/U7XKnXa7OaI/AAAAAAAAHWM/jUXgimXTsHU/s1600/Screenshot+-+07032014+-+11:25:26+PM.png" height="400" width="391" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Heston fit, with Lord-Kahl (exact wings)</td></tr></tbody></table><br />SABR behaves much better (fixing beta=1 in this case) in comparison (As I use the same truncation as for Schobel-Zhu, the flat left wing part is ignored). <br /><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"><tbody><tr><td style="text-align: center;"><a href="http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" src="http://2.bp.blogspot.com/-5f4nhQUoASE/U7XGYmQtQhI/AAAAAAAAHWA/mV2tcliE1oo/s1600/Screenshot+-+07032014+-+11:06:42+PM.png" height="400" width="391" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SABR fit for a slice of maturity 1 week</td></tr></tbody></table>For longer expiries, Heston &amp; Schobel-Zhu, even limited to 3 parameters, actually give a better fit in general than SABR.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/on-the-role-of-static-types-and-generic-types-on-productivity/">On the Role of Static Types and Generic Types on Productivity</a>
  </h1>
  <time datetime="2014-06-29T10:40:00Z" class="post-date">Sun, Jun 29, 2014</time>
   

Most developers have strong opinions on dynamic types programming languages vs static types programming languages. The former is often assumed to be good for small projects/prototyping while the later better for bigger projects. But there is a surprisingly small number of studies to back those claims.<br /><br />One such study is "<a href="http://diyhpl.us/~bryan/papers2/paperbot/7a01e5a892a6d7a9f408df01905f9359.pdf" target="_blank">An experiment about static and dynamic type systems: doubts about the positive impact of static type systems on development time</a>" and came to the conclusion that on a small project, static typing did not decrease programming time, and actually increased debugging time. However 4 years later, "<a href="http://users.dcc.uchile.cl/~rrobbes/p/ICPC2014-idetypes.pdf" target="_blank">An empirical comparison of static and dynamic type systems on API usage in the presence of an IDE: Java vs. groovy with eclipse</a>" shows that a developer is 2x more productive with Java than with Groovy using an unknown API. This contrasts a bit (but does not contradict) with their previous study "<a href="http://swp.dcc.uchile.cl/TR/2012/TR_DCC-20120418-005.pdf" target="_blank">Static Type Systems (Sometimes) have a Positive Impact on the Usability of Undocumented Software: An Empirical Evaluation</a>" that showed Groovy to be more productive on small projects. One problem is that all these studies stem from the same person.<br /><br />It's more interesting to look at generic types vs raw types use, where even less studies have been done. "<a href="http://dl.acm.org/citation.cfm?id=2509528" target="_blank">Do developers benefit from generic types?: an empirical comparison of generic and raw types in java</a>" concludes that generic types do not provide any advantages to fix typing errors, hardly surprising in my opinion. Generic types (especially with type erasure as in Java) is the typical idea that sounds good but that in practice does not really help: it makes the code actually more awkward to read and tend to make developers too lazy to create new classes that would often be more appropriate than a generic type (think Map&lt;String,List&lt;Map&lt;String, Date&gt;&gt;&gt;).



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/moore-penrose-inverse--gauss-newton-sabr-minimization/">Moore-Penrose Inverse &amp; Gauss-Newton SABR Minimization</a>
  </h1>
  <time datetime="2014-06-24T15:29:00Z" class="post-date">Tue, Jun 24, 2014</time>
   

I have found a particularly nice initial guess to calibrate SABR. As it is quite close to the true best fit, it is tempting to use a very simple minimizer to go to the best fit. Levenberg-Marquardt works well on this problem, but can we shave off a few iterations?<br /><br />I firstly considered the basic <a href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization" target="_blank">Newton's method</a>, but for least squares minimization, the Hessian (second derivatives) is needed. It's possible to obtain it, even analytically with SABR, but it's quite annoying to derive it and code it without some automatic differentiation tool. It turns out that as I experimented with the numerical Hessian, I noticed that it actually did not help convergence in our problem. <a href="http://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm" target="_blank">Gauss-Newton</a> converges similarly (likely because the initial guess is good), and what's great about it is that you just need the Jacobian (first derivatives). <a href="https://www.math.lsu.edu/system/files/MunozGroup1%20-%20Paper.pdf" target="_blank">Here</a> is a good overview of Newton, Gauss-Newton and Levenberg-Marquardt methods.<br /><br />While Gauss-Newton worked on many input data, I noticed it failed also on some long maturities equity smiles. The full Newton's method did not fare&nbsp; better. I had to take a close look at the matrices involved to understand what was going on. It turns out that sometimes, mostly when the SABR rho parameter is close to -1, the Jacobian would be nearly rank deficient (a row close to 0), but not exactly rank deficient. So everything would appear to work, but it actually misbehaves badly.<br /><br />My first idea was to solve the reduced problem if a row of the Jacobian is too small, by just removing that row, and keep the previous value for the guess corresponding to that row. And this simplistic approach made the process work on all my input data. Here is the difference in RMSE compared to a highly accurate Levenberg-Marquardt minimization for 10 iterations:<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-A37UNSIHtzQ/U6l53-sbMTI/AAAAAAAAHVA/s5g9safiiaw/s1600/Screenshot+-+06242014+-+10:01:39+AM.png" height="260" width="320" /></a></div><br /><br />Later, while reading some more material related to least square optimization, I noticed the use of the <a href="http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse" target="_blank">Moore-Penrose inverse</a> in cases where a matrix is rank deficient. The Moore-Penrose inverse is defined as:<br />$$ M^\star = V S^\star U^T$$<br />where \( S^\star \) is the diagonal matrix with inverted eigenvalues and 0 if those are deemed numerically close to 0, and \(U, V\) the eigenvectors of the SVD decomposition:<br />$$M=U S V^T$$<br />It turns out to work very well, beside being simpler to code, I expected it to be more or less equivalent to the previous approach (a tiny bit slower but we don't care as we deal with small matrices, and the real slow part is the computation of the objective function and the Hessian, which is why looking at iterations is more important).<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://2.bp.blogspot.com/-pDJ-3L1PARQ/U6l63hQQcvI/AAAAAAAAHVI/8Y9fg-TEcf8/s1600/Screenshot+-+06242014+-+02:50:44+PM.png" height="267" width="320" /></a></div><br />It seems to converge a little bit less quickly, likely due to the threshold criteria that I picked (1E-15).<br />Three iterations is actually most of the time (90%) more than enough to achieve a good accuracy (the absolute RMSE is between 1E-4 and 5E-2) as the following graph shows. The few spikes near 1E-3 represent too large errors, the rest is accurate enough compared to the absolute RMSE.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://1.bp.blogspot.com/-uRD-sBYpw_E/U6l7YOQg-NI/AAAAAAAAHVQ/aGBd1twGu5U/s1600/Screenshot+-+06242014+-+03:20:34+PM.png" height="242" width="320" /></a></div><br />To conclude, we have seen that using the Moore-Penrose inverse in a Gauss-Newton iteration allowed the Gauss-Newton method to work on rank-deficient systems.<br />I am not sure how general that is, in my example, the true minimum either lies inside the region of interest, or on the border, where the system becomes deficient. Of course, this is related to a "physical" constraint, here namely rho &gt; -1.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/one-interview-question-for-job-seekers-in-finance/">One Interview Question for Job Seekers in Finance</a>
  </h1>
  <time datetime="2014-06-19T21:51:00Z" class="post-date">Thu, Jun 19, 2014</time>
  <p>I presented in an <!-- raw HTML omitted -->earlier post<!-- raw HTML omitted --> that I was mostly disillusioned with interview questions, it&rsquo;s better to find out if you can learn something out of a candidate.<!-- raw HTML omitted --><!-- raw HTML omitted -->Well there is maybe one very simple question that could be revealing, for people who pretend to be vaguely familiar with Black-Scholes:<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->What is the price of an at-the-money  binary option under very high volatility? <!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->Alternatively it can be asked with just an at-the-money european option under very high volatility.<!-- raw HTML omitted --><!-- raw HTML omitted -->What makes think of it is that some &ldquo;product manager&rdquo; recently tested risk with volatilities at 300% and was wondering why they did not see any vega (based on a 1% additive shift), and opened bugs, generated noise&hellip;</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/on-the-importance-of-accuracy-for-bpvol-solvers/">On the importance of accuracy for bpvol solvers</a>
  </h1>
  <time datetime="2014-06-12T17:31:00Z" class="post-date">Thu, Jun 12, 2014</time>
  <p>While I was playing around calibrating the arbitrage free SABR model from Hagan (using the PDE on probability density approach), I noticed a misbehavior for some short maturity smiles. I thought it was due to the PDE implementation. Actually some of it was, but the remaining large error was due to the bpvol solver.</p>
<p>I initially took the same approach as Choi et al. in <a href="/post/building-a-more-accurate-basis-point-volatility-formula">my solver</a>, that is to work with in-the-money prices (they work with straddles) because it&rsquo;s nice and convenient. I thought it was no big deal if prices lower than 1E-16 were not solved. It turns out I was wrong. Choi et al. solver has the same issue.
<figure><img src="/post/Screenshot%20-%2006122014%20-%2005%2024%2027%20PM.png">
</figure>
</p>
<p>In the above figure, CKK denotes the Choi et al algorithm (similar with my old algorithm) and Chebyshev is my updated algorithm that is accurate with far out-of-the-money option. What happens is that even though the market price at the lowest strike is not very low, the price at the lowest strike stemming from the best fit smile is extremely low, and when we want to invert it, CKK produces a large error due to lack of representation of numbers near 1.0 as it uses indirectly the in-the-money price. That&rsquo;s where it introduces a particularly big error in this case.</p>
<p>I have updated my solver since, to work with out-of-the-money option prices as well, and have near machine accuracy on the whole range. I also reduced the number of Chebyshev polynomials used in the process. All the details are in my updated paper at <a href="http://papers.ssrn.com/abstract=2420757">http://papers.ssrn.com/abstract=2420757</a>
<figure><img src="/post/Screenshot%20-%2006122014%20-%2005%2024%2013%20PM.png">
</figure>
</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/throws-exception/">throws Exception</a>
  </h1>
  <time datetime="2014-05-27T10:49:00Z" class="post-date">Tue, May 27, 2014</time>
   

There was a big debate at work around Exception declaration in a Java API. I was quite surprised that such an apparently simple subject could end up being so controversial. The controversy was around the choice of declaring in the interfaces:<br /><br /><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">void myMethod() throws Exception</span></span><br /><br />instead of<br /><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() throws MyAPIException</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() throws MyAPIRuntimeException</span></span><br /><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><span style="font-size: x-small;">void myMethod() </span></span><br /><br />where MyAPI represents either a generic API related exception or a specific exception related to the method in question.<br /><br />The choice of "throws Exception" did not even occur to me as a possibility, but after some digging, I found that some relatively famous libraries actually followed that principle at one point, for example Apache Struts 1.x or Spring MVC. <br /><br />More modern libraries, like Google Guava, commons-math 3.x, Struts 2.x generally favor MyAPIRuntimeException where MyAPI is actually context-specific. Some old popular libraries declare a checked Exception, for example the HibernateException in Hibernate.<br /><br />This seems to be a recurring subject on Stackoverflow:<br /><a href="http://stackoverflow.com/questions/20530221/java-interface-throws-exception-best-practice" target="_blank">Stackoverflow - Java interface throws Exception best practice</a><br /><a href="http://stackoverflow.com/questions/4283634/what-to-put-in-the-throws-clause-of-an-interface-method" target="_blank">Stackoverflow - What to put in the throws clause of an interface method</a><br /><br />But those are quite poor in terms of explanations. The best comments on this subjects are from:<br /><a href="http://www.artima.com/intv/handcuffs.html"><span class="ts">Anders         Hejlsberg (C#, Delphi, Turbo Pascal creator) - The Trouble with         Checked Exceptions</span></a><br />    <a href="http://www.artima.com/intv/solid2.html"><span class="ts">James         Gosling (Java creator) - Failure and Exceptions</span></a><br /><br />    <br />This comment from Anders is particularly acute:<br />    "<b>To work around this requirement, people do ridiculous things.       For example, they decorate every method with, "</b><b><code>throws         Exception</code></b><b>." That just completely defeats the       feature, and you just made the programmer write more gobbledy       gunk. That doesn't help anybody.</b>    "<br />    <br /><br />         <br />Today I believe the API in question declares "throws Exception"... <br />         <br /><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/kde-xfce-gnome-shell-in-2014/">KDE, XFCE, Gnome-Shell in 2014</a>
  </h1>
  <time datetime="2014-05-25T09:26:00Z" class="post-date">Sun, May 25, 2014</time>
   

Many people (and notoriously, Linus Torvald) complained about Gnome-shell, especially the early iterations. Similarly KDE 4 was a nightmare of instability and inflexibility when it came out. And XFCE has always sounded a bit too basic. the moves of Gnome and KDE were particularly shocking as the earlier iteration: Gnome 2 and KDE 3 were well appreciated, productive environments.<br /><br /><span style="font-size: large;">Gnome Shell 3.10</span><br /><br />It took me a bit of time to get used to it, and in the early stages I went to KDE 4 for a while, only to come back to it later.<br /><br /><ul><li><i>Positive aspects:</i> lots of space on the desktop, things don't get in the way, looks good,very good desktop overview (fast and well presented), a dock by default, great external monitor support (plug and play, remembers settings automatically), best OSD (volume) of all.</li><li><i>Negative aspects:</i> the notifications bar looks awkward and badly integrated (better with an extension), still unstable and big memory leaks (on Fedora 20, where the integration should be the best, it regularly crashes, starts with 300Mb and goes up to 1Gb in a couple of days), fallback-session completely useless as one can not customize it at all. But the killer for my work was&nbsp; inability to share the desktop with Webex, while XFCE could.</li></ul><br /><span style="font-size: large;">KDE</span> <br /><br />I gave it a long try especially in 2012, it has not changed much in 2014. My opinion of it fell when I tried it a very short time after months of Gnome Shell, and even more so after seeing the trouble my parents had with it, compared to Gnome 2.<br /><br /><ul><li><i>Positive aspects:</i> desktop search (needs to be configured in order to scan only the relevant folders, used to be slow and resource intensive, not so much in 2014)<i>&nbsp;</i></li><li><i>Negative aspects:</i> resource hog, awful start menu, too many shiny effects by default that only distract the user from his task, silly concepts like activities, every aspect of the desktop seems to require tweaking in non obvious ways for it to be more usable, looks ok but not great.</li></ul><br /><span style="font-size: large;">XFCE</span><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://4.bp.blogspot.com/-nw0i5RiPfz0/U4RIv9aY2pI/AAAAAAAAHTQ/xF9ZwyRtXZk/s1600/Screenshot+-+05272014+-+10:06:57+AM.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" src="http://4.bp.blogspot.com/-nw0i5RiPfz0/U4RIv9aY2pI/AAAAAAAAHTQ/xF9ZwyRtXZk/s1600/Screenshot+-+05272014+-+10:06:57+AM.png" height="223" width="400" /></a></div><br />On Fedora, the default XFCE is very very basic, so much that I could hardly see a difference with one from 10 years ago. On Xubuntu, it's much much better. When I came to it from Gnome-Shell, I was surprised at how good was the "old" desktop paradigm for productivity. I also surprisingly found multiple desktops more natural to use than on Gnome Shell/KDE.<br />On Fedora the way to make it like Xubuntu is to install elementary icons, the whisker menu and choose the greybird/bluebird themes.<br /><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;"><br /></span></span><span style="font-size: x-small;"><span style="font-family: &quot;Courier New&quot;,Courier,monospace;">yum groups install "Xfce Desktop"<br />yum install xfce4-mixer.x86_64 xfce4-whiskermenu-plugin.x86_64 xfce4-cpugraph-plugin.x86_64 xfce4-mount-plugin.x86_64 xfce4-icon-theme.noarch google-droid* elementary-xfce-icon-theme.noarch xfce4-volumed.x86_64 pavucontrol.x86_64</span></span><br /><br /><ul><li><i>Positive aspects:</i> fast and lean, great start menu. </li><li><i>Negative aspects:</i> external monitor support could be more automatic like Gnome-Shell, no nice overview of all windows, default installation can be a bit too bare, sometimes not sexy (volume applet is ugly, xubuntu provides the unity indicators in xfce as a remedy), primitive OSD.</li></ul><span style="font-size: large;"><br /></span><span style="font-size: large;">Cinnamon, Unity, Conclusion</span><br /><br />I gave a short try to cinnamon as well, in hopes that it was more stable than gnome shell. In short, it was not. It's certainly less of a memory hog, but I had some strange behavior with an additional phantom panel sometimes appearing at the bottom at the screen. And overall it looks a lot less polished.<br /><br />Unity is more interesting, but it's too Ubuntu centric, I don't like the start button equivalent (slow, badly presented, don't care about HUD), the windows overview is not as useful as Gnome shell, the dock, something I usually like, is strangely annoying.<br /><br />This is a very subjective review, my feeling is that in 2014, people should not waste their time with KDE or Cinnamon. Gnome shell could be worth a try if you don't care so much about memory leaks and slight instability but value a distraction free desktop. Otherwise go for XFCE or Unity on (X)ubuntu.



  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/12/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/14/">Next</a></p>
    </main>

    
      
  


    
  </body>
</html>
