<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
  <link href="https://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.93.3" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>Chase the Devil</title>
  <meta name="description" content="A personal, independent, technical blog" />

  
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/print.css" media="print">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/poole.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/syntax.css">
  <link type="text/css" rel="stylesheet" href="https://chasethedevil.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">
<link href="https://fonts.googleapis.com/css2?family=UnifrakturMaguntia&display=swap" rel="stylesheet">
 <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css">


  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="https://chasethedevil.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Chase the Devil" />
  <script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

</head>

  <body class=" ">
  <aside class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://chasethedevil.github.io/"><h1 style="font-family: 'UnifrakturMaguntia', cursive;font-weight: normal;">Chase the Devil</h1></a>
      <p class="lead">
       A personal, independent, technical blog 
      </p>
    </div>

    <nav>
      <ul class="sidebar-nav">
        <li><a href="https://chasethedevil.github.io/">Blog</a> </li>
        <li><a href="/about/"> About </a></li><li><a href="/post/"> Posts </a></li><li><a href="/tags/"> Tags </a></li>
      </ul>

        <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
      <script type="text/javascript">document.write("<a href=\"mail" + "to:" + new Array("fabien","2ipi.com").join("@") + "?subject=your%20blog\">" + '<i class="fa fa-envelope fa-3x"></i>' + "</" + "a>");</script>
      
      
      
      
      
      
      <a href="https://twitter.com/logos01"><i class="fa fa-twitter-square fa-3x"></i></a>
      
      <a href="https://chasethedevil.github.io/index.xml" type="application/rss+xml"><i class="fa fa-rss-square fa-3x"></i></a>
      </li>
    </ul>
 </nav>

    <p>&copy; 2022. All rights reserved. </p>
  </div>
</aside>

    <main class="content container">
    <div class="posts">
<article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/parallel-can-be-slower/">Parallel Can Be Slower</a>
  </h1>
  <time datetime="2013-02-13T17:05:00Z" class="post-date">Wed, Feb 13, 2013</time>
  <p>I found a nice finite difference scheme, where the solving part can be parallelized on 2 processors at each time-step</p>
<p>I was a bit surprised to notice that the parallelized algorithm ran in some cases twice slower than the same algorithm not parallelized. I tried ForkJoinPool, ThreadPoolExecutor, my one notify/wait based parallelization. All resulted in similar performance compared to just calling thread1.run() and thread2.run() directly.</p>
<p>I am still a bit puzzled by the results. Increasing the time of the task by increasing the number of discretization points does not really improve the parallelization. The task is relatively fast to perform and is repeated many (around of 1000) times, so synchronized around 1000 times, which is likely why parallelization is not great on it: synchronization overhead reaps any benefit of the parallelization. But I expected better. Using a Thread pool of 1 thread is also much slower than calling run() twice (and fortunately slower than the pool of 2 threads).</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/scala-is-mad-part-2/">Scala is Mad (part 2)</a>
  </h1>
  <time datetime="2013-02-13T16:20:00Z" class="post-date">Wed, Feb 13, 2013</time>
  <p>I still did not abandon Scala despite my <!-- raw HTML omitted -->previous post<!-- raw HTML omitted -->, mainly because I have already quite a bit of code, and am too lazy to port it. Furthermore the issues I detailed were not serious enough to motivate a switch. But these days I am more and more fed up with Scala, especially because of the Eclipse plugin. I tried the newer, the beta, and the older, the stable, the conclusion is the same. It&rsquo;s welcome but:<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->code completion is not great compared to Java. For example one does not seem to be able to see the constructor parameters, or the method parameters can not be automatically populated.<!-- raw HTML omitted --><!-- raw HTML omitted -->the plugin makes Eclipse <em>very</em> slow. Everything seems at least 3-5x slower. On the fly compilation is also much much slower than Java&rsquo;s.<!-- raw HTML omitted --><!-- raw HTML omitted --><!-- raw HTML omitted -->It&rsquo;s nice to type less, but if overall writing is slower because of the above issues, it does not help. Beside curiosity of a new language features, I don&rsquo;t see any point in Scala today, even if some of the ideas are interesting. I am sure it will be forgotten/abandoned in a couple of years. Today, if I would try a new language, I would give Google Go a try: I don&rsquo;t think another big language can make it/be useful on the JVM (beside a scripting kind of language, like JavaScript or Jython).<!-- raw HTML omitted --><!-- raw HTML omitted -->Google Go focuses on the right problem: concurrency. It also is not constrained to JVM limitation (on the other side one can not use a Java library - but open source stuff is usually not too difficult to port from one language to another). It has one of the fastest compilers. It makes interesting practical choices: no inheritance.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/from-opensuse-to-ubuntu-13.04/">From OpenSuse to Ubuntu 13.04</a>
  </h1>
  <time datetime="2013-02-01T18:22:00Z" class="post-date">Fri, Feb 1, 2013</time>
  <p>In my Linux quest, I changed distribution again on my home desktop, from OpenSuse 11.1 with KDE to Ubuntu 13.04 (not yet released - so alpha) with Unity. Why?</p>
<ul>
<li>KDE was crashing a bit too often for my taste (more than once a week). Although I enjoyed the KDE environment.</li>
<li>Not easy to transfer files to my Android 4.2 phone. Ubuntu 13.04 is not fully there yet, but is on its way.</li>
<li>zypper is a bit too specific for my taste. I would be ok with yum+rpm or apt-get, but another tool just for a single distribution, not really.</li>
<li>Plus I&rsquo;m just curious what&rsquo;s next for Ubuntu, and Linux makes it very simple to change distributions, and reinstall applications with the same settings. So it&rsquo;s never a big task to change distribution.</li>
<li>I somehow like how Ubuntu feels, not sure what it is exactly, maybe the Debian roots.</li>
</ul>
<p>When people say OpenSuse is rock solid, I don&rsquo;t have that impression, at least on the desktop. It might have been true in the past. But in the past, most distros were very stable. I never remember having big stability issues until, maybe, late 2010. In the early 2000s, a laptop would work very well with Linux, suspend included. I remember that my Dell Inspiron 8200 was working perfectly with Mandrake and WindowMaker. Nowadays, it never seem to work that well, but is just ok: Optimus comes to mind (especially with external screen), suspend problems, wifi (not anymore).</p>
<p>So far I can see that Ubuntu 13.04 is prettier than the past versions, the installer is great. I encrypted my two hard disks, it was just a matter of ticking a box - very nice. Unity Launcher, while interesting, is still not the greatest tool to find an installed application (compared to KDE launcher or Gnome Shell). I don&rsquo;t notice any stability issue so far, even though I have some popup messages that sometimes tells me something crashed (typical for an alpha version). If I just ignore the messages, everything seems fine. OpenSuse-KDE was logging me out (session crash), or just stopped completely being responsive (hard reset necessary).</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/productivity-zero/">Productivity Zero</a>
  </h1>
  <time datetime="2013-01-24T22:03:00Z" class="post-date">Thu, Jan 24, 2013</time>
  <p>Sometimes it feels like big companies try to enforce the Productivity Zero rule.</p>
<p>Here is a guide:</p>
<ul>
<li>involve as many team as possible. It will help ensuring endless discussions about who is doing what, and then how do I interface with them. This is most (in)efficient when team managers interact and are not very technically competent. One consequence is that nobody is fully responsible/accountable, which helps reinforce the productivity zero.</li>
<li>meetings, meetings and meetings. FUD (Fear Uncertainty and Doubt) is king here. By spreading FUD, there will be more and more meetings. Even if the &ldquo;project&rdquo; is actually not a real project, but amounts to 10 lines of code, it is possible to have many meetings around it, over the span of several months (because everybody is always busy with other tasks). Another strategy is to use vocabulary, talk about technical or functional parts the others don&rsquo;t understand. Some people are masters are talking technical to functional people and vice versa.</li>
<li>multiply by 20, not 2. It is surprisingly easy to tell upper management something is going to take 3 months, when, in fact, it can be done in 3 days. This is a bit like bargaining in South East Asia: it&rsquo;s always amazing to find out how much you can push the price down (or how much they push it up).</li>
<li>hire as many well paid managers and product specialists as you can, and make sure they know nothing about the product or the functional parts but are very good at playing the political game without any content. Those people often manage to stay a long time, a real talent.</li>
</ul>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/better-finite-difference-boundaries-with-a-tridiagonal-solver/">Better Finite Difference Boundaries with a Tridiagonal Solver</a>
  </h1>
  <time datetime="2013-01-10T19:19:00Z" class="post-date">Thu, Jan 10, 2013</time>
  <p>In <!-- raw HTML omitted --><!-- raw HTML omitted -->Pricing Financial Instruments - The Finite Difference Method<!-- raw HTML omitted --><!-- raw HTML omitted -->, Tavella and Randall explain that boundary conditions using a higher order discretization (for example their &ldquo;BC2&rdquo; boundary condition) can not be solved in one pass with a simple tridiagonal solver, and suggest the use of SOR or some conjugate gradient based solver.<!-- raw HTML omitted --><!-- raw HTML omitted -->It is actually very simple to reduce the system to a tridiagonal system. The more advanced boundary conditions only use 3 adjacent values, just 1 value makes it non tridiagonal, the one in <!-- raw HTML omitted -->bold<!-- raw HTML omitted --> is the following matrix representation<!-- raw HTML omitted -->x x <!-- raw HTML omitted -->x<!-- raw HTML omitted --><!-- raw HTML omitted -->x x x<!-- raw HTML omitted -->   x x x <!-- raw HTML omitted -->       &hellip;&hellip;<!-- raw HTML omitted -->         x x x<!-- raw HTML omitted -->            x x x<!-- raw HTML omitted -->            <!-- raw HTML omitted -->x<!-- raw HTML omitted --> x x<!-- raw HTML omitted -->One just needs to replace the first line by a simple linear combination of the first 2 lines to remove the extra <!-- raw HTML omitted -->x<!-- raw HTML omitted --> and similarly for the last 2 lines. This amounts to ver little computational work. Then one can use a standard tridiagonal solver. This is how I implemented it in a past post about <!-- raw HTML omitted -->boundary conditions of a bond in the CIR model<!-- raw HTML omitted -->. It is very surprising that they did not propose that simple solution in an otherwise very good book.</p>

  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/non-central-chi-squared-distribution-in-java-or-scala/">Non Central Chi Squared Distribution in Java or Scala</a>
  </h1>
  <time datetime="2013-01-03T17:21:00Z" class="post-date">Thu, Jan 3, 2013</time>
   

I was looking for an implementation of the non central chi squared distribution function in Java, in order to price bond options under the Cox Ingersoll Ross (CIR) model and compare to a finite difference implementation. It turned out it was not so easy to find existing code for that in a library. I would have imagined that Apache common maths would do this but it does not.<br /><br />OpenGamma has a not too bad <a href="http://docs-static.opengamma.com/1.2.0/java/javadocs/com/opengamma/analytics/math/statistics/distribution/NonCentralChiSquaredDistribution.html">implementation</a>. It relies on Apache commons maths for the Gamma function implementation. I looked at what was there in C/C++. There is some old fortran based ports with plenty of goto statements. There is also a nice <a href="http://www.boost.org/doc/libs/1_36_0/libs/math/doc/sf_and_dist/html/math_toolkit/dist/dist_ref/dists/nc_chi_squared_dist.html">implementation in the Boost library</a>. It turns out it is quite easy to port it to Java. One still needs a Gamma function implementation, I looked at Boost implementation of it and it turns out to be very similar to the Apache commons maths one (which is surprisingly not too object oriented and therefore quite fast - maybe they ported it from Boost or from a common source).<br /><br />The Boost implementation seems much more robust in general thanks to:<br /><ul><li>The use of complimentary distribution function when the value is over 0.5. One drawback is that there is only one implementation of this, the Benton and Krishnamoorthy one, which is a bit slower than Ding's method.</li><li>Reverts to <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CDEQFjAA&amp;url=http%3A%2F%2Fwww.ucs.louisiana.edu%2F~kxk4695%2FCSDA-03.pdf&amp;ei=LKzlUKWvL_K00QX_t4CYBA&amp;usg=AFQjCNFgkeariqMPXVH4LLrO5RRi3nIc0Q">Benton and Krishnamoorthy</a> method in high non-centrality cases. Benton and Krishnamoorthy is always very accurate, while Fraser (used by OpenGamma) is <a href="http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CEIQFjAC&amp;url=http%3A%2F%2Fdspace.uevora.pt%2Frdpc%2Fbitstream%2F10174%2F4635%2F1%2FSpeedAccuracyCEV_FFM%255B1%255D.pdf&amp;ei=Ea3lULLsBaPL0QXKuoHYAw&amp;usg=AFQjCNGkqkA5e_-0n_DATE9BTQVoL9NTkg&amp;cad=rja">not very precise in general</a>.</li></ul>It is interesting to note that both implementations of Ding method are wildly different, Boost implementation has better performance and is simpler (I measured that my Java port is around 50% faster than OpenGamma implementation).<br /><br />If only it was simple to commit to open-source projects while working for a software company...



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/finite-difference-approximation-of-derivatives/">Finite Difference Approximation of Derivatives</a>
  </h1>
  <time datetime="2012-12-21T12:12:00Z" class="post-date">Fri, Dec 21, 2012</time>
   

A while ago, someone asked me to reference him in a paper of mine because I used formulas of a finite difference approximation of a derivative on a non uniform grid. I was shocked as those formula are very widespread (in countless papers, courses and books) and not far off elementary mathematics.

There are however some interesting old papers on the technique. Usually people approximate the first derivative by the central approximation of second order:

$$ f'(x) = \frac{f(x\_{i+1})-f(x\_{i-1})}{x\_{i+1} - x\_{i-1}} $$  

However there are some other possibilities. For example one can find a formula directly out of the Taylor expansions of \\(f(x(i+1)) and f(x(i-1))\\). <a href="https://www.google.fr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CC0QFjAA&amp;url=http%3A%2F%2Fwww.nada.kth.se%2Fkurser%2Fkth%2F2D1263%2Fl6.pdf&amp;ei=q07QUNH8GuW80QW804GIAw&amp;usg=AFQjCNGunxdXHqGsHh0czcX7e4gCnAU1WQ&amp;bvm=bv.1355534169,d.d2k">This paper</a> and <a href="http://onlinelibrary.wiley.com/doi/10.1111/j.2153-3490.1970.tb01933.x/abstract">that one</a> seems to indicate it is more precise, especially when the grid does not vary smoothly (a typical example is uniform by parts).<br /><br />This can make a big difference in practice, here is the example of a Bond priced under the Cox-Ingersoll-Ross model by finite differences. EULER is the classic central approximation, EULER1 uses the more refined approximation based on Taylor expansion, EULER2 uses Taylor expansion approximation as well as a higher order boundary condition. I used the same parameters as in the Tavella-Randall book example and a uniform grid between [0,0.2] except that I have added 2 points at the far end at 0.5 and 1.0. So the only difference between EULER and EULER1 lies in the computation of derivatives at the 3 last points.<br /><div class="separator" style="clear: both; text-align: center;"><a href="http://2.bp.blogspot.com/-mUrKdRztE-c/UNSPUoRdPdI/AAAAAAAAGLM/SEViiYfAhRs/s1600/cir_bond_euler_discretizations.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="288" src="http://2.bp.blogspot.com/-mUrKdRztE-c/UNSPUoRdPdI/AAAAAAAAGLM/SEViiYfAhRs/s400/cir_bond_euler_discretizations.png" width="400" /></a></div><div class="separator" style="clear: both; text-align: left;">I also computed the backward 2nd order first derivative on a non uniform grid (for the refined boundary). I was surprised not to find this easily on the web, so here it is:</div>
$$ f'(x\_i) = \left(\frac{1}{h\_i}+\frac{1}{h\_i+h\_{i-1}}\right) f(x\_i)- \left(\frac{1}{h\_{i-1}}+\frac{1}{h\_i}\right) f(x\_{i-1})+ \left(\frac{1}{h\_{i-1}} - \frac{1}{h\_i+h\_{i-1}} \right) f(x\_{i-2}) + ...$$ 

<div class="separator" style="clear: both; text-align: left;">Incidently while writing this post I found out it was a pain to write Math in HTML (I initially used a picture). MathML seems a bit crazy, I wonder why they couldn't just use the LaTeX standard. Update January 3rd 2013 - I now use <a href="http://mathjax.org">Mathjax</a>. It's not very good solution as I think this should typically be handled by the browser directly instead of huge javascript library, but it looks a bit better</div><br />



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/a-discontinuity/">A Discontinuity</a>
  </h1>
  <time datetime="2012-12-12T20:59:00Z" class="post-date">Wed, Dec 12, 2012</time>
   

I am comparing various finite difference schemes on simple problems and am currently stumbling upon a strange discontinuity at the boundary for some of the schemes (Crank-Nicolson, Rannacher, and TR-BDF2) when I plot an American Put Option Gamma using a log grid. It actually is more pronounced with some values of the strike, not all. The amplitude oscillates with the strike. And it does not happen on a European Put, so it's not a boundary approximation error in the code. It might well be due to the nature of the scheme as schemes based on implicit Euler work (maybe monotonicity preservation is important). This appears on this graph around S=350.<br /><br /><div class="separator" style="clear: both; text-align: center;"><a href="http://1.bp.blogspot.com/-ugmThxi_c_g/UMjh4M8JKAI/AAAAAAAAGKg/Oj-p9_AaNzM/s1600/gamma_american_boundary.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="160" src="http://1.bp.blogspot.com/-ugmThxi_c_g/UMjh4M8JKAI/AAAAAAAAGKg/Oj-p9_AaNzM/s640/gamma_american_boundary.png" width="640" /></a></div><br /><b>Update December 13, 2012</b>: after a close look at what was happening. It was after all a boundary issue. It's more visible on the American because the Gamma is more spread out. But I reproduced it on a European as well.



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/scala-is-mad/">Scala is Mad</a>
  </h1>
  <time datetime="2012-12-12T16:07:00Z" class="post-date">Wed, Dec 12, 2012</time>
   

I spent quick a bit of time to figure out why something that is usually simple to do in Java did not work in Scala: Arrays and ArrayLists with generics.<br /><br />For some technical reason (type erasure at the JVM level), Array sometimes need a parameter with a ClassManifest !?! a generic type like [T :&lt; Point : ClassManifest] need to be declared instead of simply [T :&lt; Point].<br /><br />And then the quickSort method somehow does not work if invoked on a generic... like quickSort(points) where points: Array[T]. I could not figure out yet how to do this one, I just casted to points.asInstanceOf[Array[Point]], quite ugly.<br /><br />In contrast I did not even have to think much to write the Java equivalent. Generics in Scala, while having a nice syntax, are just crazy. This is something that goes beyond generics. Some of the Scala library and syntax is nice, but overall, the IDE integration is still very buggy, and productivity is not higher.<br /><br /><b>Update Dec 12 2012</b>: here is the actual code (this is kept close to the Java equivalent on purpose):<br /><pre>object Point {<br />  def sortAndRemoveIdenticalPoints[T <: Point : ClassManifest](points : Array[T]) : Array[T] = {<br />      Sorting.quickSort(points.asInstanceOf[Array[Point]])<br />      val l = new ArrayBuffer[T](points.length)<br />      var previous = points(0)<br />      l += points(0)<br />      for (i <- 1 until points.length) {<br />        if(math.abs(points(i).value - previous.value)< Epsilon.MACHINE_EPSILON_SQRT) {<br />          l += points(i)<br />        }<br />      }<br />      return l.toArray<br />    }<br />    return points<br />  }<br />}<br /><br />class Point(val value: Double, val isMiddle: Boolean) extends Ordered[Point] {<br />  def compare(that: Point): Int = {<br />    return math.signum(this.value - that.value).toInt<br />  }<br />}<br /><!-----><!--:--></-></:></pre>In Java one can just use Arrays.sort(points) if points is a T[]. And the method can work with a subclass of Point. 



  
</article><article class="post">
  <h1 class="post-title">
    <a href="https://chasethedevil.github.io/post/local-volatility-delta--dynamic/">Local Volatility Delta &amp; Dynamic</a>
  </h1>
  <time datetime="2012-11-29T12:30:00Z" class="post-date">Thu, Nov 29, 2012</time>
   

This will be a very technical post, I am not sure that it will be very understandable by people not familiar with the implied volatility surface.<br /><div><br /></div><div>Something one notices when computing an option price under local volatility using a PDE solver, is how different is the Delta from the standard Black-Scholes Delta, even though the price will be very close for a Vanilla option. In deed, the Finite difference grid will have a different local volatility at each point and the Delta will take into account a change in local volatility as well.</div><div><br /></div><div>But this finite-difference grid Delta is also different from a standard numerical Delta where one just move the initial spot up and down, and takes the difference of computed prices. The numerical Delta will eventually include a change in implied volatility, depending if the surface is sticky-strike (vol will stay constant) or sticky-delta (vol will change). So the numerical Delta produced with a sticky-strike surface will be the same as the standard Black-Scholes Delta. In reality, what happens is that the local volatility is different when the spot moves up, if we recompute it: it is not static. The finite difference solver computes Delta with a static local volatility. If we call twice the finite difference solver with a different initial spot, we will reproduce the correct Delta, that takes into account the dynamic of the implied volatility surface.</div><div><br /></div><div>Here how different it can be if the delta is computed from the grid (static local volatility) or numerically (dynamic local volatility) on an exotic trade:</div><div class="separator" style="clear: both; text-align: center;"><a href="http://3.bp.blogspot.com/-WW8ZE7urDGE/ULdHKNe_VXI/AAAAAAAAGKQ/JQ5Rd7wQYkk/s1600/static_localvol.png" imageanchor="1" style="margin-left: 1em; margin-right: 1em;"><img border="0" height="235" src="http://3.bp.blogspot.com/-WW8ZE7urDGE/ULdHKNe_VXI/AAAAAAAAGKQ/JQ5Rd7wQYkk/s320/static_localvol.png" width="320" /></a></div><div><br /><br />This is often why people assume the local volatility model is wrong, not consistent. It is wrong if we consider the local volatility surface as static to compute hedges.</div>



  
</article>
</div>
<p style="text-align:left; width:49%; display: inline-block;"><a href="/page/18/">Previous</a></p>
<p style="text-align:right; width:50%;  display: inline-block;"><a href="/page/20/">Next</a></p>
    </main>

    
      
    
  </body>
</html>
